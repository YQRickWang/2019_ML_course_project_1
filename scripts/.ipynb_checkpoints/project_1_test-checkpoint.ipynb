{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "import datetime\n",
    "from imps_zda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input train data from csv\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\train.csv'  \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30), (250000,))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of y,tX,ids\n",
    "y.shape,tX.shape,ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 7.45542426e-04, -3.57912369e-05,  4.45005565e-05,  1.02384759e-04,\n",
      "        1.65709638e-05, -8.27618041e-05,  2.12646571e-05,  1.23279777e-06,\n",
      "       -1.67086922e-05,  1.88924878e-04,  7.68269459e-07, -8.89718836e-07,\n",
      "        1.79526102e-05,  4.40138515e-05, -9.20492221e-07,  7.31776350e-06,\n",
      "        2.41253959e-05,  3.66168425e-06, -7.58781738e-06,  5.45773518e-05,\n",
      "       -1.68543763e-06,  2.28216786e-04,  7.28386025e-08,  4.72923720e-04,\n",
      "        3.16097994e-04,  3.14143716e-04,  3.67198696e-05,  1.91842242e-05,\n",
      "        1.03967182e-05,  1.20785453e-04]), 0.5718113686029341)\n"
     ]
    }
   ],
   "source": [
    "dim = tX.shape[1]\n",
    "initial_w = np.zeros(dim)\n",
    "    \n",
    "#least squares SGD\n",
    "weight_SGD = least_squares_SGD(y, tX, initial_w, 1000, 1e-8)\n",
    "print(weight_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(tX):\n",
    "    \n",
    "    length = len(tX)\n",
    "    dim = tX.shape[1]\n",
    "    delete_index=np.array([],dtype=np.int32)\n",
    "    \n",
    "\n",
    "    #for all features\n",
    "    for d in range(dim):\n",
    "        if(np.abs(np.std(tX[:,d]))<1e-4):\n",
    "            delete_index = np.append(delete_index,d)\n",
    "        else:\n",
    "            median = np.median(tX[:,d][tX[:,d]!=-999])\n",
    "            tX[:,d][tX[:,d]==-999] = median\n",
    "            mean = np.mean(tX[:,d])\n",
    "            std = np.std(tX[:,d])\n",
    "            _max = mean+2*std\n",
    "            _min = mean-2*std\n",
    "            \n",
    "            tX[:,d][tX[:,d]>_max] = _max\n",
    "            tX[:,d][tX[:,d]<_min] = _min\n",
    "            \n",
    "            #calculate again\n",
    "            tX[:,d] = (tX[:,d]-np.mean(tX[:,d]))/np.std(tX[:,d])\n",
    "        \n",
    "    tX=np.delete(tX,delete_index,axis=1)\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_groups(y_train,y_test,tX_train,tX_test):\n",
    "    tX_train_jet_num = tX_train[:,22]\n",
    "    tX_test_jet_num = tX_test[:,22]\n",
    "    \n",
    "    y_train_0 = np.copy(y_train[tX_train_jet_num==0])\n",
    "    y_train_1 = np.copy(y_train[tX_train_jet_num==1])\n",
    "    y_train_2 = np.copy(y_train[tX_train_jet_num==2])\n",
    "    y_train_3 = np.copy(y_train[tX_train_jet_num==3])\n",
    "    \n",
    "    y_test_0 = np.copy(y_test[tX_test_jet_num==0])\n",
    "    y_test_1 = np.copy(y_test[tX_test_jet_num==1])\n",
    "    y_test_2 = np.copy(y_test[tX_test_jet_num==2])\n",
    "    y_test_3 = np.copy(y_test[tX_test_jet_num==3])\n",
    "    \n",
    "    tX_train_0 =np.copy(tX_train[tX_train_jet_num==0])\n",
    "    tX_train_1 =np.copy(tX_train[tX_train_jet_num==1])\n",
    "    tX_train_2 =np.copy(tX_train[tX_train_jet_num==2])\n",
    "    tX_train_3 =np.copy(tX_train[tX_train_jet_num==3])\n",
    "    \n",
    "    tX_test_0 = np.copy(tX_test[tX_test_jet_num==0])\n",
    "    tX_test_1 = np.copy(tX_test[tX_test_jet_num==1])\n",
    "    tX_test_2 = np.copy(tX_test[tX_test_jet_num==2])\n",
    "    tX_test_3 = np.copy(tX_test[tX_test_jet_num==3])\n",
    "    \n",
    "    return y_train_0,y_train_1,y_train_2,y_train_3,y_test_0,y_test_1,y_test_2,y_test_3,tX_train_0,tX_train_1,tX_train_2,tX_train_3,tX_test_0,tX_test_1,tX_test_2,tX_test_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(tX,degree):\n",
    "    \n",
    "    N=tX.shape[0]\n",
    "    dim = 10\n",
    "    \n",
    "    \n",
    "    for d in range(dim):\n",
    "        phi = np.zeros((N,degree-1))\n",
    "        valid_index = (tX[:,d]!=-999)\n",
    "        for t in range(2,degree+1):\n",
    "            phi[valid_index,t-2] = np.power(tX[valid_index,d],t)\n",
    "            phi[~valid_index,t-2] = -999\n",
    "        tX = np.concatenate((tX,phi),axis=1)\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(tX,degree):\n",
    "    \n",
    "    \n",
    "    tX=build_poly(tX,degree)\n",
    "    \n",
    "    \n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y,k_fold,seed):\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row/k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices=np.random.permutation(num_row)\n",
    "    k_indices = [indices[k*interval:(k+1)*interval] for k in range (k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y,tX,k_indices,k,lambda_,gamma,max_iters):\n",
    "    \n",
    "    tX_test_indice = k_indices[k]\n",
    "    tX_train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tX_train_indice = tX_train_indice.reshape(-1)\n",
    "    y_test = y[tX_test_indice]\n",
    "    y_train = y[tX_train_indice]\n",
    "    tX_test = tX[tX_test_indice]\n",
    "    tX_train = tX[tX_train_indice]\n",
    "    \n",
    "    #split train and test into four groups\n",
    "    y_train_0,y_train_1,y_train_2,y_train_3,y_test_0,y_test_1,y_test_2,y_test_3,tX_train_0,tX_train_1,tX_train_2,tX_train_3,tX_test_0,tX_test_1,tX_test_2,tX_test_3=split_groups(y_train,y_test,tX_train,tX_test)\n",
    "    \n",
    "    tX_train_0=data_cleaning(tX_train_0)\n",
    "    tX_train_1=data_cleaning(tX_train_1)\n",
    "    tX_train_2=data_cleaning(tX_train_2)\n",
    "    tX_train_3=data_cleaning(tX_train_3)\n",
    "    \n",
    "    tX_test_0=data_cleaning(tX_test_0)\n",
    "    tX_test_1=data_cleaning(tX_test_1)\n",
    "    tX_test_2=data_cleaning(tX_test_2)\n",
    "    tX_test_3=data_cleaning(tX_test_3)  \n",
    "\n",
    "#     tX_train_0 = build_feature(tX_train_0,2)\n",
    "#     tX_train_1 = build_feature(tX_train_1,2)\n",
    "#     tX_train_2 = build_feature(tX_train_2,2)\n",
    "#     tX_train_3 = build_feature(tX_train_3,2)\n",
    "    \n",
    "#     tX_train_1 = build_cross_term(tX_train_1)\n",
    "#     tX_train_2 = build_cross_term(tX_train_2)\n",
    "#     tX_train_3 = build_cross_term(tX_train_3)\n",
    "    \n",
    "#     tX_test_0 = build_feature(tX_test_0,2)\n",
    "#     tX_test_1 = build_feature(tX_test_1,2)\n",
    "#     tX_test_2 = build_feature(tX_test_2,2)\n",
    "#     tX_test_3 = build_feature(tX_test_3,2)\n",
    "    \n",
    "#     tX_train_0=data_cleaning(tX_train_0)\n",
    "#     tX_train_1=data_cleaning(tX_train_1)\n",
    "#     tX_train_2=data_cleaning(tX_train_2)\n",
    "#     tX_train_3=data_cleaning(tX_train_3)\n",
    "    \n",
    "#     tX_test_0=data_cleaning(tX_test_0)\n",
    "#     tX_test_1=data_cleaning(tX_test_1)\n",
    "#     tX_test_2=data_cleaning(tX_test_2)\n",
    "#     tX_test_3=data_cleaning(tX_test_3)      \n",
    "    \n",
    "    weight_SGD_0,weight_LR_0,weight_0=stacking_train(y_train_0,tX_train_0,lambda_,gamma,max_iters)\n",
    "    y_pred_0=stacking_test(tX_test_0,weight_SGD_0,weight_LR_0,weight_0)\n",
    "\n",
    "    weight_SGD_1,weight_LR_1,weight_1=stacking_train(y_train_1,tX_train_1,lambda_,gamma,max_iters)\n",
    "    y_pred_1=stacking_test(tX_test_1,weight_SGD_1,weight_LR_1,weight_1)\n",
    "\n",
    "    weight_SGD_2,weight_LR_2,weight_2=stacking_train(y_train_2,tX_train_2,lambda_,gamma,max_iters)\n",
    "    y_pred_2=stacking_test(tX_test_2,weight_SGD_2,weight_LR_2,weight_2)\n",
    "    \n",
    "    weight_SGD_3,weight_LR_3,weight_3=stacking_train(y_train_3,tX_train_3,lambda_,gamma,max_iters)\n",
    "    y_pred_3=stacking_test(tX_test_3,weight_SGD_3,weight_LR_3,weight_3)\n",
    "    \n",
    "    num_0 = np.count_nonzero(y_pred_0==y_test_0)\n",
    "    num_1 = np.count_nonzero(y_pred_1==y_test_1)\n",
    "    num_2 = np.count_nonzero(y_pred_2==y_test_2)\n",
    "    num_3 = np.count_nonzero(y_pred_3==y_test_3)\n",
    "    \n",
    "    accuracy_0 = num_0/len(y_pred_0)\n",
    "    accuracy_1 = num_1/len(y_pred_1)\n",
    "    accuracy_2 = num_2/len(y_pred_2)\n",
    "    accuracy_3 = num_3/len(y_pred_3)\n",
    "    \n",
    "    num = len(y_test)\n",
    "    print(f'group 0 accuracy: {accuracy_0}')\n",
    "    print(f'group 1 accuracy: {accuracy_1}')\n",
    "    print(f'group 2 accuracy: {accuracy_2}')\n",
    "    print(f'group 3 accuracy: {accuracy_3}')\n",
    "    accuracy = (num_0+num_1+num_2+num_3)/num\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_train(y,tX,lambda_,gamma,max_iters):\n",
    "    \n",
    "    dim = tX.shape[1]\n",
    "    initial_w = np.zeros(dim)\n",
    "    \n",
    "    #least squares SGD\n",
    "    weight_SGD,cost_SGD = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "    y_pred_SGD=predict_labels(weight_SGD, tX)\n",
    "\n",
    "    tX = np.append(tX,np.array([y_pred_SGD]).T,axis=1)\n",
    "    \n",
    "    #Logistic Regression\n",
    "    dim = tX.shape[1]\n",
    "    initial_w = np.append(initial_w,0)    \n",
    "    \n",
    "    weight_LR,cost_LR= reg_logistic_regression(y, tX, lambda_, initial_w, max_iters, gamma)\n",
    "    y_pred_LR=predict_labels(weight_LR, tX)\n",
    "    tX = np.append(tX,np.array([y_pred_LR]).T,axis=1)\n",
    "    \n",
    "    #Ridge Regression\n",
    "    weight,cost_RR = ridge_regression(y,tX,lambda_)\n",
    "    \n",
    "    \n",
    "    return weight_SGD,weight_LR,weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_test(tX,weight_SGD,weight_LR,weight):\n",
    "    \n",
    "    y_pred_SGD=predict_labels(weight_SGD, tX)\n",
    "    tX = np.append(tX,np.array([y_pred_SGD]).T,axis=1)\n",
    "    \n",
    "    y_pred_LR=predict_labels(weight_LR, tX)\n",
    "    tX = np.append(tX,np.array([y_pred_LR]).T,axis=1)\n",
    "    \n",
    "    y_pred=predict_labels(weight, tX)\n",
    "\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    k_fold = 4\n",
    "    seed = 12\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    total = 0\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        accuracy = cross_validation(y,tX,k_indices,k,1e-7,1e-7,10000)\n",
    "        total  = total+accuracy\n",
    "        print(f'{k}:{accuracy} ')\n",
    "    \n",
    "    average=total/k_fold\n",
    "    print(f'average accuracy:{average}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 0 accuracy: 0.7378392421759463\n",
      "group 1 accuracy: 0.6854163456105203\n",
      "group 2 accuracy: 0.7373907465319541\n",
      "group 3 accuracy: 0.6602922605087498\n",
      "0:0.714544 \n",
      "group 0 accuracy: 0.7368736035748484\n",
      "group 1 accuracy: 0.6803485793843139\n",
      "group 2 accuracy: 0.739326375711575\n",
      "group 3 accuracy: 0.6543095458758109\n",
      "1:0.712704 \n",
      "group 0 accuracy: 0.7299158990788946\n",
      "group 1 accuracy: 0.6855777223607648\n",
      "group 2 accuracy: 0.7444215856939389\n",
      "group 3 accuracy: 0.641566265060241\n",
      "2:0.711216 \n",
      "group 0 accuracy: 0.7315768302493966\n",
      "group 1 accuracy: 0.6853776497221651\n",
      "group 2 accuracy: 0.7346696244652194\n",
      "group 3 accuracy: 0.6519168756718022\n",
      "3:0.71072 \n",
      "average accuracy:0.712296\n"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
