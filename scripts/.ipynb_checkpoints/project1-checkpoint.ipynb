{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "import datetime\n",
    "from costs import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define index of labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30), (250000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape,tX.shape,ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 51.655,  68.768, 162.172, ...,  60.526,  19.362,  72.756])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tX=np.delete(tX,[15,18,20,25,28],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DER_mass_MMC = tX[:,0]\n",
    "DER_mass_transverse_met_lep=tX[:,1]\n",
    "DER_mass_vis = tX[:,2]\n",
    "DER_pt_h = tX[:,3]\n",
    "DER_deltaeta_jet_jet = tX[:,4]\n",
    "DER_mass_jet_jet = tX[:,5]\n",
    "DER_prodeta_jet_jet = tX[:,6]\n",
    "DER_deltar_tau_lep = tX[:,7]\n",
    "DER_pt_tot = tX[:,8]\n",
    "DER_sum_pt = tX[:,9]\n",
    "DER_pt_ratio_lep_tau = tX[:,10]\n",
    "DER_met_phi_centrality = tX[:,11]\n",
    "DER_lep_eta_centrality = tX[:,12]\n",
    "PRI_tau_pt = tX[:,13]\n",
    "PRI_tau_eta = tX[:,14]\n",
    "PRI_tau_phi = tX[:,15]\n",
    "PRI_lep_pt = tX[:,16]\n",
    "PRI_lep_eta =tX[:,17]\n",
    "PRI_lep_phi = tX[:,18]\n",
    "PRI_met=tX[:,19]\n",
    "PRI_met_phi = tX[:,20]\n",
    "PRI_met_sumet = tX[:,21]\n",
    "PRI_jet_num=tX[:,22]\n",
    "PRI_jet_leading_pt=tX[:,23]\n",
    "PRI_jet_leading_eta = tX[:,24]\n",
    "PRI_jet_leading_phi = tX[:,25]\n",
    "PRI_jet_subleading_pt=tX[:,26]\n",
    "PRI_jet_subleading_eta = tX[:,27]\n",
    "PRI_jet_subleading_phi = tX[:,28]\n",
    "PRI_jet_all_pt = tX[:,29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x(p_t,phi):\n",
    "    px = p_t*np.cos(phi)\n",
    "    return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_y(p_t,phi):\n",
    "    py = p_t*np.sin(phi)\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_z(p_t,eta):\n",
    "    pz = p_t*np.sinh(eta)\n",
    "    return pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mass are neglected, E = p\n",
    "def particle_energy(px,py,pz):\n",
    "    energy = np.sqrt(px**2+py**2+pz**2)\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_product(px_1,py_1,pz_1,px_2,py_2,pz_2):\n",
    "    N = len(px_1)\n",
    "    cp_x = np.zeros(N)\n",
    "    cp_y = np.zeros(N)\n",
    "    cp_z = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        temp_cross=np.cross(np.array([px_1[t],py_1[t],pz_1[t]]),np.array([px_2[t],py_2[t],pz_2[t]]))\n",
    "        cp_x[t] = temp_cross[0]\n",
    "        cp_y[t] = temp_cross[1]\n",
    "        cp_z[t] = temp_cross[2]\n",
    "    return cp_x,cp_y,cp_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(px_1,py_1,pz_1,px_2,py_2,pz_2):\n",
    "    N = len(px_1)\n",
    "    dp = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        dp[t] = np.inner(np.array([px_1[t],py_1[t],pz_1[t]]),np.array([px_2[t],py_2[t],pz_2[t]]))\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(px_1,py_1,pz_1,px_2,py_2,pz_2):\n",
    "    cp = dot_product(px_1,py_1,pz_1,px_2,py_2,pz_2)/(np.sqrt(px_1**2+py_1**2+pz_1**2)*np.sqrt(px_2**2+py_2**2+pz_2**2))\n",
    "    \n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinant_vector(px_1,py_1,pz_1,px_2,py_2,pz_2,px_3,py_3,pz_3):\n",
    "    N = len(px_1)\n",
    "    dv = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        temp_vector = np.array([[px_1[t],py_1[t],pz_1[t]],[px_2[t],py_2[t],pz_2[t]],[px_3[t],py_3[t],pz_3[t]]])\n",
    "        dv[t] = np.linalg.det(temp_vector)\n",
    "\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_p_xyz(px,py,pz):\n",
    "    sp = px+py+pz\n",
    "    return sp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate px,py,pz and append to the tX\n",
    "#tau\n",
    "p_tau_x = p_x(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_y = p_y(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_z = p_z(PRI_tau_pt,PRI_tau_eta)\n",
    "#lep\n",
    "p_lep_x = p_x(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_y = p_y(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_z = p_z(PRI_lep_pt,PRI_lep_eta)\n",
    "#met\n",
    "p_met_x = p_x(PRI_met,PRI_met_phi)\n",
    "p_met_y = p_y(PRI_met,PRI_met_phi)\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate energy of particle\n",
    "p_tau_energy = particle_energy(p_tau_x,p_tau_y,p_tau_z)\n",
    "p_lep_energy = particle_energy(p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([p_tau_energy,p_lep_energy]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(y)\n",
    "#vector product\n",
    "#tau-lep\n",
    "\n",
    "tau_lep_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cosine_similarity = cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#tau-met\n",
    "tau_met_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cross_x,tau_met_cross_y,tau_met_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cosine_similarity =cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "#lep-met\n",
    "lep_met_dot = dot_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cross_x,lep_met_cross_y,lep_met_cross_z = cross_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cosine_similarity =cosine_similarity(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([tau_lep_dot,tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z,tau_lep_cosine_similarity]).T,axis=1)\n",
    "tX=np.append(tX,np.array([tau_met_dot,tau_met_cross_x,tau_met_cross_y,tau_met_cross_z,tau_met_cosine_similarity]).T,axis=1)\n",
    "tX=np.append(tX,np.array([lep_met_dot,lep_met_cross_x,lep_met_cross_y,lep_met_cross_z,lep_met_cosine_similarity]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate determinant vector\n",
    "d_vector = determinant_vector(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([d_vector]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardlize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many samples' der_mass_mmc is undefined\n",
    "#undefined when the topology of the event is too far from the expeceted topology\n",
    "np.count_nonzero(DER_mass_MMC==-999)\n",
    "#the number of defined DER_mass\n",
    "len(DER_mass_MMC[DER_mass_MMC!=-999])\n",
    "np.mean(DER_mass_MMC[DER_mass_MMC!=-999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many sample's der_mass_transverse_met_lep is undefined\n",
    "#all the values are defined\n",
    "np.count_nonzero(DER_mass_transverse_met_lep==-999)\n",
    "np.mean(DER_mass_transverse_met_lep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide samples into four groups\n",
    "ids_0 = ids[PRI_jet_num==0]\n",
    "ids_1 = ids[PRI_jet_num==1]\n",
    "ids_2 = ids[PRI_jet_num==2]\n",
    "ids_3 = ids[PRI_jet_num==3]\n",
    "\n",
    "tX_0 = np.copy(tX[PRI_jet_num==0,:])\n",
    "tX_1 = np.copy(tX[PRI_jet_num==1,:])\n",
    "tX_2 = np.copy(tX[PRI_jet_num==2,:])\n",
    "tX_3 = np.copy(tX[PRI_jet_num==3,:])\n",
    "\n",
    "y_0 = y[PRI_jet_num==0]\n",
    "y_1 = y[PRI_jet_num==1]\n",
    "y_2 = y[PRI_jet_num==2]\n",
    "y_3 = y[PRI_jet_num==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2551419735169598\n",
      "0.35734550706695556\n",
      "0.5108080747930686\n",
      "0.3036906695542321\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(y_0==1)/len(y_0))\n",
    "print(np.count_nonzero(y_1==1)/len(y_1))\n",
    "print(np.count_nonzero(y_2==1)/len(y_2))\n",
    "print(np.count_nonzero(y_3==1)/len(y_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cheating\n",
    "from sklearn import tree\n",
    "clf_0 = tree.DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "clf_1 = tree.DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "clf_2 = tree.DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "clf_3 = tree.DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "clf_0.fit(tX_0,y_0)\n",
    "clf_1.fit(tX_1,y_1)\n",
    "clf_2.fit(tX_2,y_2)\n",
    "clf_3.fit(tX_3,y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete some features and standardize\n",
    "tX_0=np.delete(tX_0,[4,5,6,12,22,23,24,25,26,27,28,29],1)\n",
    "tX_1=np.delete(tX_1,[4,5,6,12,22,26,27,28,29],1)\n",
    "tX_2=np.delete(tX_2,[22],1)\n",
    "tX_3=np.delete(tX_3,[22],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "num_dim_0 = tX_0.shape[1]\n",
    "num_dim_1 = tX_1.shape[1]\n",
    "num_dim_2_3 = tX_2.shape[1]\n",
    "for t in range(num_dim_0):\n",
    "    tX_0[:,t] = (tX_0[:,t]-np.mean(tX_0[:,t]))/(np.std(tX_0[:,t]))\n",
    "for t in range(num_dim_1):\n",
    "    tX_1[:,t] = (tX_1[:,t]-np.mean(tX_1[:,t]))/(np.std(tX_1[:,t]))    \n",
    "for t in range(num_dim_2_3):\n",
    "    tX_2[:,t] = (tX_2[:,t]-np.mean(tX_2[:,t]))/(np.std(tX_2[:,t]))\n",
    "    tX_3[:,t] = (tX_3[:,t]-np.mean(tX_3[:,t]))/(np.std(tX_3[:,t]))                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stadndardize all features\n",
    "num_dim = tX.shape[1]\n",
    "for t in range(num_dim):\n",
    "    tX[:,t] = (tX[:,t]-np.mean(tX[:,t]))/(np.std(tX[:,t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99913\n",
      "In feature 0,there are 26123 samples are undefined\n",
      "\n",
      "In feature 1,there are 0 samples are undefined\n",
      "\n",
      "In feature 2,there are 0 samples are undefined\n",
      "\n",
      "In feature 3,there are 0 samples are undefined\n",
      "\n",
      "In feature 4,there are 99913 samples are undefined\n",
      "\n",
      "In feature 5,there are 99913 samples are undefined\n",
      "\n",
      "In feature 6,there are 99913 samples are undefined\n",
      "\n",
      "In feature 7,there are 0 samples are undefined\n",
      "\n",
      "In feature 8,there are 0 samples are undefined\n",
      "\n",
      "In feature 9,there are 0 samples are undefined\n",
      "\n",
      "In feature 10,there are 0 samples are undefined\n",
      "\n",
      "In feature 11,there are 0 samples are undefined\n",
      "\n",
      "In feature 12,there are 99913 samples are undefined\n",
      "\n",
      "In feature 13,there are 0 samples are undefined\n",
      "\n",
      "In feature 14,there are 0 samples are undefined\n",
      "\n",
      "In feature 15,there are 0 samples are undefined\n",
      "\n",
      "In feature 16,there are 0 samples are undefined\n",
      "\n",
      "In feature 17,there are 0 samples are undefined\n",
      "\n",
      "In feature 18,there are 0 samples are undefined\n",
      "\n",
      "In feature 19,there are 0 samples are undefined\n",
      "\n",
      "In feature 20,there are 0 samples are undefined\n",
      "\n",
      "In feature 21,there are 0 samples are undefined\n",
      "\n",
      "In feature 22,there are 0 samples are undefined\n",
      "\n",
      "In feature 23,there are 99913 samples are undefined\n",
      "\n",
      "In feature 24,there are 99913 samples are undefined\n",
      "\n",
      "In feature 25,there are 99913 samples are undefined\n",
      "\n",
      "In feature 26,there are 99913 samples are undefined\n",
      "\n",
      "In feature 27,there are 99913 samples are undefined\n",
      "\n",
      "In feature 28,there are 99913 samples are undefined\n",
      "\n",
      "In feature 29,there are 0 samples are undefined\n",
      "\n",
      "In feature 30,there are 0 samples are undefined\n",
      "\n",
      "In feature 31,there are 0 samples are undefined\n",
      "\n",
      "In feature 32,there are 0 samples are undefined\n",
      "\n",
      "In feature 33,there are 0 samples are undefined\n",
      "\n",
      "In feature 34,there are 0 samples are undefined\n",
      "\n",
      "In feature 35,there are 0 samples are undefined\n",
      "\n",
      "In feature 36,there are 0 samples are undefined\n",
      "\n",
      "In feature 37,there are 0 samples are undefined\n",
      "\n",
      "In feature 38,there are 0 samples are undefined\n",
      "\n",
      "In feature 39,there are 0 samples are undefined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#standard basic statistics\n",
    "M = tX_0.shape[1]\n",
    "print(len(tX_0))\n",
    "for t in range(M):\n",
    "    #number of undefined \n",
    "    valid_index = (tX_0[:,t]!=-999)\n",
    "    undefined_count = np.count_nonzero(tX_0[:,t]==-999)\n",
    "    zero_count = np.count_nonzero(tX_0[:,t]==0)\n",
    "    mean = np.mean(tX_0[:,t][valid_index])\n",
    "    std  = np.std(tX_0[:,t][valid_index])\n",
    "    var = np.var(tX_0[:,t][valid_index])\n",
    "    print(f'In feature {t},there are {undefined_count} samples are undefined\\n')\n",
    "    #print(f'In feature {t},the std is\\n')\n",
    "    #print(f'In feature {t},there are {zero_count} samples are zero\\n')\n",
    "    #print(f'For valid samples in this feature, the mean is {mean}, the standard deviation is {std},the variance is {var}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardlize all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Transverse Mass \n",
    "def transverse_mass(a_t,a_phi,b_t,b_phi):\n",
    "    \n",
    "    mass = np.sqrt((a_t+b_t)**2-(a_t*np.cos(a_phi)+b_t*np.cos(b_phi))**2-(a_t*np.sin(a_phi)+b_t*np.sin(b_phi))**2)\n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#transverse_mass test\n",
    "print(transverse_mass(tX[:,19],tX[:,20],tX[:,16],tX[:,18]))\n",
    "print(tX[:,1])\n",
    "print((transverse_mass(tX[:,19],tX[:,20],tX[:,16],tX[:,18])-tX[:,1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Invariant mass\n",
    "def invariant_mass(a_t,a_eta,a_phi,b_t,b_eta,b_phi):\n",
    "    a_z = a_t*np.sinh(a_eta)\n",
    "    b_z = b_t*np.sinh(b_eta)\n",
    "    \n",
    "    a_xyz = np.sqrt(a_t**2+(a_z)**2)\n",
    "    b_xyz = np.sqrt(b_t**2+(b_z)**2)\n",
    "    ab_x = a_t*np.cos(a_phi)+b_t*np.cos(b_phi)\n",
    "    ab_y =a_t*np.sin(a_phi)+b_t*np.sin(b_phi)\n",
    "    ab_z =a_z+b_z\n",
    "    \n",
    "    \n",
    "    mass = np.sqrt((a_xyz+b_xyz)**2-(ab_x)**2-(ab_y)**2-(ab_z)**2)\n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#invariant mass test\n",
    "print(invariant_mass(tX[:,13],tX[:,14],tX[:,15],tX[:,16],tX[:,17],tX[:,18]))\n",
    "print(tX[:,2])\n",
    "print((invariant_mass(tX[:,13],tX[:,14],tX[:,15],tX[:,16],tX[:,17],tX[:,18])-tX[:,2]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Modulus of vecotr sum\n",
    "def modulus_vector(a_t,a_phi,b_t,b_phi,c_met,c_phi):\n",
    "    \n",
    "    x = a_t*np.cos(a_phi)+b_t*np.cos(b_phi)+c_met*np.cos(c_phi)\n",
    "    y = a_t*np.sin(a_phi)+b_t*np.sin(b_phi)+c_met*np.sin(c_phi)\n",
    "    \n",
    "    \n",
    "    p_t = np.sqrt(x**2+y**2)\n",
    "    \n",
    "    modulus = p_t*1.0\n",
    "    return modulus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modulus of the vector sum test\n",
    "print(modulus_vector(tX[:,13],tX[:,15],tX[:,16],tX[:,18],tX[:,19],tX[:,20]))\n",
    "print(tX[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Pseudorapidity Separation\n",
    "def pseudorapidity_separation(jet_num,leading_eta,subleading_eta):\n",
    "    \n",
    "    \n",
    "    N = len(leading_eta)\n",
    "    sep = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        if(jet_num[t]<=1):\n",
    "            sep[t] = -999\n",
    "        else:\n",
    "            sep[t] = np.abs(leading_eta[t] - subleading_eta[t])\n",
    "    return sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#P Separation Test\n",
    "print(pseudorapidity_separation(tX[:,22],tX[:,24],tX[:,27]))\n",
    "print(tX[:,4])\n",
    "print((pseudorapidity_separation(tX[:,22],tX[:,24],tX[:,27])-tX[:,4]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invariant_mass_jet(jet_num,a_t,a_eta,a_phi,b_t,b_eta,b_phi):\n",
    "    \n",
    "    N = len(a_t)\n",
    "    mass = np.zeros(N)\n",
    "    \n",
    "    for t in range(N):\n",
    "        if(jet_num[t]<=1):\n",
    "            mass[t] = -999\n",
    "        else:\n",
    "            #mass[t] = invariant_mass(a_t,)\n",
    "    \n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to derive feature: DER_mass_transverse_met_lep from other features\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge_regression test\n",
    "#simple test\n",
    "weight_0 = ridge_regression(y_0,tX_0,1e-6)\n",
    "weight_1 = ridge_regression(y_1,tX_1,1e-6)\n",
    "weight_2 = ridge_regression(y_2,tX_2,1e-6)\n",
    "weight_3 = ridge_regression(y_3,tX_3,1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = ridge_regression(y,tX,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_0\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_0\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_0 = stochastic_gradient_descent(\n",
    "    y_0, tX_0, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_1\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_1\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_1 = stochastic_gradient_descent(\n",
    "    y_1, tX_1, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_2\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_2\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_2 = stochastic_gradient_descent(\n",
    "    y_2, tX_2, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_3\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_3\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_3 = stochastic_gradient_descent(\n",
    "    y_3, tX_3, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_0 = sgd_ws_0[50]\n",
    "weight_1 = sgd_ws_1[50]\n",
    "weight_2 = sgd_ws_2[50]\n",
    "weight_3 = sgd_ws_3[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 1e-7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.uniform(size=30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=sgd_ws[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "DER_mass_MMC = tX_test[:,0]\n",
    "DER_mass_transverse_met_lep=tX_test[:,1]\n",
    "DER_mass_vis = tX_test[:,2]\n",
    "DER_pt_h = tX_test[:,3]\n",
    "DER_deltaeta_jet_jet = tX_test[:,4]\n",
    "DER_mass_jet_jet = tX_test[:,5]\n",
    "DER_prodeta_jet_jet = tX_test[:,6]\n",
    "DER_deltar_tau_lep = tX_test[:,7]\n",
    "DER_pt_tot = tX_test[:,8]\n",
    "DER_sum_pt = tX_test[:,9]\n",
    "DER_pt_ratio_lep_tau = tX_test[:,10]\n",
    "DER_met_phi_centrality = tX_test[:,11]\n",
    "DER_lep_eta_centrality = tX_test[:,12]\n",
    "PRI_tau_pt = tX_test[:,13]\n",
    "PRI_tau_eta = tX_test[:,14]\n",
    "PRI_tau_phi = tX_test[:,15]\n",
    "PRI_lep_pt = tX_test[:,16]\n",
    "PRI_lep_eta =tX_test[:,17]\n",
    "PRI_lep_phi = tX_test[:,18]\n",
    "PRI_met=tX_test[:,19]\n",
    "PRI_met_phi = tX_test[:,20]\n",
    "PRI_met_sumet = tX_test[:,21]\n",
    "PRI_jet_num=tX_test[:,22]\n",
    "PRI_jet_leading_pt=tX_test[:,23]\n",
    "PRI_jet_leading_eta = tX_test[:,24]\n",
    "PRI_jet_leading_phi = tX_test[:,25]\n",
    "PRI_jet_subleading_pt=tX_test[:,26]\n",
    "PRI_jet_subleading_eta = tX_test[:,27]\n",
    "PRI_jet_subleading_phi = tX_test[:,28]\n",
    "PRI_jet_all_pt = tX_test[:,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "#generate px,py,pz and append to the tX\n",
    "#tau\n",
    "p_tau_x = p_x(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_y = p_y(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_z = p_z(PRI_tau_pt,PRI_tau_eta)\n",
    "#lep\n",
    "p_lep_x = p_x(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_y = p_y(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_z = p_z(PRI_lep_pt,PRI_lep_eta)\n",
    "#met\n",
    "p_met_x = p_x(PRI_met,PRI_met_phi)\n",
    "p_met_y = p_y(PRI_met,PRI_met_phi)\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate energy of particle\n",
    "p_tau_energy = particle_energy(p_tau_x,p_tau_y,p_tau_z)\n",
    "p_lep_energy = particle_energy(p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([p_tau_energy,p_lep_energy]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = tX_test.shape[0]\n",
    "#vector product\n",
    "#tau-lep\n",
    "\n",
    "tau_lep_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cosine_similarity = cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#tau-met\n",
    "tau_met_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cross_x,tau_met_cross_y,tau_met_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cosine_similarity =cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "#lep-met\n",
    "lep_met_dot = dot_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cross_x,lep_met_cross_y,lep_met_cross_z = cross_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cosine_similarity =cosine_similarity(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([tau_lep_dot,tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z,tau_lep_cosine_similarity]).T,axis=1)\n",
    "tX_test=np.append(tX_test,np.array([tau_met_dot,tau_met_cross_x,tau_met_cross_y,tau_met_cross_z,tau_met_cosine_similarity]).T,axis=1)\n",
    "tX_test=np.append(tX_test,np.array([lep_met_dot,lep_met_cross_x,lep_met_cross_y,lep_met_cross_z,lep_met_cosine_similarity]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate determinant vector\n",
    "d_vector = determinant_vector(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([d_vector]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRI_jet_num_test = tX_test[:,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide test\n",
    "\n",
    "ids_test_0 = ids_test[PRI_jet_num_test==0]\n",
    "ids_test_1 = ids_test[PRI_jet_num_test==1]\n",
    "ids_test_2 = ids_test[PRI_jet_num_test==2]\n",
    "ids_test_3 = ids_test[PRI_jet_num_test==3]\n",
    "\n",
    "tX_test_0 = np.copy(tX_test[PRI_jet_num_test==0,:])\n",
    "tX_test_1 = np.copy(tX_test[PRI_jet_num_test==1,:])\n",
    "tX_test_2 = np.copy(tX_test[PRI_jet_num_test==2,:])\n",
    "tX_test_3 = np.copy(tX_test[PRI_jet_num_test==3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0=np.delete(tX_test_0,[4,5,6,12,22,23,24,25,26,27,28,29],1)\n",
    "tX_test_1=np.delete(tX_test_1,[4,5,6,12,22,26,27,28,29],1)\n",
    "tX_test_2=np.delete(tX_test_2,[22],1)\n",
    "tX_test_3=np.delete(tX_test_3,[22],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "num_test_dim_0 = tX_test_0.shape[1]\n",
    "num_test_dim_1 = tX_test_1.shape[1]\n",
    "num_test_dim_2_3 = tX_test_2.shape[1]\n",
    "for t in range(num_test_dim_0):\n",
    "    tX_test_0[:,t] = (tX_test_0[:,t]-np.mean(tX_test_0[:,t]))/(np.std(tX_test_0[:,t]))\n",
    "\n",
    "for t in range(num_test_dim_1):\n",
    "    tX_test_1[:,t] = (tX_test_1[:,t]-np.mean(tX_test_1[:,t]))/(np.std(tX_test_1[:,t]))    \n",
    "\n",
    "for t in range(num_test_dim_2_3):\n",
    "    tX_test_2[:,t] = (tX_test_2[:,t]-np.mean(tX_test_2[:,t]))/(np.std(tX_test_2[:,t]))\n",
    "    tX_test_3[:,t] = (tX_test_3[:,t]-np.mean(tX_test_3[:,t]))/(np.std(tX_test_3[:,t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_test_dim = tX_test.shape[1]\n",
    "\n",
    "# for t in range(num_test_dim):\n",
    "#     tX_test[:,t]=(tX_test[:,t]-np.mean(tX_test[:,t]))/(np.std(tX_test[:,t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_test_0)+len(ids_test_1)+len(ids_test_2)+len(ids_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\submission_test1.csv'\n",
    "# y_pred = predict_labels(weight,tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\submission_test1.csv'\n",
    "y_pred_0 = clf_0.predict(tX_test_0)\n",
    "y_pred_1 = clf_1.predict(tX_test_1)\n",
    "y_pred_2 = clf_2.predict(tX_test_2)\n",
    "y_pred_3 = clf_3.predict(tX_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\submission_test1.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred_0 = predict_labels(weight_0, tX_test_0)\n",
    "y_pred_1 = predict_labels(weight_1, tX_test_1)\n",
    "y_pred_2 = predict_labels(weight_2, tX_test_2)\n",
    "y_pred_3 = predict_labels(weight_3, tX_test_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine y_pred\n",
    "y_pred  = np.zeros(len(ids_test))\n",
    "y_pred[PRI_jet_num_test==0] = y_pred_0\n",
    "y_pred[PRI_jet_num_test==1] = y_pred_1\n",
    "y_pred[PRI_jet_num_test==2] = y_pred_2\n",
    "y_pred[PRI_jet_num_test==3] = y_pred_3\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
