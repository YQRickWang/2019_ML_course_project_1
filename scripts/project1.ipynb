{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "from costs import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define index of labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30), (250000,))"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape,tX.shape,ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 51.655,  68.768, 162.172, ...,  60.526,  19.362,  72.756])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tX=np.delete(tX,[15,18,20,25,28],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "DER_mass_MMC = tX[:,0]\n",
    "DER_mass_transverse_met_lep=tX[:,1]\n",
    "DER_mass_vis = tX[:,2]\n",
    "DER_pt_h = tX[:,3]\n",
    "DER_deltaeta_jet_jet = tX[:,4]\n",
    "DER_mass_jet_jet = tX[:,5]\n",
    "DER_prodeta_jet_jet = tX[:,6]\n",
    "DER_deltar_tau_lep = tX[:,7]\n",
    "DER_pt_tot = tX[:,8]\n",
    "DER_sum_pt = tX[:,9]\n",
    "DER_pt_ratio_lep_tau = tX[:,10]\n",
    "DER_met_phi_centrality = tX[:,11]\n",
    "DER_lep_eta_centrality = tX[:,12]\n",
    "PRI_tau_pt = tX[:,13]\n",
    "PRI_tau_eta = tX[:,14]\n",
    "PRI_tau_phi = tX[:,15]\n",
    "PRI_lep_pt = tX[:,16]\n",
    "PRI_lep_eta =tX[:,17]\n",
    "PRI_lep_phi = tX[:,18]\n",
    "PRI_met=tX[:,19]\n",
    "PRI_met_phi = tX[:,20]\n",
    "PRI_met_sumet = tX[:,21]\n",
    "PRI_jet_num=tX[:,22]\n",
    "PRI_jet_leading_pt=tX[:,23]\n",
    "PRI_jet_leading_eta = tX[:,24]\n",
    "PRI_jet_leading_phi = tX[:,25]\n",
    "PRI_jet_subleading_pt=tX[:,26]\n",
    "PRI_jet_subleading_eta = tX[:,27]\n",
    "PRI_jet_subleading_phi = tX[:,28]\n",
    "PRI_jet_all_pt = tX[:,29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x(p_t,phi):\n",
    "    px = p_t*np.cos(phi)\n",
    "    return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_y(p_t,phi):\n",
    "    py = p_t*np.sin(phi)\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_z(p_t,eta):\n",
    "    pz = p_t*np.sinh(eta)\n",
    "    return pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mass are neglected, E = p\n",
    "def particle_energy(px,py,pz):\n",
    "    energy = np.sqrt(px**2+py**2+pz**2)\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_product(px_1,py_1,pz_1,px_2,py_2,pz_2):\n",
    "    N = len(px_1)\n",
    "    cp_x = np.zeros(N)\n",
    "    cp_y = np.zeros(N)\n",
    "    cp_z = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        temp_cross=np.cross(np.array([px_1[t],py_1[t],pz_1[t]]),np.array([px_2[t],py_2[t],pz_2[t]]))\n",
    "        cp_x[t] = temp_cross[0]\n",
    "        cp_y[t] = temp_cross[1]\n",
    "        cp_z[t] = temp_cross[2]\n",
    "    return cp_x,cp_y,cp_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(px_1,py_1,pz_1,px_2,py_2,pz_2):\n",
    "    N = len(px_1)\n",
    "    dp = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        dp[t] = np.inner(np.array([px_1[t],py_1[t],pz_1[t]]),np.array([px_2[t],py_2[t],pz_2[t]]))\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(px_1,py_1,pz_1,px_2,py_2,pz_2):\n",
    "    cp = dot_product(px_1,py_1,pz_1,px_2,py_2,pz_2)/(np.sqrt(px_1**2+py_1**2+pz_1**2)*np.sqrt(px_2**2+py_2**2+pz_2**2))\n",
    "    \n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinant_vector(px_1,py_1,pz_1,px_2,py_2,pz_2,px_3,py_3,pz_3):\n",
    "    N = len(px_1)\n",
    "    dv = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        temp_vector = np.array([[px_1,py_1,pz_1],[px_2,py_2,pz_2],[px_3,py_3,pz_3]])\n",
    "        dv[t] = np.linalg.det(temp_vecotr)\n",
    "\n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_p_xyz(px,py,pz):\n",
    "    sp = px+py+pz\n",
    "    return sp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate px,py,pz and append to the tX\n",
    "#tau\n",
    "p_tau_x = p_x(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_y = p_y(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_z = p_z(PRI_tau_pt,PRI_tau_eta)\n",
    "#lep\n",
    "p_lep_x = p_x(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_y = p_y(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_z = p_z(PRI_lep_pt,PRI_lep_eta)\n",
    "#met\n",
    "p_met_x = p_x(PRI_met,PRI_met_phi)\n",
    "p_met_y = p_y(PRI_met,PRI_met_phi)\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate energy of particle\n",
    "p_tau_energy = particle_energy(p_tau_x,p_tau_y,p_tau_z)\n",
    "p_lep_energy = particle_energy(p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([p_tau_energy,p_lep_energy]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(y)\n",
    "#vector product\n",
    "#tau-lep\n",
    "\n",
    "tau_lep_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cosine_similarity = cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#tau-met\n",
    "tau_met_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cross_x,tau_met_cross_y,tau_met_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cosine_similarity =cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "#lep-met\n",
    "lep_met_dot = dot_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cross_x,lep_met_cross_y,lep_met_cross_z = cross_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cosine_similarity =cosine_similarity(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "\n",
    "#append features\n",
    "tX=np.append(tX,np.array([tau_lep_dot,tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z,tau_lep_cosine_similarity]).T,axis=1)\n",
    "tX=np.append(tX,np.array([tau_met_dot,tau_met_cross_x,tau_met_cross_y,tau_met_cross_z,tau_met_cosine_similarity]).T,axis=1)\n",
    "tX=np.append(tX,np.array([lep_met_dot,lep_met_cross_x,lep_met_cross_y,lep_met_cross_z,lep_met_cosine_similarity]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 55)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardlize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.85852835958957"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count how many samples' der_mass_mmc is undefined\n",
    "#undefined when the topology of the event is too far from the expeceted topology\n",
    "np.count_nonzero(DER_mass_MMC==-999)\n",
    "#the number of defined DER_mass\n",
    "len(DER_mass_MMC[DER_mass_MMC!=-999])\n",
    "np.mean(DER_mass_MMC[DER_mass_MMC!=-999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.239819276"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count how many sample's der_mass_transverse_met_lep is undefined\n",
    "#all the values are defined\n",
    "np.count_nonzero(DER_mass_transverse_met_lep==-999)\n",
    "np.mean(DER_mass_transverse_met_lep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide samples into four groups\n",
    "ids_0 = ids[PRI_jet_num==0]\n",
    "ids_1 = ids[PRI_jet_num==1]\n",
    "ids_2 = ids[PRI_jet_num==2]\n",
    "ids_3 = ids[PRI_jet_num==3]\n",
    "\n",
    "tX_0 = np.copy(tX[PRI_jet_num==0,:])\n",
    "tX_1 = np.copy(tX[PRI_jet_num==1,:])\n",
    "tX_2 = np.copy(tX[PRI_jet_num==2,:])\n",
    "tX_3 = np.copy(tX[PRI_jet_num==3,:])\n",
    "\n",
    "y_0 = y[PRI_jet_num==0]\n",
    "y_1 = y[PRI_jet_num==1]\n",
    "y_2 = y[PRI_jet_num==2]\n",
    "y_3 = y[PRI_jet_num==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete some features and standardize\n",
    "# tX_0=np.delete(tX_0,[4,5,6,12,22,23,24,25,26,27,28,29],1)\n",
    "# tX_1=np.delete(tX_1,[4,5,6,12,22,26,27,28],1)\n",
    "# tX_2=np.delete(tX_2,[22],1)\n",
    "# tX_3=np.delete(tX_3,[22],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DEVELOPMENT\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "C:\\DEVELOPMENT\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "C:\\DEVELOPMENT\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\DEVELOPMENT\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#standardize data\n",
    "num_dim_0 = tX_0.shape[1]\n",
    "num_dim_1 = tX_1.shape[1]\n",
    "num_dim_2_3 = tX_2.shape[1]\n",
    "for t in range(num_dim_0):\n",
    "    tX_0[:,t] = (tX_0[:,t]-np.mean(tX_0[:,t]))/(np.std(tX_0[:,t]))\n",
    "for t in range(num_dim_1):\n",
    "    tX_1[:,t] = (tX_1[:,t]-np.mean(tX_1[:,t]))/(np.std(tX_1[:,t]))    \n",
    "for t in range(num_dim_2_3):\n",
    "    tX_2[:,t] = (tX_2[:,t]-np.mean(tX_2[:,t]))/(np.std(tX_2[:,t]))\n",
    "    tX_3[:,t] = (tX_3[:,t]-np.mean(tX_3[:,t]))/(np.std(tX_3[:,t]))                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 43)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41459643,  0.32963926,  0.32899482,  0.21950954, -1.13926952,\n",
       "       -1.13926952, -1.13926952,  0.22344687,  0.21950954,  0.32223093,\n",
       "        0.22214712,  0.21719962, -1.13926952,  0.24973699,  0.21669658,\n",
       "        0.21896027,  0.29144061,  0.21823698,  0.21473199,  0.26120501,\n",
       "        0.21902825,  0.33595448,  0.21894667, -1.13926952, -1.13926952,\n",
       "       -1.13926952, -1.13926952, -1.13926952, -1.13926952,  0.21894667,\n",
       "        0.24973545,  0.21925457,  0.14132448,  0.14651543,  0.21593233,\n",
       "        0.17936272,  0.26112897,  0.22148065,  0.30245265,  0.30154365,\n",
       "        0.83796025,  0.03788434,  5.25067292,  0.16708742,  0.21911257,\n",
       "        1.17477513,  0.36361901, -2.18936478,  0.26677796,  0.21944735,\n",
       "       -2.03392823,  0.29272329, -1.00918776,  0.17747254,  0.2177536 ])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In feature 0,there are 38114 samples are undefined\n",
      "\n",
      "In feature 1,there are 0 samples are undefined\n",
      "\n",
      "In feature 2,there are 0 samples are undefined\n",
      "\n",
      "In feature 3,there are 0 samples are undefined\n",
      "\n",
      "In feature 4,there are 177457 samples are undefined\n",
      "\n",
      "In feature 5,there are 177457 samples are undefined\n",
      "\n",
      "In feature 6,there are 177457 samples are undefined\n",
      "\n",
      "In feature 7,there are 0 samples are undefined\n",
      "\n",
      "In feature 8,there are 0 samples are undefined\n",
      "\n",
      "In feature 9,there are 0 samples are undefined\n",
      "\n",
      "In feature 10,there are 0 samples are undefined\n",
      "\n",
      "In feature 11,there are 0 samples are undefined\n",
      "\n",
      "In feature 12,there are 177457 samples are undefined\n",
      "\n",
      "In feature 13,there are 0 samples are undefined\n",
      "\n",
      "In feature 14,there are 0 samples are undefined\n",
      "\n",
      "In feature 15,there are 0 samples are undefined\n",
      "\n",
      "In feature 16,there are 0 samples are undefined\n",
      "\n",
      "In feature 17,there are 0 samples are undefined\n",
      "\n",
      "In feature 18,there are 0 samples are undefined\n",
      "\n",
      "In feature 19,there are 0 samples are undefined\n",
      "\n",
      "In feature 20,there are 0 samples are undefined\n",
      "\n",
      "In feature 21,there are 0 samples are undefined\n",
      "\n",
      "In feature 22,there are 0 samples are undefined\n",
      "\n",
      "In feature 23,there are 99913 samples are undefined\n",
      "\n",
      "In feature 24,there are 99913 samples are undefined\n",
      "\n",
      "In feature 25,there are 99913 samples are undefined\n",
      "\n",
      "In feature 26,there are 177457 samples are undefined\n",
      "\n",
      "In feature 27,there are 177457 samples are undefined\n",
      "\n",
      "In feature 28,there are 177457 samples are undefined\n",
      "\n",
      "In feature 29,there are 0 samples are undefined\n",
      "\n",
      "In feature 30,there are 0 samples are undefined\n",
      "\n",
      "In feature 31,there are 0 samples are undefined\n",
      "\n",
      "In feature 32,there are 0 samples are undefined\n",
      "\n",
      "In feature 33,there are 0 samples are undefined\n",
      "\n",
      "In feature 34,there are 0 samples are undefined\n",
      "\n",
      "In feature 35,there are 0 samples are undefined\n",
      "\n",
      "In feature 36,there are 0 samples are undefined\n",
      "\n",
      "In feature 37,there are 0 samples are undefined\n",
      "\n",
      "In feature 38,there are 0 samples are undefined\n",
      "\n",
      "In feature 39,there are 0 samples are undefined\n",
      "\n",
      "In feature 40,there are 0 samples are undefined\n",
      "\n",
      "In feature 41,there are 0 samples are undefined\n",
      "\n",
      "In feature 42,there are 0 samples are undefined\n",
      "\n",
      "In feature 43,there are 0 samples are undefined\n",
      "\n",
      "In feature 44,there are 0 samples are undefined\n",
      "\n",
      "In feature 45,there are 0 samples are undefined\n",
      "\n",
      "In feature 46,there are 0 samples are undefined\n",
      "\n",
      "In feature 47,there are 0 samples are undefined\n",
      "\n",
      "In feature 48,there are 0 samples are undefined\n",
      "\n",
      "In feature 49,there are 0 samples are undefined\n",
      "\n",
      "In feature 50,there are 0 samples are undefined\n",
      "\n",
      "In feature 51,there are 0 samples are undefined\n",
      "\n",
      "In feature 52,there are 0 samples are undefined\n",
      "\n",
      "In feature 53,there are 0 samples are undefined\n",
      "\n",
      "In feature 54,there are 0 samples are undefined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#standard basic statistics\n",
    "M = tX.shape[1]\n",
    "for t in range(M):\n",
    "    #number of undefined \n",
    "    valid_index = (tX[:,t]!=-999)\n",
    "    undefined_count = np.count_nonzero(tX[:,t]==-999)\n",
    "    zero_count = np.count_nonzero(tX[:,t]==0)\n",
    "    mean = np.mean(tX[:,t][valid_index])\n",
    "    std  = np.std(tX[:,t][valid_index])\n",
    "    var = np.var(tX[:,t][valid_index])\n",
    "    print(f'In feature {t},there are {undefined_count} samples are undefined\\n')\n",
    "    #print(f'In feature {t},there are {zero_count} samples are zero\\n')\n",
    "    #print(f'For valid samples in this feature, the mean is {mean}, the standard deviation is {std},the variance is {var}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardlize all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Transverse Mass \n",
    "def transverse_mass(a_t,a_phi,b_t,b_phi):\n",
    "    \n",
    "    \n",
    "    mass = np.sqrt((a_t+b_t)**2-(a_t*np.cos(a_phi)+b_t*np.cos(b_phi))**2-(a_t*np.sin(a_phi)+b_t*np.sin(b_phi))**2)\n",
    "    \n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 51.66192009  68.78329787 162.174512   ...  60.52079714  19.36191784\n",
      "  72.75137486]\n",
      "[ 51.655  68.768 162.172 ...  60.526  19.362  72.756]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DEVELOPMENT\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#transverse_mass test\n",
    "print(transverse_mass(tX[:,19],tX[:,20],tX[:,16],tX[:,18]))\n",
    "print(tX[:,1])\n",
    "print((transverse_mass(tX[:,19],tX[:,20],tX[:,16],tX[:,18])-tX[:,1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Invariant mass\n",
    "def invariant_mass(a_t,a_eta,a_phi,b_t,b_eta,b_phi):\n",
    "    a_z = a_t*np.sinh(a_eta)\n",
    "    b_z = b_t*np.sinh(b_eta)\n",
    "    \n",
    "    a_xyz = np.sqrt(a_t**2+(a_z)**2)\n",
    "    b_xyz = np.sqrt(b_t**2+(b_z)**2)\n",
    "    ab_x = a_t*np.cos(a_phi)+b_t*np.cos(b_phi)\n",
    "    ab_y =a_t*np.sin(a_phi)+b_t*np.sin(b_phi)\n",
    "    ab_z =a_z+b_z\n",
    "    \n",
    "    \n",
    "    mass = np.sqrt((a_xyz+b_xyz)**2-(ab_x)**2-(ab_y)**2-(ab_z)**2)\n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97.80554839 103.22216251 125.92230193 ...  75.82162756  68.79599297\n",
      "  70.82931157]\n",
      "[ 97.827 103.235 125.953 ...  75.839  68.812  70.831]\n",
      "-0.01725799703735722\n"
     ]
    }
   ],
   "source": [
    "#invariant mass test\n",
    "print(invariant_mass(tX[:,13],tX[:,14],tX[:,15],tX[:,16],tX[:,17],tX[:,18]))\n",
    "print(tX[:,2])\n",
    "print((invariant_mass(tX[:,13],tX[:,14],tX[:,15],tX[:,16],tX[:,17],tX[:,18])-tX[:,2]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Modulus of vecotr sum\n",
    "def modulus_vector(a_t,a_phi,b_t,b_phi,c_met,c_phi):\n",
    "    \n",
    "    x = a_t*np.cos(a_phi)+b_t*np.cos(b_phi)+c_met*np.cos(c_phi)\n",
    "    y = a_t*np.sin(a_phi)+b_t*np.sin(b_phi)+c_met*np.sin(c_phi)\n",
    "    \n",
    "    \n",
    "    p_t = np.sqrt(x**2+y**2)\n",
    "    \n",
    "    modulus = p_t*1.0\n",
    "    return modulus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.94739991 48.12297042 35.62593902 ... 39.78224526 13.5098374\n",
      "  7.47152233]\n",
      "[27.98  48.146 35.635 ... 39.757 13.504  7.479]\n"
     ]
    }
   ],
   "source": [
    "#modulus of the vector sum test\n",
    "print(modulus_vector(tX[:,13],tX[:,15],tX[:,16],tX[:,18],tX[:,19],tX[:,20]))\n",
    "print(tX[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Pseudorapidity Separation\n",
    "def pseudorapidity_separation(jet_num,leading_eta,subleading_eta):\n",
    "    \n",
    "    \n",
    "    N = len(leading_eta)\n",
    "    sep = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        if(jet_num[t]<=1):\n",
    "            sep[t] = -999\n",
    "        else:\n",
    "            sep[t] = np.abs(leading_eta[t] - subleading_eta[t])\n",
    "    return sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.10e-01 -9.99e+02 -9.99e+02 ... -9.99e+02 -9.99e+02 -9.99e+02]\n",
      "[ 9.10e-01 -9.99e+02 -9.99e+02 ... -9.99e+02 -9.99e+02 -9.99e+02]\n",
      "3.0000000000019655e-07\n"
     ]
    }
   ],
   "source": [
    "#P Separation Test\n",
    "print(pseudorapidity_separation(tX[:,22],tX[:,24],tX[:,27]))\n",
    "print(tX[:,4])\n",
    "print((pseudorapidity_separation(tX[:,22],tX[:,24],tX[:,27])-tX[:,4]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invariant_mass_jet(jet_num,a_t,a_eta,a_phi,b_t,b_eta,b_phi):\n",
    "    \n",
    "    N = len(a_t)\n",
    "    mass = np.zeros(N)\n",
    "    \n",
    "    for t in range(N):\n",
    "        if(jet_num[t]<=1):\n",
    "            mass[t] = -999\n",
    "        else:\n",
    "            #mass[t] = invariant_mass(a_t,)\n",
    "    \n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to derive feature: DER_mass_transverse_met_lep from other features\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge_regression test\n",
    "#simple test\n",
    "weight_0 = ridge_regression(y_0,tX_0,5)\n",
    "weight_1 = ridge_regression(y_1,tX_1,5)\n",
    "weight_2 = ridge_regression(y_2,tX_2,5)\n",
    "weight_3 = ridge_regression(y_3,tX_3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 55)"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=0.25951088046288434\n",
      "SGD(1/49): loss=0.2594896301694272\n",
      "SGD(2/49): loss=0.2595319199295533\n",
      "SGD(3/49): loss=0.25952310963431774\n",
      "SGD(4/49): loss=0.2596241872203506\n",
      "SGD(5/49): loss=0.25952678541972773\n",
      "SGD(6/49): loss=0.25963090977084535\n",
      "SGD(7/49): loss=0.2595597348772314\n",
      "SGD(8/49): loss=0.2598229600024645\n",
      "SGD(9/49): loss=0.2600497151147627\n",
      "SGD(10/49): loss=0.2596577521613132\n",
      "SGD(11/49): loss=0.25975187323579124\n",
      "SGD(12/49): loss=0.25978629584659085\n",
      "SGD(13/49): loss=0.25986492768449587\n",
      "SGD(14/49): loss=0.2599421258826066\n",
      "SGD(15/49): loss=0.2596483142183506\n",
      "SGD(16/49): loss=0.25976627494909266\n",
      "SGD(17/49): loss=0.2598091935023254\n",
      "SGD(18/49): loss=0.2597778410363432\n",
      "SGD(19/49): loss=0.26059209596934024\n",
      "SGD(20/49): loss=0.2606417726829958\n",
      "SGD(21/49): loss=0.26074620680372823\n",
      "SGD(22/49): loss=0.2605296002396429\n",
      "SGD(23/49): loss=0.2602937697140178\n",
      "SGD(24/49): loss=0.2600649468137439\n",
      "SGD(25/49): loss=0.2600148961180211\n",
      "SGD(26/49): loss=0.2599977741281006\n",
      "SGD(27/49): loss=0.26010043693758467\n",
      "SGD(28/49): loss=0.2626831576316612\n",
      "SGD(29/49): loss=0.2609598860802726\n",
      "SGD(30/49): loss=0.260341334109716\n",
      "SGD(31/49): loss=0.26027950767629326\n",
      "SGD(32/49): loss=0.2600556346927127\n",
      "SGD(33/49): loss=0.2600985794467603\n",
      "SGD(34/49): loss=0.2599860970988355\n",
      "SGD(35/49): loss=0.2598873851939338\n",
      "SGD(36/49): loss=0.25994509368053054\n",
      "SGD(37/49): loss=0.25984994418226737\n",
      "SGD(38/49): loss=0.25983842737397406\n",
      "SGD(39/49): loss=0.2599024318465255\n",
      "SGD(40/49): loss=0.26026260498454523\n",
      "SGD(41/49): loss=0.26021459972111616\n",
      "SGD(42/49): loss=0.2599213212791615\n",
      "SGD(43/49): loss=0.2599432263892199\n",
      "SGD(44/49): loss=0.26030960982201534\n",
      "SGD(45/49): loss=0.26044587341118697\n",
      "SGD(46/49): loss=0.2606763610096868\n",
      "SGD(47/49): loss=0.2603779407215689\n",
      "SGD(48/49): loss=0.2601633181655469\n",
      "SGD(49/49): loss=0.26017186996985675\n",
      "SGD: execution time=2.207 seconds\n"
     ]
    }
   ],
   "source": [
    "#w_0\n",
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_0\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_0 = stochastic_gradient_descent(\n",
    "    y_0, tX_0, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=0.36687793194262497\n",
      "SGD(1/49): loss=0.3668732027043324\n",
      "SGD(2/49): loss=0.36759942731220746\n",
      "SGD(3/49): loss=0.3672871424563169\n",
      "SGD(4/49): loss=0.36637421941577364\n",
      "SGD(5/49): loss=0.3662327308448116\n",
      "SGD(6/49): loss=0.3662208181289026\n",
      "SGD(7/49): loss=0.36629809522503515\n",
      "SGD(8/49): loss=0.3663819610540214\n",
      "SGD(9/49): loss=0.3666398896139908\n",
      "SGD(10/49): loss=0.3666579538107991\n",
      "SGD(11/49): loss=0.36723617825752614\n",
      "SGD(12/49): loss=0.3667375254229782\n",
      "SGD(13/49): loss=0.3667235374354852\n",
      "SGD(14/49): loss=0.36652835109305293\n",
      "SGD(15/49): loss=0.3668123446575254\n",
      "SGD(16/49): loss=0.3665464959576645\n",
      "SGD(17/49): loss=0.3665444961690759\n",
      "SGD(18/49): loss=0.3666239839108742\n",
      "SGD(19/49): loss=0.3665124863278421\n",
      "SGD(20/49): loss=0.366470878132222\n",
      "SGD(21/49): loss=0.36654928321403485\n",
      "SGD(22/49): loss=0.3666214546586283\n",
      "SGD(23/49): loss=0.3667194843856218\n",
      "SGD(24/49): loss=0.3666868653156304\n",
      "SGD(25/49): loss=0.3663802908799355\n",
      "SGD(26/49): loss=0.36638215152672876\n",
      "SGD(27/49): loss=0.3663303218457909\n",
      "SGD(28/49): loss=0.3663001673932673\n",
      "SGD(29/49): loss=0.3663111828799496\n",
      "SGD(30/49): loss=0.366528436285737\n",
      "SGD(31/49): loss=0.3671484749722931\n",
      "SGD(32/49): loss=0.3667974371116501\n",
      "SGD(33/49): loss=0.36646932569856683\n",
      "SGD(34/49): loss=0.3664241190885591\n",
      "SGD(35/49): loss=0.36627183886204934\n",
      "SGD(36/49): loss=0.36640408802012814\n",
      "SGD(37/49): loss=0.3662713213094905\n",
      "SGD(38/49): loss=0.3662560248174728\n",
      "SGD(39/49): loss=0.366597641998403\n",
      "SGD(40/49): loss=0.36675791840521194\n",
      "SGD(41/49): loss=0.3669215083376849\n",
      "SGD(42/49): loss=0.36722439327070705\n",
      "SGD(43/49): loss=0.36701966312394996\n",
      "SGD(44/49): loss=0.3671764059407045\n",
      "SGD(45/49): loss=0.3668352483599633\n",
      "SGD(46/49): loss=0.36638342721267464\n",
      "SGD(47/49): loss=0.36623414880082567\n",
      "SGD(48/49): loss=0.3663306459319309\n",
      "SGD(49/49): loss=0.3665284560884487\n",
      "SGD: execution time=1.675 seconds\n"
     ]
    }
   ],
   "source": [
    "#w_1\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_1\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_1 = stochastic_gradient_descent(\n",
    "    y_1, tX_1, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=0.350769728336585\n",
      "SGD(1/49): loss=0.3505448346892893\n",
      "SGD(2/49): loss=0.3505339269250971\n",
      "SGD(3/49): loss=0.35060576248417824\n",
      "SGD(4/49): loss=0.3505521344829427\n",
      "SGD(5/49): loss=0.35058843831784897\n",
      "SGD(6/49): loss=0.3522731215434181\n",
      "SGD(7/49): loss=0.3527471935957522\n",
      "SGD(8/49): loss=0.3556317252541354\n",
      "SGD(9/49): loss=0.35663892199656055\n",
      "SGD(10/49): loss=0.35129639422390374\n",
      "SGD(11/49): loss=0.3510018553699851\n",
      "SGD(12/49): loss=0.3513508972238892\n",
      "SGD(13/49): loss=0.3514125837890089\n",
      "SGD(14/49): loss=0.3515250153108364\n",
      "SGD(15/49): loss=0.35151495689609874\n",
      "SGD(16/49): loss=0.351206726727009\n",
      "SGD(17/49): loss=0.3529138106927525\n",
      "SGD(18/49): loss=0.3534674123863927\n",
      "SGD(19/49): loss=0.35454693393647874\n",
      "SGD(20/49): loss=0.3526850857443757\n",
      "SGD(21/49): loss=0.3547654184489923\n",
      "SGD(22/49): loss=0.35409189596493124\n",
      "SGD(23/49): loss=0.3548192979026911\n",
      "SGD(24/49): loss=0.35385027568453237\n",
      "SGD(25/49): loss=0.35394199791451514\n",
      "SGD(26/49): loss=0.3523791216402672\n",
      "SGD(27/49): loss=0.3510261749757858\n",
      "SGD(28/49): loss=0.35121398109266144\n",
      "SGD(29/49): loss=0.35178684448176734\n",
      "SGD(30/49): loss=0.3536206740842265\n",
      "SGD(31/49): loss=0.35271310619270163\n",
      "SGD(32/49): loss=0.3521151812681378\n",
      "SGD(33/49): loss=0.3521223994571497\n",
      "SGD(34/49): loss=0.35173292266949296\n",
      "SGD(35/49): loss=0.3513648496676379\n",
      "SGD(36/49): loss=0.35126659336411714\n",
      "SGD(37/49): loss=0.35141932765372097\n",
      "SGD(38/49): loss=0.35087203710789944\n",
      "SGD(39/49): loss=0.3508513794126171\n",
      "SGD(40/49): loss=0.351222079100092\n",
      "SGD(41/49): loss=0.3512325806732975\n",
      "SGD(42/49): loss=0.35171029657230607\n",
      "SGD(43/49): loss=0.3524110975724768\n",
      "SGD(44/49): loss=0.3526387082447091\n",
      "SGD(45/49): loss=0.3511984626586536\n",
      "SGD(46/49): loss=0.35080723841622174\n",
      "SGD(47/49): loss=0.3508637419398405\n",
      "SGD(48/49): loss=0.3509319680989646\n",
      "SGD(49/49): loss=0.35093747427234295\n",
      "SGD: execution time=1.165 seconds\n"
     ]
    }
   ],
   "source": [
    "#w_2\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_2\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_2 = stochastic_gradient_descent(\n",
    "    y_2, tX_2, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=0.35483203064571106\n",
      "SGD(1/49): loss=0.3548035339845815\n",
      "SGD(2/49): loss=0.3548314297566519\n",
      "SGD(3/49): loss=0.35520615760482277\n",
      "SGD(4/49): loss=0.3569477329186128\n",
      "SGD(5/49): loss=0.35723012530951365\n",
      "SGD(6/49): loss=0.35820183364898417\n",
      "SGD(7/49): loss=0.35692888880436524\n",
      "SGD(8/49): loss=0.3563067750206281\n",
      "SGD(9/49): loss=0.35537760762121007\n",
      "SGD(10/49): loss=0.3555213429873563\n",
      "SGD(11/49): loss=0.35486243816215723\n",
      "SGD(12/49): loss=0.3552381287205158\n",
      "SGD(13/49): loss=0.35488178736824394\n",
      "SGD(14/49): loss=0.35530919324654625\n",
      "SGD(15/49): loss=0.35461474539000387\n",
      "SGD(16/49): loss=0.3557985605101148\n",
      "SGD(17/49): loss=0.3549296505256801\n",
      "SGD(18/49): loss=0.35515949818697223\n",
      "SGD(19/49): loss=0.35544029073377087\n",
      "SGD(20/49): loss=0.3551874719471612\n",
      "SGD(21/49): loss=0.35622782740543607\n",
      "SGD(22/49): loss=0.3581001458878435\n",
      "SGD(23/49): loss=0.3614239651009113\n",
      "SGD(24/49): loss=0.3579478861878582\n",
      "SGD(25/49): loss=0.35550728203691007\n",
      "SGD(26/49): loss=0.35738784791486916\n",
      "SGD(27/49): loss=0.3584134388471033\n",
      "SGD(28/49): loss=0.356520107194316\n",
      "SGD(29/49): loss=0.35539813198129644\n",
      "SGD(30/49): loss=0.36795487826626544\n",
      "SGD(31/49): loss=0.36249158865990166\n",
      "SGD(32/49): loss=0.3596574593672316\n",
      "SGD(33/49): loss=0.3613879933552717\n",
      "SGD(34/49): loss=0.3591808474148699\n",
      "SGD(35/49): loss=0.3576702907459522\n",
      "SGD(36/49): loss=0.35701994615737526\n",
      "SGD(37/49): loss=0.355179396022156\n",
      "SGD(38/49): loss=0.3550708220327596\n",
      "SGD(39/49): loss=0.355592484036501\n",
      "SGD(40/49): loss=0.3552546381966078\n",
      "SGD(41/49): loss=0.3558442288182919\n",
      "SGD(42/49): loss=0.35608223874764033\n",
      "SGD(43/49): loss=0.35556123307300247\n",
      "SGD(44/49): loss=0.3551576178615328\n",
      "SGD(45/49): loss=0.3552621942248302\n",
      "SGD(46/49): loss=0.3552612945822369\n",
      "SGD(47/49): loss=0.3558677790816889\n",
      "SGD(48/49): loss=0.3573651623241496\n",
      "SGD(49/49): loss=0.35847336841236993\n",
      "SGD: execution time=0.420 seconds\n"
     ]
    }
   ],
   "source": [
    "#w_3\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 1e-9\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = weight_3\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws_3 = stochastic_gradient_descent(\n",
    "    y_3, tX_3, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_0 = sgd_ws_0[50]\n",
    "weight_1 = sgd_ws_1[50]\n",
    "weight_2 = sgd_ws_2[50]\n",
    "weight_3 = sgd_ws_3[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=4026597.208806231\n",
      "Gradient Descent(1/999): loss=858320.896280879\n",
      "Gradient Descent(2/999): loss=278103.8668724276\n",
      "Gradient Descent(3/999): loss=168869.76711567058\n",
      "Gradient Descent(4/999): loss=145445.58778938354\n",
      "Gradient Descent(5/999): loss=137750.43407121304\n",
      "Gradient Descent(6/999): loss=133029.59443376292\n",
      "Gradient Descent(7/999): loss=128958.17531372968\n",
      "Gradient Descent(8/999): loss=125109.04764643247\n",
      "Gradient Descent(9/999): loss=121400.56154920557\n",
      "Gradient Descent(10/999): loss=117814.1184050088\n",
      "Gradient Descent(11/999): loss=114342.77422178564\n",
      "Gradient Descent(12/999): loss=110981.87652578918\n",
      "Gradient Descent(13/999): loss=107727.34960038488\n",
      "Gradient Descent(14/999): loss=104575.37214741267\n",
      "Gradient Descent(15/999): loss=101522.30912237997\n",
      "Gradient Descent(16/999): loss=98564.69058087931\n",
      "Gradient Descent(17/999): loss=95699.19977692603\n",
      "Gradient Descent(18/999): loss=92922.66358064636\n",
      "Gradient Descent(19/999): loss=90232.04389708361\n",
      "Gradient Descent(20/999): loss=87624.42979467967\n",
      "Gradient Descent(21/999): loss=85097.03024363433\n",
      "Gradient Descent(22/999): loss=82647.1674035891\n",
      "Gradient Descent(23/999): loss=80272.27041118189\n",
      "Gradient Descent(24/999): loss=77969.86962362191\n",
      "Gradient Descent(25/999): loss=75737.59127869386\n",
      "Gradient Descent(26/999): loss=73573.15253530981\n",
      "Gradient Descent(27/999): loss=71474.35686205597\n",
      "Gradient Descent(28/999): loss=69439.08974418994\n",
      "Gradient Descent(29/999): loss=67465.31468226503\n",
      "Gradient Descent(30/999): loss=65551.06945802284\n",
      "Gradient Descent(31/999): loss=63694.46264542387\n",
      "Gradient Descent(32/999): loss=61893.670346706436\n",
      "Gradient Descent(33/999): loss=60146.93313519134\n",
      "Gradient Descent(34/999): loss=58452.5531882058\n",
      "Gradient Descent(35/999): loss=56808.89159499987\n",
      "Gradient Descent(36/999): loss=55214.36582588652\n",
      "Gradient Descent(37/999): loss=53667.447350067516\n",
      "Gradient Descent(38/999): loss=52166.6593907229\n",
      "Gradient Descent(39/999): loss=50710.57480695124\n",
      "Gradient Descent(40/999): loss=49297.81409306604\n",
      "Gradient Descent(41/999): loss=47927.04348658248\n",
      "Gradient Descent(42/999): loss=46596.973176983214\n",
      "Gradient Descent(43/999): loss=45306.355608035126\n",
      "Gradient Descent(44/999): loss=44053.98386704833\n",
      "Gradient Descent(45/999): loss=42838.69015503228\n",
      "Gradient Descent(46/999): loss=41659.344332213936\n",
      "Gradient Descent(47/999): loss=40514.85253384727\n",
      "Gradient Descent(48/999): loss=39404.15585166425\n",
      "Gradient Descent(49/999): loss=38326.2290767007\n",
      "Gradient Descent(50/999): loss=37280.07949957831\n",
      "Gradient Descent(51/999): loss=36264.745764640174\n",
      "Gradient Descent(52/999): loss=35279.29677462582\n",
      "Gradient Descent(53/999): loss=34322.83064283249\n",
      "Gradient Descent(54/999): loss=33394.4736899495\n",
      "Gradient Descent(55/999): loss=32493.379482968103\n",
      "Gradient Descent(56/999): loss=31618.727913768624\n",
      "Gradient Descent(57/999): loss=30769.724315166284\n",
      "Gradient Descent(58/999): loss=29945.59861236227\n",
      "Gradient Descent(59/999): loss=29145.604507896922\n",
      "Gradient Descent(60/999): loss=28369.01869833902\n",
      "Gradient Descent(61/999): loss=27615.14012107095\n",
      "Gradient Descent(62/999): loss=26883.2892296442\n",
      "Gradient Descent(63/999): loss=26172.807296284587\n",
      "Gradient Descent(64/999): loss=25483.055740223255\n",
      "Gradient Descent(65/999): loss=24813.415480617197\n",
      "Gradient Descent(66/999): loss=24163.286312904438\n",
      "Gradient Descent(67/999): loss=23532.08630751305\n",
      "Gradient Descent(68/999): loss=22919.251229911373\n",
      "Gradient Descent(69/999): loss=22324.233981050238\n",
      "Gradient Descent(70/999): loss=21746.504057304806\n",
      "Gradient Descent(71/999): loss=21185.547029077872\n",
      "Gradient Descent(72/999): loss=20640.864037275052\n",
      "Gradient Descent(73/999): loss=20111.971306907915\n",
      "Gradient Descent(74/999): loss=19598.399677122958\n",
      "Gradient Descent(75/999): loss=19099.694146993195\n",
      "Gradient Descent(76/999): loss=18615.41343644512\n",
      "Gradient Descent(77/999): loss=18145.129561727117\n",
      "Gradient Descent(78/999): loss=17688.427424856265\n",
      "Gradient Descent(79/999): loss=17244.904416509584\n",
      "Gradient Descent(80/999): loss=16814.170031852103\n",
      "Gradient Descent(81/999): loss=16395.845498819588\n",
      "Gradient Descent(82/999): loss=15989.563418396561\n",
      "Gradient Descent(83/999): loss=15594.967416452546\n",
      "Gradient Descent(84/999): loss=15211.711806719215\n",
      "Gradient Descent(85/999): loss=14839.461264510723\n",
      "Gradient Descent(86/999): loss=14477.890510806987\n",
      "Gradient Descent(87/999): loss=14126.684006336623\n",
      "Gradient Descent(88/999): loss=13785.535655311922\n",
      "Gradient Descent(89/999): loss=13454.148518483133\n",
      "Gradient Descent(90/999): loss=13132.234535193287\n",
      "Gradient Descent(91/999): loss=12819.514254127824\n",
      "Gradient Descent(92/999): loss=12515.71657246595\n",
      "Gradient Descent(93/999): loss=12220.578483152272\n",
      "Gradient Descent(94/999): loss=11933.844830018303\n",
      "Gradient Descent(95/999): loss=11655.268070494225\n",
      "Gradient Descent(96/999): loss=11384.60804566103\n",
      "Gradient Descent(97/999): loss=11121.631757402813\n",
      "Gradient Descent(98/999): loss=10866.11315242792\n",
      "Gradient Descent(99/999): loss=10617.832912936368\n",
      "Gradient Descent(100/999): loss=10376.578253718892\n",
      "Gradient Descent(101/999): loss=10142.142725481071\n",
      "Gradient Descent(102/999): loss=9914.32602419313\n",
      "Gradient Descent(103/999): loss=9692.933806273237\n",
      "Gradient Descent(104/999): loss=9477.777509418984\n",
      "Gradient Descent(105/999): loss=9268.67417890799\n",
      "Gradient Descent(106/999): loss=9065.446299194984\n",
      "Gradient Descent(107/999): loss=8867.921630638572\n",
      "Gradient Descent(108/999): loss=8675.933051196555\n",
      "Gradient Descent(109/999): loss=8489.318402934223\n",
      "Gradient Descent(110/999): loss=8307.920343195205\n",
      "Gradient Descent(111/999): loss=8131.586200289465\n",
      "Gradient Descent(112/999): loss=7960.167833558059\n",
      "Gradient Descent(113/999): loss=7793.521497678585\n",
      "Gradient Descent(114/999): loss=7631.507711080049\n",
      "Gradient Descent(115/999): loss=7473.991128340021\n",
      "Gradient Descent(116/999): loss=7320.840416440991\n",
      "Gradient Descent(117/999): loss=7171.928134767126\n",
      "Gradient Descent(118/999): loss=7027.1306187260825\n",
      "Gradient Descent(119/999): loss=6886.327866884584\n",
      "Gradient Descent(120/999): loss=6749.403431509791\n",
      "Gradient Descent(121/999): loss=6616.24431241207\n",
      "Gradient Descent(122/999): loss=6486.740853988037\n",
      "Gradient Descent(123/999): loss=6360.786645365968\n",
      "Gradient Descent(124/999): loss=6238.278423558755\n",
      "Gradient Descent(125/999): loss=6119.115979532618\n",
      "Gradient Descent(126/999): loss=6003.2020671025675\n",
      "Gradient Descent(127/999): loss=5890.442314568555\n",
      "Gradient Descent(128/999): loss=5780.745139008774\n",
      "Gradient Descent(129/999): loss=5674.021663149337\n",
      "Gradient Descent(130/999): loss=5570.185634731947\n",
      "Gradient Descent(131/999): loss=5469.15334830368\n",
      "Gradient Descent(132/999): loss=5370.843569355391\n",
      "Gradient Descent(133/999): loss=5275.177460737386\n",
      "Gradient Descent(134/999): loss=5182.078511283424\n",
      "Gradient Descent(135/999): loss=5091.4724665760095\n",
      "Gradient Descent(136/999): loss=5003.287261788222\n",
      "Gradient Descent(137/999): loss=4917.452956539141\n",
      "Gradient Descent(138/999): loss=4833.901671702007\n",
      "Gradient Descent(139/999): loss=4752.56752810599\n",
      "Gradient Descent(140/999): loss=4673.386587074423\n",
      "Gradient Descent(141/999): loss=4596.296792743906\n",
      "Gradient Descent(142/999): loss=4521.23791611057\n",
      "Gradient Descent(143/999): loss=4448.15150075134\n",
      "Gradient Descent(144/999): loss=4376.980810169625\n",
      "Gradient Descent(145/999): loss=4307.670776716476\n",
      "Gradient Descent(146/999): loss=4240.167952039655\n",
      "Gradient Descent(147/999): loss=4174.420459014599\n",
      "Gradient Descent(148/999): loss=4110.377945112605\n",
      "Gradient Descent(149/999): loss=4047.9915371629254\n",
      "Gradient Descent(150/999): loss=3987.213797466841\n",
      "Gradient Descent(151/999): loss=3927.9986812229545\n",
      "Gradient Descent(152/999): loss=3870.30149522429\n",
      "Gradient Descent(153/999): loss=3814.078857788901\n",
      "Gradient Descent(154/999): loss=3759.288659886908\n",
      "Gradient Descent(155/999): loss=3705.8900274279663\n",
      "Gradient Descent(156/999): loss=3653.8432846742985\n",
      "Gradient Descent(157/999): loss=3603.109918745449\n",
      "Gradient Descent(158/999): loss=3553.652545181976\n",
      "Gradient Descent(159/999): loss=3505.4348745362563\n",
      "Gradient Descent(160/999): loss=3458.4216799595483\n",
      "Gradient Descent(161/999): loss=3412.578765755442\n",
      "Gradient Descent(162/999): loss=3367.872936870646\n",
      "Gradient Descent(163/999): loss=3324.271969294999\n",
      "Gradient Descent(164/999): loss=3281.744581343435\n",
      "Gradient Descent(165/999): loss=3240.260405793439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(166/999): loss=3199.789962852331\n",
      "Gradient Descent(167/999): loss=3160.30463392951\n",
      "Gradient Descent(168/999): loss=3121.7766361895406\n",
      "Gradient Descent(169/999): loss=3084.178997862635\n",
      "Gradient Descent(170/999): loss=3047.485534289902\n",
      "Gradient Descent(171/999): loss=3011.6708246813027\n",
      "Gradient Descent(172/999): loss=2976.7101895649803\n",
      "Gradient Descent(173/999): loss=2942.5796689072804\n",
      "Gradient Descent(174/999): loss=2909.2560008833525\n",
      "Gradient Descent(175/999): loss=2876.716601278893\n",
      "Gradient Descent(176/999): loss=2844.939543504118\n",
      "Gradient Descent(177/999): loss=2813.9035392016626\n",
      "Gradient Descent(178/999): loss=2783.5879194306276\n",
      "Gradient Descent(179/999): loss=2753.972616409573\n",
      "Gradient Descent(180/999): loss=2725.0381458017\n",
      "Gradient Descent(181/999): loss=2696.765589526051\n",
      "Gradient Descent(182/999): loss=2669.1365790790023\n",
      "Gradient Descent(183/999): loss=2642.133279350772\n",
      "Gradient Descent(184/999): loss=2615.7383729221897\n",
      "Gradient Descent(185/999): loss=2589.935044827358\n",
      "Gradient Descent(186/999): loss=2564.706967768302\n",
      "Gradient Descent(187/999): loss=2540.0382877681154\n",
      "Gradient Descent(188/999): loss=2515.913610249498\n",
      "Gradient Descent(189/999): loss=2492.317986526\n",
      "Gradient Descent(190/999): loss=2469.2369006936597\n",
      "Gradient Descent(191/999): loss=2446.656256911077\n",
      "Gradient Descent(192/999): loss=2424.5623670563464\n",
      "Gradient Descent(193/999): loss=2402.941938749601\n",
      "Gradient Descent(194/999): loss=2381.7820637302775\n",
      "Gradient Descent(195/999): loss=2361.070206578517\n",
      "Gradient Descent(196/999): loss=2340.794193770444\n",
      "Gradient Descent(197/999): loss=2320.942203057386\n",
      "Gradient Descent(198/999): loss=2301.5027531593655\n",
      "Gradient Descent(199/999): loss=2282.4646937635143\n",
      "Gradient Descent(200/999): loss=2263.8171958183234\n",
      "Gradient Descent(201/999): loss=2245.5497421149216\n",
      "Gradient Descent(202/999): loss=2227.6521181468324\n",
      "Gradient Descent(203/999): loss=2210.1144032399325\n",
      "Gradient Descent(204/999): loss=2192.926961944549\n",
      "Gradient Descent(205/999): loss=2176.0804356819235\n",
      "Gradient Descent(206/999): loss=2159.565734637444\n",
      "Gradient Descent(207/999): loss=2143.3740298933403\n",
      "Gradient Descent(208/999): loss=2127.496745793687\n",
      "Gradient Descent(209/999): loss=2111.9255525348326\n",
      "Gradient Descent(210/999): loss=2096.6523589745457\n",
      "Gradient Descent(211/999): loss=2081.66930565337\n",
      "Gradient Descent(212/999): loss=2066.9687580218974\n",
      "Gradient Descent(213/999): loss=2052.543299867827\n",
      "Gradient Descent(214/999): loss=2038.3857269368907\n",
      "Gradient Descent(215/999): loss=2024.4890407418702\n",
      "Gradient Descent(216/999): loss=2010.8464425541397\n",
      "Gradient Descent(217/999): loss=1997.4513275723007\n",
      "Gradient Descent(218/999): loss=1984.297279262662\n",
      "Gradient Descent(219/999): loss=1971.3780638664634\n",
      "Gradient Descent(220/999): loss=1958.6876250688915\n",
      "Gradient Descent(221/999): loss=1946.2200788250966\n",
      "Gradient Descent(222/999): loss=1933.969708338548\n",
      "Gradient Descent(223/999): loss=1921.9309591872113\n",
      "Gradient Descent(224/999): loss=1910.0984345931715\n",
      "Gradient Descent(225/999): loss=1898.4668908314443\n",
      "Gradient Descent(226/999): loss=1887.0312327738507\n",
      "Gradient Descent(227/999): loss=1875.7865095639593\n",
      "Gradient Descent(228/999): loss=1864.7279104192028\n",
      "Gradient Descent(229/999): loss=1853.8507605564205\n",
      "Gradient Descent(230/999): loss=1843.150517237151\n",
      "Gradient Descent(231/999): loss=1832.622765929141\n",
      "Gradient Descent(232/999): loss=1822.263216580642\n",
      "Gradient Descent(233/999): loss=1812.0677000041212\n",
      "Gradient Descent(234/999): loss=1802.0321643662007\n",
      "Gradient Descent(235/999): loss=1792.152671780627\n",
      "Gradient Descent(236/999): loss=1782.4253950012758\n",
      "Gradient Descent(237/999): loss=1772.8466142121933\n",
      "Gradient Descent(238/999): loss=1763.412713911829\n",
      "Gradient Descent(239/999): loss=1754.1201798886689\n",
      "Gradient Descent(240/999): loss=1744.9655962855634\n",
      "Gradient Descent(241/999): loss=1735.94564275014\n",
      "Gradient Descent(242/999): loss=1727.057091668746\n",
      "Gradient Descent(243/999): loss=1718.2968054814633\n",
      "Gradient Descent(244/999): loss=1709.6617340757998\n",
      "Gradient Descent(245/999): loss=1701.1489122567268\n",
      "Gradient Descent(246/999): loss=1692.7554572908218\n",
      "Gradient Descent(247/999): loss=1684.478566522319\n",
      "Gradient Descent(248/999): loss=1676.3155150589512\n",
      "Gradient Descent(249/999): loss=1668.2636535255253\n",
      "Gradient Descent(250/999): loss=1660.3204058832277\n",
      "Gradient Descent(251/999): loss=1652.4832673127298\n",
      "Gradient Descent(252/999): loss=1644.7498021592064\n",
      "Gradient Descent(253/999): loss=1637.1176419374403\n",
      "Gradient Descent(254/999): loss=1629.5844833952542\n",
      "Gradient Descent(255/999): loss=1622.1480866335387\n",
      "Gradient Descent(256/999): loss=1614.8062732812145\n",
      "Gradient Descent(257/999): loss=1607.5569247235105\n",
      "Gradient Descent(258/999): loss=1600.3979803819877\n",
      "Gradient Descent(259/999): loss=1593.3274360447883\n",
      "Gradient Descent(260/999): loss=1586.3433422456249\n",
      "Gradient Descent(261/999): loss=1579.4438026900868\n",
      "Gradient Descent(262/999): loss=1572.6269727278534\n",
      "Gradient Descent(263/999): loss=1565.8910578694865\n",
      "Gradient Descent(264/999): loss=1559.2343123464748\n",
      "Gradient Descent(265/999): loss=1552.655037713262\n",
      "Gradient Descent(266/999): loss=1546.1515814900229\n",
      "Gradient Descent(267/999): loss=1539.722335845007\n",
      "Gradient Descent(268/999): loss=1533.365736315252\n",
      "Gradient Descent(269/999): loss=1527.0802605645752\n",
      "Gradient Descent(270/999): loss=1520.864427177731\n",
      "Gradient Descent(271/999): loss=1514.7167944896696\n",
      "Gradient Descent(272/999): loss=1508.6359594488738\n",
      "Gradient Descent(273/999): loss=1502.6205565137748\n",
      "Gradient Descent(274/999): loss=1496.6692565812673\n",
      "Gradient Descent(275/999): loss=1490.7807659463888\n",
      "Gradient Descent(276/999): loss=1484.9538252922505\n",
      "Gradient Descent(277/999): loss=1479.1872087093236\n",
      "Gradient Descent(278/999): loss=1473.4797227432318\n",
      "Gradient Descent(279/999): loss=1467.8302054702037\n",
      "Gradient Descent(280/999): loss=1462.2375255993813\n",
      "Gradient Descent(281/999): loss=1456.7005816011974\n",
      "Gradient Descent(282/999): loss=1451.2183008610505\n",
      "Gradient Descent(283/999): loss=1445.7896388575457\n",
      "Gradient Descent(284/999): loss=1440.4135783645797\n",
      "Gradient Descent(285/999): loss=1435.0891286765657\n",
      "Gradient Descent(286/999): loss=1429.8153248561264\n",
      "Gradient Descent(287/999): loss=1424.5912270035983\n",
      "Gradient Descent(288/999): loss=1419.4159195477077\n",
      "Gradient Descent(289/999): loss=1414.2885105567964\n",
      "Gradient Descent(290/999): loss=1409.208131069995\n",
      "Gradient Descent(291/999): loss=1404.1739344477703\n",
      "Gradient Descent(292/999): loss=1399.1850957412714\n",
      "Gradient Descent(293/999): loss=1394.2408110799217\n",
      "Gradient Descent(294/999): loss=1389.3402970767368\n",
      "Gradient Descent(295/999): loss=1384.4827902508378\n",
      "Gradient Descent(296/999): loss=1379.6675464666648\n",
      "Gradient Descent(297/999): loss=1374.8938403893976\n",
      "Gradient Descent(298/999): loss=1370.1609649561185\n",
      "Gradient Descent(299/999): loss=1365.4682308622469\n",
      "Gradient Descent(300/999): loss=1360.8149660628083\n",
      "Gradient Descent(301/999): loss=1356.2005152881027\n",
      "Gradient Descent(302/999): loss=1351.6242395733507\n",
      "Gradient Descent(303/999): loss=1347.0855158019087\n",
      "Gradient Descent(304/999): loss=1342.5837362616649\n",
      "Gradient Descent(305/999): loss=1338.1183082142177\n",
      "Gradient Descent(306/999): loss=1333.6886534764828\n",
      "Gradient Descent(307/999): loss=1329.294208014347\n",
      "Gradient Descent(308/999): loss=1324.9344215480342\n",
      "Gradient Descent(309/999): loss=1320.6087571688317\n",
      "Gradient Descent(310/999): loss=1316.3166909668503\n",
      "Gradient Descent(311/999): loss=1312.0577116694988\n",
      "Gradient Descent(312/999): loss=1307.8313202903532\n",
      "Gradient Descent(313/999): loss=1303.6370297881315\n",
      "Gradient Descent(314/999): loss=1299.4743647354642\n",
      "Gradient Descent(315/999): loss=1295.3428609971866\n",
      "Gradient Descent(316/999): loss=1291.2420654178734\n",
      "Gradient Descent(317/999): loss=1287.1715355183474\n",
      "Gradient Descent(318/999): loss=1283.1308392008948\n",
      "Gradient Descent(319/999): loss=1279.1195544629477\n",
      "Gradient Descent(320/999): loss=1275.1372691189695\n",
      "Gradient Descent(321/999): loss=1271.183580530325\n",
      "Gradient Descent(322/999): loss=1267.258095342887\n",
      "Gradient Descent(323/999): loss=1263.3604292321622\n",
      "Gradient Descent(324/999): loss=1259.490206655719\n",
      "Gradient Descent(325/999): loss=1255.6470606126986\n",
      "Gradient Descent(326/999): loss=1251.830632410216\n",
      "Gradient Descent(327/999): loss=1248.0405714364347\n",
      "Gradient Descent(328/999): loss=1244.2765349401384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(329/999): loss=1240.5381878165947\n",
      "Gradient Descent(330/999): loss=1236.8252023995492\n",
      "Gradient Descent(331/999): loss=1233.1372582591455\n",
      "Gradient Descent(332/999): loss=1229.4740420056237\n",
      "Gradient Descent(333/999): loss=1225.8352470986201\n",
      "Gradient Descent(334/999): loss=1222.2205736619003\n",
      "Gradient Descent(335/999): loss=1218.629728303379\n",
      "Gradient Descent(336/999): loss=1215.0624239402675\n",
      "Gradient Descent(337/999): loss=1211.5183796292017\n",
      "Gradient Descent(338/999): loss=1207.9973204012035\n",
      "Gradient Descent(339/999): loss=1204.4989771013445\n",
      "Gradient Descent(340/999): loss=1201.0230862329645\n",
      "Gradient Descent(341/999): loss=1197.569389806323\n",
      "Gradient Descent(342/999): loss=1194.137635191548\n",
      "Gradient Descent(343/999): loss=1190.7275749757634\n",
      "Gradient Descent(344/999): loss=1187.3389668242733\n",
      "Gradient Descent(345/999): loss=1183.9715733456812\n",
      "Gradient Descent(346/999): loss=1180.625161960837\n",
      "Gradient Descent(347/999): loss=1177.2995047754982\n",
      "Gradient Descent(348/999): loss=1173.994378456598\n",
      "Gradient Descent(349/999): loss=1170.7095641120166\n",
      "Gradient Descent(350/999): loss=1167.4448471737535\n",
      "Gradient Descent(351/999): loss=1164.2000172844082\n",
      "Gradient Descent(352/999): loss=1160.9748681868625\n",
      "Gradient Descent(353/999): loss=1157.7691976170854\n",
      "Gradient Descent(354/999): loss=1154.5828071999642\n",
      "Gradient Descent(355/999): loss=1151.4155023480685\n",
      "Gradient Descent(356/999): loss=1148.2670921632762\n",
      "Gradient Descent(357/999): loss=1145.1373893411699\n",
      "Gradient Descent(358/999): loss=1142.0262100781226\n",
      "Gradient Descent(359/999): loss=1138.9333739810052\n",
      "Gradient Descent(360/999): loss=1135.8587039794247\n",
      "Gradient Descent(361/999): loss=1132.8020262404393\n",
      "Gradient Descent(362/999): loss=1129.7631700856628\n",
      "Gradient Descent(363/999): loss=1126.7419679106965\n",
      "Gradient Descent(364/999): loss=1123.7382551068274\n",
      "Gradient Descent(365/999): loss=1120.7518699849127\n",
      "Gradient Descent(366/999): loss=1117.7826537014037\n",
      "Gradient Descent(367/999): loss=1114.8304501864354\n",
      "Gradient Descent(368/999): loss=1111.8951060739355\n",
      "Gradient Descent(369/999): loss=1108.9764706336757\n",
      "Gradient Descent(370/999): loss=1106.0743957052332\n",
      "Gradient Descent(371/999): loss=1103.1887356337838\n",
      "Gradient Descent(372/999): loss=1100.3193472076985\n",
      "Gradient Descent(373/999): loss=1097.466089597864\n",
      "Gradient Descent(374/999): loss=1094.628824298709\n",
      "Gradient Descent(375/999): loss=1091.8074150708567\n",
      "Gradient Descent(376/999): loss=1089.001727885381\n",
      "Gradient Descent(377/999): loss=1086.211630869609\n",
      "Gradient Descent(378/999): loss=1083.4369942544251\n",
      "Gradient Descent(379/999): loss=1080.6776903230361\n",
      "Gradient Descent(380/999): loss=1077.9335933611612\n",
      "Gradient Descent(381/999): loss=1075.2045796085924\n",
      "Gradient Descent(382/999): loss=1072.4905272121025\n",
      "Gradient Descent(383/999): loss=1069.7913161796453\n",
      "Gradient Descent(384/999): loss=1067.1068283358302\n",
      "Gradient Descent(385/999): loss=1064.4369472786134\n",
      "Gradient Descent(386/999): loss=1061.78155833719\n",
      "Gradient Descent(387/999): loss=1059.1405485310427\n",
      "Gradient Descent(388/999): loss=1056.513806530113\n",
      "Gradient Descent(389/999): loss=1053.901222616073\n",
      "Gradient Descent(390/999): loss=1051.3026886446496\n",
      "Gradient Descent(391/999): loss=1048.718098008991\n",
      "Gradient Descent(392/999): loss=1046.1473456040276\n",
      "Gradient Descent(393/999): loss=1043.5903277918158\n",
      "Gradient Descent(394/999): loss=1041.0469423678176\n",
      "Gradient Descent(395/999): loss=1038.5170885281104\n",
      "Gradient Descent(396/999): loss=1036.0006668374822\n",
      "Gradient Descent(397/999): loss=1033.4975791983998\n",
      "Gradient Descent(398/999): loss=1031.0077288208172\n",
      "Gradient Descent(399/999): loss=1028.531020192805\n",
      "Gradient Descent(400/999): loss=1026.0673590519764\n",
      "Gradient Descent(401/999): loss=1023.616652357685\n",
      "Gradient Descent(402/999): loss=1021.1788082639752\n",
      "Gradient Descent(403/999): loss=1018.7537360932635\n",
      "Gradient Descent(404/999): loss=1016.3413463107279\n",
      "Gradient Descent(405/999): loss=1013.9415504993875\n",
      "Gradient Descent(406/999): loss=1011.5542613358526\n",
      "Gradient Descent(407/999): loss=1009.1793925667249\n",
      "Gradient Descent(408/999): loss=1006.8168589856315\n",
      "Gradient Descent(409/999): loss=1004.4665764108697\n",
      "Gradient Descent(410/999): loss=1002.1284616636561\n",
      "Gradient Descent(411/999): loss=999.80243254695\n",
      "Gradient Descent(412/999): loss=997.4884078248409\n",
      "Gradient Descent(413/999): loss=995.1863072024911\n",
      "Gradient Descent(414/999): loss=992.8960513065998\n",
      "Gradient Descent(415/999): loss=990.6175616663963\n",
      "Gradient Descent(416/999): loss=988.3507606951268\n",
      "Gradient Descent(417/999): loss=986.0955716720359\n",
      "Gradient Descent(418/999): loss=983.8519187248219\n",
      "Gradient Descent(419/999): loss=981.6197268125505\n",
      "Gradient Descent(420/999): loss=979.3989217090233\n",
      "Gradient Descent(421/999): loss=977.1894299865745\n",
      "Gradient Descent(422/999): loss=974.9911790003024\n",
      "Gradient Descent(423/999): loss=972.8040968726984\n",
      "Gradient Descent(424/999): loss=970.6281124786921\n",
      "Gradient Descent(425/999): loss=968.46315543107\n",
      "Gradient Descent(426/999): loss=966.3091560662878\n",
      "Gradient Descent(427/999): loss=964.1660454306361\n",
      "Gradient Descent(428/999): loss=962.0337552667736\n",
      "Gradient Descent(429/999): loss=959.9122180006027\n",
      "Gradient Descent(430/999): loss=957.8013667284854\n",
      "Gradient Descent(431/999): loss=955.7011352047824\n",
      "Gradient Descent(432/999): loss=953.6114578297204\n",
      "Gradient Descent(433/999): loss=951.5322696375566\n",
      "Gradient Descent(434/999): loss=949.4635062850591\n",
      "Gradient Descent(435/999): loss=947.4051040402674\n",
      "Gradient Descent(436/999): loss=945.3569997715457\n",
      "Gradient Descent(437/999): loss=943.3191309369115\n",
      "Gradient Descent(438/999): loss=941.2914355736326\n",
      "Gradient Descent(439/999): loss=939.273852288083\n",
      "Gradient Descent(440/999): loss=937.2663202458597\n",
      "Gradient Descent(441/999): loss=935.2687791621424\n",
      "Gradient Descent(442/999): loss=933.2811692922947\n",
      "Gradient Descent(443/999): loss=931.3034314226979\n",
      "Gradient Descent(444/999): loss=929.3355068618155\n",
      "Gradient Descent(445/999): loss=927.3773374314744\n",
      "Gradient Descent(446/999): loss=925.4288654583664\n",
      "Gradient Descent(447/999): loss=923.4900337657526\n",
      "Gradient Descent(448/999): loss=921.5607856653758\n",
      "Gradient Descent(449/999): loss=919.6410649495685\n",
      "Gradient Descent(450/999): loss=917.7308158835536\n",
      "Gradient Descent(451/999): loss=915.8299831979323\n",
      "Gradient Descent(452/999): loss=913.9385120813512\n",
      "Gradient Descent(453/999): loss=912.0563481733512\n",
      "Gradient Descent(454/999): loss=910.1834375573827\n",
      "Gradient Descent(455/999): loss=908.3197267539923\n",
      "Gradient Descent(456/999): loss=906.4651627141694\n",
      "Gradient Descent(457/999): loss=904.619692812852\n",
      "Gradient Descent(458/999): loss=902.7832648425866\n",
      "Gradient Descent(459/999): loss=900.9558270073372\n",
      "Gradient Descent(460/999): loss=899.1373279164385\n",
      "Gradient Descent(461/999): loss=897.3277165786949\n",
      "Gradient Descent(462/999): loss=895.5269423966109\n",
      "Gradient Descent(463/999): loss=893.7349551607616\n",
      "Gradient Descent(464/999): loss=891.9517050442886\n",
      "Gradient Descent(465/999): loss=890.1771425975253\n",
      "Gradient Descent(466/999): loss=888.4112187427463\n",
      "Gradient Descent(467/999): loss=886.6538847690332\n",
      "Gradient Descent(468/999): loss=884.905092327261\n",
      "Gradient Descent(469/999): loss=883.1647934251943\n",
      "Gradient Descent(470/999): loss=881.4329404226983\n",
      "Gradient Descent(471/999): loss=879.7094860270538\n",
      "Gradient Descent(472/999): loss=877.9943832883774\n",
      "Gradient Descent(473/999): loss=876.2875855951455\n",
      "Gradient Descent(474/999): loss=874.5890466698146\n",
      "Gradient Descent(475/999): loss=872.8987205645401\n",
      "Gradient Descent(476/999): loss=871.2165616569874\n",
      "Gradient Descent(477/999): loss=869.5425246462358\n",
      "Gradient Descent(478/999): loss=867.87656454877\n",
      "Gradient Descent(479/999): loss=866.2186366945583\n",
      "Gradient Descent(480/999): loss=864.5686967232152\n",
      "Gradient Descent(481/999): loss=862.9267005802465\n",
      "Gradient Descent(482/999): loss=861.2926045133736\n",
      "Gradient Descent(483/999): loss=859.6663650689345\n",
      "Gradient Descent(484/999): loss=858.0479390883609\n",
      "Gradient Descent(485/999): loss=856.4372837047318\n",
      "Gradient Descent(486/999): loss=854.8343563393931\n",
      "Gradient Descent(487/999): loss=853.2391146986525\n",
      "Gradient Descent(488/999): loss=851.6515167705381\n",
      "Gradient Descent(489/999): loss=850.0715208216257\n",
      "Gradient Descent(490/999): loss=848.4990853939295\n",
      "Gradient Descent(491/999): loss=846.9341693018562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(492/999): loss=845.376731629218\n",
      "Gradient Descent(493/999): loss=843.8267317263084\n",
      "Gradient Descent(494/999): loss=842.284129207034\n",
      "Gradient Descent(495/999): loss=840.7488839461016\n",
      "Gradient Descent(496/999): loss=839.2209560762632\n",
      "Gradient Descent(497/999): loss=837.7003059856122\n",
      "Gradient Descent(498/999): loss=836.1868943149315\n",
      "Gradient Descent(499/999): loss=834.6806819550955\n",
      "Gradient Descent(500/999): loss=833.1816300445174\n",
      "Gradient Descent(501/999): loss=831.689699966648\n",
      "Gradient Descent(502/999): loss=830.2048533475196\n",
      "Gradient Descent(503/999): loss=828.7270520533367\n",
      "Gradient Descent(504/999): loss=827.2562581881105\n",
      "Gradient Descent(505/999): loss=825.7924340913378\n",
      "Gradient Descent(506/999): loss=824.3355423357228\n",
      "Gradient Descent(507/999): loss=822.8855457249356\n",
      "Gradient Descent(508/999): loss=821.4424072914196\n",
      "Gradient Descent(509/999): loss=820.0060902942286\n",
      "Gradient Descent(510/999): loss=818.5765582169096\n",
      "Gradient Descent(511/999): loss=817.1537747654186\n",
      "Gradient Descent(512/999): loss=815.7377038660751\n",
      "Gradient Descent(513/999): loss=814.32830966355\n",
      "Gradient Descent(514/999): loss=812.9255565188901\n",
      "Gradient Descent(515/999): loss=811.5294090075739\n",
      "Gradient Descent(516/999): loss=810.1398319176051\n",
      "Gradient Descent(517/999): loss=808.7567902476302\n",
      "Gradient Descent(518/999): loss=807.3802492050957\n",
      "Gradient Descent(519/999): loss=806.0101742044309\n",
      "Gradient Descent(520/999): loss=804.6465308652619\n",
      "Gradient Descent(521/999): loss=803.2892850106532\n",
      "Gradient Descent(522/999): loss=801.9384026653821\n",
      "Gradient Descent(523/999): loss=800.5938500542335\n",
      "Gradient Descent(524/999): loss=799.2555936003286\n",
      "Gradient Descent(525/999): loss=797.9235999234761\n",
      "Gradient Descent(526/999): loss=796.5978358385506\n",
      "Gradient Descent(527/999): loss=795.2782683538942\n",
      "Gradient Descent(528/999): loss=793.9648646697467\n",
      "Gradient Descent(529/999): loss=792.6575921766937\n",
      "Gradient Descent(530/999): loss=791.3564184541434\n",
      "Gradient Descent(531/999): loss=790.0613112688238\n",
      "Gradient Descent(532/999): loss=788.7722385733023\n",
      "Gradient Descent(533/999): loss=787.4891685045275\n",
      "Gradient Descent(534/999): loss=786.2120693823906\n",
      "Gradient Descent(535/999): loss=784.940909708312\n",
      "Gradient Descent(536/999): loss=783.6756581638407\n",
      "Gradient Descent(537/999): loss=782.4162836092823\n",
      "Gradient Descent(538/999): loss=781.1627550823388\n",
      "Gradient Descent(539/999): loss=779.9150417967714\n",
      "Gradient Descent(540/999): loss=778.6731131410816\n",
      "Gradient Descent(541/999): loss=777.4369386772075\n",
      "Gradient Descent(542/999): loss=776.2064881392424\n",
      "Gradient Descent(543/999): loss=774.9817314321654\n",
      "Gradient Descent(544/999): loss=773.7626386305935\n",
      "Gradient Descent(545/999): loss=772.5491799775472\n",
      "Gradient Descent(546/999): loss=771.3413258832326\n",
      "Gradient Descent(547/999): loss=770.1390469238419\n",
      "Gradient Descent(548/999): loss=768.9423138403647\n",
      "Gradient Descent(549/999): loss=767.7510975374184\n",
      "Gradient Descent(550/999): loss=766.5653690820936\n",
      "Gradient Descent(551/999): loss=765.3850997028077\n",
      "Gradient Descent(552/999): loss=764.2102607881819\n",
      "Gradient Descent(553/999): loss=763.0408238859234\n",
      "Gradient Descent(554/999): loss=761.8767607017295\n",
      "Gradient Descent(555/999): loss=760.7180430981956\n",
      "Gradient Descent(556/999): loss=759.564643093745\n",
      "Gradient Descent(557/999): loss=758.4165328615645\n",
      "Gradient Descent(558/999): loss=757.2736847285586\n",
      "Gradient Descent(559/999): loss=756.1360711743085\n",
      "Gradient Descent(560/999): loss=755.0036648300511\n",
      "Gradient Descent(561/999): loss=753.8764384776639\n",
      "Gradient Descent(562/999): loss=752.7543650486627\n",
      "Gradient Descent(563/999): loss=751.6374176232127\n",
      "Gradient Descent(564/999): loss=750.5255694291473\n",
      "Gradient Descent(565/999): loss=749.4187938410003\n",
      "Gradient Descent(566/999): loss=748.3170643790463\n",
      "Gradient Descent(567/999): loss=747.2203547083552\n",
      "Gradient Descent(568/999): loss=746.1286386378517\n",
      "Gradient Descent(569/999): loss=745.0418901193888\n",
      "Gradient Descent(570/999): loss=743.9600832468296\n",
      "Gradient Descent(571/999): loss=742.8831922551389\n",
      "Gradient Descent(572/999): loss=741.8111915194827\n",
      "Gradient Descent(573/999): loss=740.7440555543395\n",
      "Gradient Descent(574/999): loss=739.6817590126207\n",
      "Gradient Descent(575/999): loss=738.6242766847945\n",
      "Gradient Descent(576/999): loss=737.5715834980265\n",
      "Gradient Descent(577/999): loss=736.523654515323\n",
      "Gradient Descent(578/999): loss=735.4804649346827\n",
      "Gradient Descent(579/999): loss=734.4419900882637\n",
      "Gradient Descent(580/999): loss=733.408205441548\n",
      "Gradient Descent(581/999): loss=732.3790865925206\n",
      "Gradient Descent(582/999): loss=731.3546092708576\n",
      "Gradient Descent(583/999): loss=730.3347493371156\n",
      "Gradient Descent(584/999): loss=729.3194827819354\n",
      "Gradient Descent(585/999): loss=728.3087857252476\n",
      "Gradient Descent(586/999): loss=727.3026344154902\n",
      "Gradient Descent(587/999): loss=726.3010052288297\n",
      "Gradient Descent(588/999): loss=725.3038746683899\n",
      "Gradient Descent(589/999): loss=724.31121936349\n",
      "Gradient Descent(590/999): loss=723.3230160688868\n",
      "Gradient Descent(591/999): loss=722.3392416640244\n",
      "Gradient Descent(592/999): loss=721.3598731522897\n",
      "Gradient Descent(593/999): loss=720.3848876602773\n",
      "Gradient Descent(594/999): loss=719.4142624370567\n",
      "Gradient Descent(595/999): loss=718.4479748534492\n",
      "Gradient Descent(596/999): loss=717.4860024013072\n",
      "Gradient Descent(597/999): loss=716.5283226928038\n",
      "Gradient Descent(598/999): loss=715.5749134597252\n",
      "Gradient Descent(599/999): loss=714.6257525527703\n",
      "Gradient Descent(600/999): loss=713.6808179408561\n",
      "Gradient Descent(601/999): loss=712.7400877104278\n",
      "Gradient Descent(602/999): loss=711.8035400647748\n",
      "Gradient Descent(603/999): loss=710.8711533233542\n",
      "Gradient Descent(604/999): loss=709.9429059211162\n",
      "Gradient Descent(605/999): loss=709.018776407838\n",
      "Gradient Descent(606/999): loss=708.0987434474607\n",
      "Gradient Descent(607/999): loss=707.1827858174333\n",
      "Gradient Descent(608/999): loss=706.2708824080596\n",
      "Gradient Descent(609/999): loss=705.3630122218525\n",
      "Gradient Descent(610/999): loss=704.4591543728915\n",
      "Gradient Descent(611/999): loss=703.559288086186\n",
      "Gradient Descent(612/999): loss=702.6633926970432\n",
      "Gradient Descent(613/999): loss=701.7714476504401\n",
      "Gradient Descent(614/999): loss=700.8834325004016\n",
      "Gradient Descent(615/999): loss=699.9993269093831\n",
      "Gradient Descent(616/999): loss=699.1191106476542\n",
      "Gradient Descent(617/999): loss=698.2427635926927\n",
      "Gradient Descent(618/999): loss=697.3702657285786\n",
      "Gradient Descent(619/999): loss=696.5015971453946\n",
      "Gradient Descent(620/999): loss=695.6367380386288\n",
      "Gradient Descent(621/999): loss=694.7756687085862\n",
      "Gradient Descent(622/999): loss=693.9183695597974\n",
      "Gradient Descent(623/999): loss=693.0648211004384\n",
      "Gradient Descent(624/999): loss=692.2150039417508\n",
      "Gradient Descent(625/999): loss=691.3688987974663\n",
      "Gradient Descent(626/999): loss=690.5264864832362\n",
      "Gradient Descent(627/999): loss=689.6877479160637\n",
      "Gradient Descent(628/999): loss=688.8526641137427\n",
      "Gradient Descent(629/999): loss=688.0212161942957\n",
      "Gradient Descent(630/999): loss=687.1933853754207\n",
      "Gradient Descent(631/999): loss=686.3691529739389\n",
      "Gradient Descent(632/999): loss=685.5485004052464\n",
      "Gradient Descent(633/999): loss=684.7314091827711\n",
      "Gradient Descent(634/999): loss=683.91786091743\n",
      "Gradient Descent(635/999): loss=683.1078373170949\n",
      "Gradient Descent(636/999): loss=682.301320186058\n",
      "Gradient Descent(637/999): loss=681.4982914245005\n",
      "Gradient Descent(638/999): loss=680.6987330279693\n",
      "Gradient Descent(639/999): loss=679.9026270868509\n",
      "Gradient Descent(640/999): loss=679.1099557858545\n",
      "Gradient Descent(641/999): loss=678.3207014034948\n",
      "Gradient Descent(642/999): loss=677.5348463115783\n",
      "Gradient Descent(643/999): loss=676.7523729746964\n",
      "Gradient Descent(644/999): loss=675.9732639497159\n",
      "Gradient Descent(645/999): loss=675.1975018852797\n",
      "Gradient Descent(646/999): loss=674.4250695213032\n",
      "Gradient Descent(647/999): loss=673.655949688481\n",
      "Gradient Descent(648/999): loss=672.8901253077917\n",
      "Gradient Descent(649/999): loss=672.127579390007\n",
      "Gradient Descent(650/999): loss=671.3682950352078\n",
      "Gradient Descent(651/999): loss=670.6122554322951\n",
      "Gradient Descent(652/999): loss=669.8594438585126\n",
      "Gradient Descent(653/999): loss=669.1098436789671\n",
      "Gradient Descent(654/999): loss=668.3634383461535\n",
      "Gradient Descent(655/999): loss=667.620211399482\n",
      "Gradient Descent(656/999): loss=666.8801464648084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(657/999): loss=666.14322725397\n",
      "Gradient Descent(658/999): loss=665.4094375643184\n",
      "Gradient Descent(659/999): loss=664.6787612782615\n",
      "Gradient Descent(660/999): loss=663.9511823628033\n",
      "Gradient Descent(661/999): loss=663.2266848690901\n",
      "Gradient Descent(662/999): loss=662.5052529319576\n",
      "Gradient Descent(663/999): loss=661.7868707694801\n",
      "Gradient Descent(664/999): loss=661.0715226825251\n",
      "Gradient Descent(665/999): loss=660.3591930543073\n",
      "Gradient Descent(666/999): loss=659.6498663499475\n",
      "Gradient Descent(667/999): loss=658.943527116033\n",
      "Gradient Descent(668/999): loss=658.2401599801823\n",
      "Gradient Descent(669/999): loss=657.5397496506089\n",
      "Gradient Descent(670/999): loss=656.8422809156923\n",
      "Gradient Descent(671/999): loss=656.1477386435467\n",
      "Gradient Descent(672/999): loss=655.4561077815966\n",
      "Gradient Descent(673/999): loss=654.7673733561519\n",
      "Gradient Descent(674/999): loss=654.081520471987\n",
      "Gradient Descent(675/999): loss=653.3985343119218\n",
      "Gradient Descent(676/999): loss=652.7184001364041\n",
      "Gradient Descent(677/999): loss=652.0411032830975\n",
      "Gradient Descent(678/999): loss=651.3666291664679\n",
      "Gradient Descent(679/999): loss=650.6949632773755\n",
      "Gradient Descent(680/999): loss=650.0260911826666\n",
      "Gradient Descent(681/999): loss=649.3599985247708\n",
      "Gradient Descent(682/999): loss=648.696671021297\n",
      "Gradient Descent(683/999): loss=648.0360944646334\n",
      "Gradient Descent(684/999): loss=647.3782547215509\n",
      "Gradient Descent(685/999): loss=646.7231377328087\n",
      "Gradient Descent(686/999): loss=646.0707295127573\n",
      "Gradient Descent(687/999): loss=645.4210161489511\n",
      "Gradient Descent(688/999): loss=644.773983801759\n",
      "Gradient Descent(689/999): loss=644.1296187039766\n",
      "Gradient Descent(690/999): loss=643.4879071604435\n",
      "Gradient Descent(691/999): loss=642.8488355476596\n",
      "Gradient Descent(692/999): loss=642.2123903134062\n",
      "Gradient Descent(693/999): loss=641.5785579763685\n",
      "Gradient Descent(694/999): loss=640.9473251257584\n",
      "Gradient Descent(695/999): loss=640.3186784209423\n",
      "Gradient Descent(696/999): loss=639.6926045910682\n",
      "Gradient Descent(697/999): loss=639.0690904346983\n",
      "Gradient Descent(698/999): loss=638.4481228194395\n",
      "Gradient Descent(699/999): loss=637.8296886815791\n",
      "Gradient Descent(700/999): loss=637.2137750257218\n",
      "Gradient Descent(701/999): loss=636.6003689244272\n",
      "Gradient Descent(702/999): loss=635.9894575178525\n",
      "Gradient Descent(703/999): loss=635.3810280133928\n",
      "Gradient Descent(704/999): loss=634.7750676853283\n",
      "Gradient Descent(705/999): loss=634.1715638744689\n",
      "Gradient Descent(706/999): loss=633.570503987804\n",
      "Gradient Descent(707/999): loss=632.9718754981532\n",
      "Gradient Descent(708/999): loss=632.3756659438181\n",
      "Gradient Descent(709/999): loss=631.7818629282366\n",
      "Gradient Descent(710/999): loss=631.1904541196398\n",
      "Gradient Descent(711/999): loss=630.6014272507106\n",
      "Gradient Descent(712/999): loss=630.014770118243\n",
      "Gradient Descent(713/999): loss=629.4304705828035\n",
      "Gradient Descent(714/999): loss=628.8485165683967\n",
      "Gradient Descent(715/999): loss=628.2688960621292\n",
      "Gradient Descent(716/999): loss=627.6915971138783\n",
      "Gradient Descent(717/999): loss=627.1166078359603\n",
      "Gradient Descent(718/999): loss=626.5439164028026\n",
      "Gradient Descent(719/999): loss=625.9735110506149\n",
      "Gradient Descent(720/999): loss=625.4053800770664\n",
      "Gradient Descent(721/999): loss=624.8395118409602\n",
      "Gradient Descent(722/999): loss=624.275894761912\n",
      "Gradient Descent(723/999): loss=623.7145173200298\n",
      "Gradient Descent(724/999): loss=623.1553680555976\n",
      "Gradient Descent(725/999): loss=622.5984355687553\n",
      "Gradient Descent(726/999): loss=622.043708519186\n",
      "Gradient Descent(727/999): loss=621.4911756258035\n",
      "Gradient Descent(728/999): loss=620.9408256664389\n",
      "Gradient Descent(729/999): loss=620.3926474775323\n",
      "Gradient Descent(730/999): loss=619.8466299538233\n",
      "Gradient Descent(731/999): loss=619.3027620480465\n",
      "Gradient Descent(732/999): loss=618.7610327706248\n",
      "Gradient Descent(733/999): loss=618.2214311893667\n",
      "Gradient Descent(734/999): loss=617.6839464291659\n",
      "Gradient Descent(735/999): loss=617.1485676716998\n",
      "Gradient Descent(736/999): loss=616.6152841551328\n",
      "Gradient Descent(737/999): loss=616.0840851738176\n",
      "Gradient Descent(738/999): loss=615.5549600780029\n",
      "Gradient Descent(739/999): loss=615.0278982735362\n",
      "Gradient Descent(740/999): loss=614.502889221576\n",
      "Gradient Descent(741/999): loss=613.9799224382975\n",
      "Gradient Descent(742/999): loss=613.4589874946072\n",
      "Gradient Descent(743/999): loss=612.9400740158534\n",
      "Gradient Descent(744/999): loss=612.4231716815412\n",
      "Gradient Descent(745/999): loss=611.9082702250481\n",
      "Gradient Descent(746/999): loss=611.3953594333417\n",
      "Gradient Descent(747/999): loss=610.8844291466986\n",
      "Gradient Descent(748/999): loss=610.3754692584239\n",
      "Gradient Descent(749/999): loss=609.8684697145741\n",
      "Gradient Descent(750/999): loss=609.3634205136797\n",
      "Gradient Descent(751/999): loss=608.8603117064697\n",
      "Gradient Descent(752/999): loss=608.3591333955992\n",
      "Gradient Descent(753/999): loss=607.8598757353756\n",
      "Gradient Descent(754/999): loss=607.362528931489\n",
      "Gradient Descent(755/999): loss=606.8670832407419\n",
      "Gradient Descent(756/999): loss=606.3735289707818\n",
      "Gradient Descent(757/999): loss=605.8818564798343\n",
      "Gradient Descent(758/999): loss=605.3920561764381\n",
      "Gradient Descent(759/999): loss=604.9041185191818\n",
      "Gradient Descent(760/999): loss=604.4180340164402\n",
      "Gradient Descent(761/999): loss=603.933793226116\n",
      "Gradient Descent(762/999): loss=603.4513867553766\n",
      "Gradient Descent(763/999): loss=602.9708052603995\n",
      "Gradient Descent(764/999): loss=602.4920394461138\n",
      "Gradient Descent(765/999): loss=602.0150800659442\n",
      "Gradient Descent(766/999): loss=601.5399179215588\n",
      "Gradient Descent(767/999): loss=601.0665438626143\n",
      "Gradient Descent(768/999): loss=600.5949487865067\n",
      "Gradient Descent(769/999): loss=600.1251236381195\n",
      "Gradient Descent(770/999): loss=599.657059409576\n",
      "Gradient Descent(771/999): loss=599.1907471399919\n",
      "Gradient Descent(772/999): loss=598.7261779152283\n",
      "Gradient Descent(773/999): loss=598.263342867648\n",
      "Gradient Descent(774/999): loss=597.8022331758715\n",
      "Gradient Descent(775/999): loss=597.3428400645349\n",
      "Gradient Descent(776/999): loss=596.885154804049\n",
      "Gradient Descent(777/999): loss=596.4291687103597\n",
      "Gradient Descent(778/999): loss=595.974873144709\n",
      "Gradient Descent(779/999): loss=595.5222595133988\n",
      "Gradient Descent(780/999): loss=595.0713192675547\n",
      "Gradient Descent(781/999): loss=594.6220439028915\n",
      "Gradient Descent(782/999): loss=594.174424959479\n",
      "Gradient Descent(783/999): loss=593.7284540215106\n",
      "Gradient Descent(784/999): loss=593.2841227170728\n",
      "Gradient Descent(785/999): loss=592.8414227179153\n",
      "Gradient Descent(786/999): loss=592.4003457392204\n",
      "Gradient Descent(787/999): loss=591.960883539379\n",
      "Gradient Descent(788/999): loss=591.5230279197629\n",
      "Gradient Descent(789/999): loss=591.0867707244988\n",
      "Gradient Descent(790/999): loss=590.6521038402474\n",
      "Gradient Descent(791/999): loss=590.2190191959779\n",
      "Gradient Descent(792/999): loss=589.7875087627485\n",
      "Gradient Descent(793/999): loss=589.3575645534859\n",
      "Gradient Descent(794/999): loss=588.9291786227652\n",
      "Gradient Descent(795/999): loss=588.5023430665933\n",
      "Gradient Descent(796/999): loss=588.0770500221923\n",
      "Gradient Descent(797/999): loss=587.6532916677825\n",
      "Gradient Descent(798/999): loss=587.2310602223687\n",
      "Gradient Descent(799/999): loss=586.8103479455278\n",
      "Gradient Descent(800/999): loss=586.3911471371946\n",
      "Gradient Descent(801/999): loss=585.9734501374523\n",
      "Gradient Descent(802/999): loss=585.5572493263222\n",
      "Gradient Descent(803/999): loss=585.1425371235545\n",
      "Gradient Descent(804/999): loss=584.7293059884204\n",
      "Gradient Descent(805/999): loss=584.3175484195056\n",
      "Gradient Descent(806/999): loss=583.9072569545058\n",
      "Gradient Descent(807/999): loss=583.4984241700199\n",
      "Gradient Descent(808/999): loss=583.0910426813485\n",
      "Gradient Descent(809/999): loss=582.6851051422901\n",
      "Gradient Descent(810/999): loss=582.2806042449417\n",
      "Gradient Descent(811/999): loss=581.8775327194963\n",
      "Gradient Descent(812/999): loss=581.4758833340447\n",
      "Gradient Descent(813/999): loss=581.0756488943786\n",
      "Gradient Descent(814/999): loss=580.6768222437909\n",
      "Gradient Descent(815/999): loss=580.2793962628817\n",
      "Gradient Descent(816/999): loss=579.8833638693624\n",
      "Gradient Descent(817/999): loss=579.4887180178616\n",
      "Gradient Descent(818/999): loss=579.0954516997328\n",
      "Gradient Descent(819/999): loss=578.7035579428609\n",
      "Gradient Descent(820/999): loss=578.3130298114734\n",
      "Gradient Descent(821/999): loss=577.9238604059485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(822/999): loss=577.536042862627\n",
      "Gradient Descent(823/999): loss=577.1495703536242\n",
      "Gradient Descent(824/999): loss=576.7644360866425\n",
      "Gradient Descent(825/999): loss=576.3806333047858\n",
      "Gradient Descent(826/999): loss=575.9981552863745\n",
      "Gradient Descent(827/999): loss=575.6169953447612\n",
      "Gradient Descent(828/999): loss=575.237146828148\n",
      "Gradient Descent(829/999): loss=574.8586031194021\n",
      "Gradient Descent(830/999): loss=574.4813576358792\n",
      "Gradient Descent(831/999): loss=574.1054038292388\n",
      "Gradient Descent(832/999): loss=573.7307351852664\n",
      "Gradient Descent(833/999): loss=573.3573452236965\n",
      "Gradient Descent(834/999): loss=572.9852274980328\n",
      "Gradient Descent(835/999): loss=572.6143755953734\n",
      "Gradient Descent(836/999): loss=572.2447831362349\n",
      "Gradient Descent(837/999): loss=571.8764437743769\n",
      "Gradient Descent(838/999): loss=571.5093511966296\n",
      "Gradient Descent(839/999): loss=571.1434991227196\n",
      "Gradient Descent(840/999): loss=570.7788813050994\n",
      "Gradient Descent(841/999): loss=570.4154915287745\n",
      "Gradient Descent(842/999): loss=570.0533236111353\n",
      "Gradient Descent(843/999): loss=569.6923714017863\n",
      "Gradient Descent(844/999): loss=569.3326287823791\n",
      "Gradient Descent(845/999): loss=568.9740896664432\n",
      "Gradient Descent(846/999): loss=568.6167479992213\n",
      "Gradient Descent(847/999): loss=568.2605977575018\n",
      "Gradient Descent(848/999): loss=567.9056329494564\n",
      "Gradient Descent(849/999): loss=567.5518476144724\n",
      "Gradient Descent(850/999): loss=567.1992358229929\n",
      "Gradient Descent(851/999): loss=566.8477916763534\n",
      "Gradient Descent(852/999): loss=566.4975093066198\n",
      "Gradient Descent(853/999): loss=566.1483828764285\n",
      "Gradient Descent(854/999): loss=565.8004065788268\n",
      "Gradient Descent(855/999): loss=565.453574637113\n",
      "Gradient Descent(856/999): loss=565.1078813046796\n",
      "Gradient Descent(857/999): loss=564.7633208648544\n",
      "Gradient Descent(858/999): loss=564.4198876307466\n",
      "Gradient Descent(859/999): loss=564.0775759450886\n",
      "Gradient Descent(860/999): loss=563.7363801800828\n",
      "Gradient Descent(861/999): loss=563.3962947372471\n",
      "Gradient Descent(862/999): loss=563.057314047261\n",
      "Gradient Descent(863/999): loss=562.719432569815\n",
      "Gradient Descent(864/999): loss=562.3826447934575\n",
      "Gradient Descent(865/999): loss=562.0469452354446\n",
      "Gradient Descent(866/999): loss=561.7123284415899\n",
      "Gradient Descent(867/999): loss=561.3787889861156\n",
      "Gradient Descent(868/999): loss=561.0463214715041\n",
      "Gradient Descent(869/999): loss=560.7149205283494\n",
      "Gradient Descent(870/999): loss=560.3845808152117\n",
      "Gradient Descent(871/999): loss=560.05529701847\n",
      "Gradient Descent(872/999): loss=559.7270638521779\n",
      "Gradient Descent(873/999): loss=559.3998760579178\n",
      "Gradient Descent(874/999): loss=559.073728404658\n",
      "Gradient Descent(875/999): loss=558.7486156886091\n",
      "Gradient Descent(876/999): loss=558.4245327330825\n",
      "Gradient Descent(877/999): loss=558.1014743883471\n",
      "Gradient Descent(878/999): loss=557.7794355314907\n",
      "Gradient Descent(879/999): loss=557.4584110662779\n",
      "Gradient Descent(880/999): loss=557.1383959230116\n",
      "Gradient Descent(881/999): loss=556.8193850583954\n",
      "Gradient Descent(882/999): loss=556.5013734553931\n",
      "Gradient Descent(883/999): loss=556.1843561230942\n",
      "Gradient Descent(884/999): loss=555.8683280965755\n",
      "Gradient Descent(885/999): loss=555.5532844367666\n",
      "Gradient Descent(886/999): loss=555.2392202303135\n",
      "Gradient Descent(887/999): loss=554.926130589446\n",
      "Gradient Descent(888/999): loss=554.6140106518425\n",
      "Gradient Descent(889/999): loss=554.3028555804977\n",
      "Gradient Descent(890/999): loss=553.9926605635907\n",
      "Gradient Descent(891/999): loss=553.6834208143524\n",
      "Gradient Descent(892/999): loss=553.3751315709361\n",
      "Gradient Descent(893/999): loss=553.0677880962855\n",
      "Gradient Descent(894/999): loss=552.7613856780068\n",
      "Gradient Descent(895/999): loss=552.4559196282398\n",
      "Gradient Descent(896/999): loss=552.1513852835275\n",
      "Gradient Descent(897/999): loss=551.8477780046918\n",
      "Gradient Descent(898/999): loss=551.5450931767042\n",
      "Gradient Descent(899/999): loss=551.2433262085618\n",
      "Gradient Descent(900/999): loss=550.9424725331588\n",
      "Gradient Descent(901/999): loss=550.6425276071654\n",
      "Gradient Descent(902/999): loss=550.3434869109003\n",
      "Gradient Descent(903/999): loss=550.0453459482093\n",
      "Gradient Descent(904/999): loss=549.7481002463411\n",
      "Gradient Descent(905/999): loss=549.4517453558259\n",
      "Gradient Descent(906/999): loss=549.1562768503535\n",
      "Gradient Descent(907/999): loss=548.8616903266523\n",
      "Gradient Descent(908/999): loss=548.5679814043687\n",
      "Gradient Descent(909/999): loss=548.2751457259475\n",
      "Gradient Descent(910/999): loss=547.9831789565138\n",
      "Gradient Descent(911/999): loss=547.6920767837531\n",
      "Gradient Descent(912/999): loss=547.4018349177935\n",
      "Gradient Descent(913/999): loss=547.11244909109\n",
      "Gradient Descent(914/999): loss=546.8239150583062\n",
      "Gradient Descent(915/999): loss=546.5362285961991\n",
      "Gradient Descent(916/999): loss=546.2493855035038\n",
      "Gradient Descent(917/999): loss=545.9633816008184\n",
      "Gradient Descent(918/999): loss=545.6782127304895\n",
      "Gradient Descent(919/999): loss=545.3938747564994\n",
      "Gradient Descent(920/999): loss=545.1103635643524\n",
      "Gradient Descent(921/999): loss=544.8276750609625\n",
      "Gradient Descent(922/999): loss=544.5458051745424\n",
      "Gradient Descent(923/999): loss=544.2647498544907\n",
      "Gradient Descent(924/999): loss=543.9845050712823\n",
      "Gradient Descent(925/999): loss=543.7050668163587\n",
      "Gradient Descent(926/999): loss=543.4264311020169\n",
      "Gradient Descent(927/999): loss=543.1485939613023\n",
      "Gradient Descent(928/999): loss=542.8715514478992\n",
      "Gradient Descent(929/999): loss=542.5952996360229\n",
      "Gradient Descent(930/999): loss=542.3198346203135\n",
      "Gradient Descent(931/999): loss=542.045152515728\n",
      "Gradient Descent(932/999): loss=541.7712494574349\n",
      "Gradient Descent(933/999): loss=541.498121600709\n",
      "Gradient Descent(934/999): loss=541.2257651208248\n",
      "Gradient Descent(935/999): loss=540.9541762129543\n",
      "Gradient Descent(936/999): loss=540.6833510920603\n",
      "Gradient Descent(937/999): loss=540.4132859927964\n",
      "Gradient Descent(938/999): loss=540.1439771694012\n",
      "Gradient Descent(939/999): loss=539.8754208955971\n",
      "Gradient Descent(940/999): loss=539.6076134644893\n",
      "Gradient Descent(941/999): loss=539.3405511884632\n",
      "Gradient Descent(942/999): loss=539.0742303990847\n",
      "Gradient Descent(943/999): loss=538.8086474469997\n",
      "Gradient Descent(944/999): loss=538.5437987018339\n",
      "Gradient Descent(945/999): loss=538.2796805520949\n",
      "Gradient Descent(946/999): loss=538.0162894050724\n",
      "Gradient Descent(947/999): loss=537.7536216867408\n",
      "Gradient Descent(948/999): loss=537.4916738416607\n",
      "Gradient Descent(949/999): loss=537.230442332884\n",
      "Gradient Descent(950/999): loss=536.9699236418543\n",
      "Gradient Descent(951/999): loss=536.710114268313\n",
      "Gradient Descent(952/999): loss=536.4510107302033\n",
      "Gradient Descent(953/999): loss=536.1926095635743\n",
      "Gradient Descent(954/999): loss=535.9349073224876\n",
      "Gradient Descent(955/999): loss=535.6779005789226\n",
      "Gradient Descent(956/999): loss=535.421585922683\n",
      "Gradient Descent(957/999): loss=535.1659599613039\n",
      "Gradient Descent(958/999): loss=534.9110193199579\n",
      "Gradient Descent(959/999): loss=534.6567606413656\n",
      "Gradient Descent(960/999): loss=534.403180585702\n",
      "Gradient Descent(961/999): loss=534.1502758305053\n",
      "Gradient Descent(962/999): loss=533.8980430705869\n",
      "Gradient Descent(963/999): loss=533.6464790179411\n",
      "Gradient Descent(964/999): loss=533.3955804016555\n",
      "Gradient Descent(965/999): loss=533.1453439678207\n",
      "Gradient Descent(966/999): loss=532.8957664794433\n",
      "Gradient Descent(967/999): loss=532.6468447163556\n",
      "Gradient Descent(968/999): loss=532.3985754751294\n",
      "Gradient Descent(969/999): loss=532.1509555689872\n",
      "Gradient Descent(970/999): loss=531.903981827716\n",
      "Gradient Descent(971/999): loss=531.6576510975812\n",
      "Gradient Descent(972/999): loss=531.4119602412391\n",
      "Gradient Descent(973/999): loss=531.1669061376527\n",
      "Gradient Descent(974/999): loss=530.9224856820067\n",
      "Gradient Descent(975/999): loss=530.6786957856202\n",
      "Gradient Descent(976/999): loss=530.4355333758666\n",
      "Gradient Descent(977/999): loss=530.1929953960861\n",
      "Gradient Descent(978/999): loss=529.9510788055038\n",
      "Gradient Descent(979/999): loss=529.7097805791472\n",
      "Gradient Descent(980/999): loss=529.4690977077627\n",
      "Gradient Descent(981/999): loss=529.2290271977341\n",
      "Gradient Descent(982/999): loss=528.9895660710009\n",
      "Gradient Descent(983/999): loss=528.7507113649767\n",
      "Gradient Descent(984/999): loss=528.5124601324687\n",
      "Gradient Descent(985/999): loss=528.2748094415972\n",
      "Gradient Descent(986/999): loss=528.0377563757154\n",
      "Gradient Descent(987/999): loss=527.801298033331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(988/999): loss=527.5654315280249\n",
      "Gradient Descent(989/999): loss=527.3301539883747\n",
      "Gradient Descent(990/999): loss=527.0954625578752\n",
      "Gradient Descent(991/999): loss=526.8613543948605\n",
      "Gradient Descent(992/999): loss=526.6278266724269\n",
      "Gradient Descent(993/999): loss=526.3948765783555\n",
      "Gradient Descent(994/999): loss=526.1625013150358\n",
      "Gradient Descent(995/999): loss=525.9306980993891\n",
      "Gradient Descent(996/999): loss=525.6994641627928\n",
      "Gradient Descent(997/999): loss=525.4687967510051\n",
      "Gradient Descent(998/999): loss=525.2386931240895\n",
      "Gradient Descent(999/999): loss=525.0091505563403\n",
      "Gradient Descent: execution time=198.645 seconds\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 1e-7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.uniform(size=30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=sgd_ws[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "DER_mass_MMC = tX_test[:,0]\n",
    "DER_mass_transverse_met_lep=tX_test[:,1]\n",
    "DER_mass_vis = tX_test[:,2]\n",
    "DER_pt_h = tX_test[:,3]\n",
    "DER_deltaeta_jet_jet = tX_test[:,4]\n",
    "DER_mass_jet_jet = tX_test[:,5]\n",
    "DER_prodeta_jet_jet = tX_test[:,6]\n",
    "DER_deltar_tau_lep = tX_test[:,7]\n",
    "DER_pt_tot = tX_test[:,8]\n",
    "DER_sum_pt = tX_test[:,9]\n",
    "DER_pt_ratio_lep_tau = tX_test[:,10]\n",
    "DER_met_phi_centrality = tX_test[:,11]\n",
    "DER_lep_eta_centrality = tX_test[:,12]\n",
    "PRI_tau_pt = tX_test[:,13]\n",
    "PRI_tau_eta = tX_test[:,14]\n",
    "PRI_tau_phi = tX_test[:,15]\n",
    "PRI_lep_pt = tX_test[:,16]\n",
    "PRI_lep_eta =tX_test[:,17]\n",
    "PRI_lep_phi = tX_test[:,18]\n",
    "PRI_met=tX_test[:,19]\n",
    "PRI_met_phi = tX_test[:,20]\n",
    "PRI_met_sumet = tX_test[:,21]\n",
    "PRI_jet_num=tX_test[:,22]\n",
    "PRI_jet_leading_pt=tX_test[:,23]\n",
    "PRI_jet_leading_eta = tX_test[:,24]\n",
    "PRI_jet_leading_phi = tX_test[:,25]\n",
    "PRI_jet_subleading_pt=tX_test[:,26]\n",
    "PRI_jet_subleading_eta = tX_test[:,27]\n",
    "PRI_jet_subleading_phi = tX_test[:,28]\n",
    "PRI_jet_all_pt = tX_test[:,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "#generate px,py,pz and append to the tX\n",
    "#tau\n",
    "p_tau_x = p_x(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_y = p_y(PRI_tau_pt,PRI_tau_phi)\n",
    "p_tau_z = p_z(PRI_tau_pt,PRI_tau_eta)\n",
    "#lep\n",
    "p_lep_x = p_x(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_y = p_y(PRI_lep_pt,PRI_lep_phi)\n",
    "p_lep_z = p_z(PRI_lep_pt,PRI_lep_eta)\n",
    "#met\n",
    "p_met_x = p_x(PRI_met,PRI_met_phi)\n",
    "p_met_y = p_y(PRI_met,PRI_met_phi)\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate energy of particle\n",
    "p_tau_energy = particle_energy(p_tau_x,p_tau_y,p_tau_z)\n",
    "p_lep_energy = particle_energy(p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([p_tau_energy,p_lep_energy]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = tX_test.shape[0]\n",
    "#vector product\n",
    "#tau-lep\n",
    "\n",
    "tau_lep_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "tau_lep_cosine_similarity = cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_lep_x,p_lep_y,p_lep_z)\n",
    "\n",
    "#tau-met\n",
    "tau_met_dot = dot_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cross_x,tau_met_cross_y,tau_met_cross_z = cross_product(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "tau_met_cosine_similarity =cosine_similarity(p_tau_x,p_tau_y,p_tau_z,p_met_x,p_met_y,np.zeros(N))\n",
    "#lep-met\n",
    "lep_met_dot = dot_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cross_x,lep_met_cross_y,lep_met_cross_z = cross_product(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "lep_met_cosine_similarity =cosine_similarity(p_lep_x,p_lep_y,p_lep_z,p_met_x,p_met_y,np.zeros(N))\n",
    "\n",
    "#append features\n",
    "tX_test=np.append(tX_test,np.array([tau_lep_dot,tau_lep_cross_x,tau_lep_cross_y,tau_lep_cross_z,tau_lep_cosine_similarity]).T,axis=1)\n",
    "tX_test=np.append(tX_test,np.array([tau_met_dot,tau_met_cross_x,tau_met_cross_y,tau_met_cross_z,tau_met_cosine_similarity]).T,axis=1)\n",
    "tX_test=np.append(tX_test,np.array([lep_met_dot,lep_met_cross_x,lep_met_cross_y,lep_met_cross_z,lep_met_cosine_similarity]).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRI_jet_num_test = tX_test[:,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide test\n",
    "\n",
    "ids_test_0 = ids_test[PRI_jet_num_test==0]\n",
    "ids_test_1 = ids_test[PRI_jet_num_test==1]\n",
    "ids_test_2 = ids_test[PRI_jet_num_test==2]\n",
    "ids_test_3 = ids_test[PRI_jet_num_test==3]\n",
    "\n",
    "tX_test_0 = np.copy(tX_test[PRI_jet_num_test==0,:])\n",
    "tX_test_1 = np.copy(tX_test[PRI_jet_num_test==1,:])\n",
    "tX_test_2 = np.copy(tX_test[PRI_jet_num_test==2,:])\n",
    "tX_test_3 = np.copy(tX_test[PRI_jet_num_test==3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX_test_0=np.delete(tX_test_0,[4,5,6,12,22,23,24,25,26,27,28,29],1)\n",
    "# tX_test_1=np.delete(tX_test_1,[4,5,6,12,22,26,27,28],1)\n",
    "# tX_test_2=np.delete(tX_test_2,[22],1)\n",
    "# tX_test_3=np.delete(tX_test_3,[22],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "num_test_dim_0 = tX_test_0.shape[1]\n",
    "num_test_dim_1 = tX_test_1.shape[1]\n",
    "num_test_dim_2_3 = tX_test_2.shape[1]\n",
    "for t in range(num_test_dim_0):\n",
    "    tX_test_0[:,t] = (tX_test_0[:,t]-np.mean(tX_test_0[:,t]))/(np.std(tX_test_0[:,t]))\n",
    "\n",
    "for t in range(num_test_dim_1):\n",
    "    tX_test_1[:,t] = (tX_test_1[:,t]-np.mean(tX_test_1[:,t]))/(np.std(tX_test_1[:,t]))    \n",
    "\n",
    "for t in range(num_test_dim_2_3):\n",
    "    tX_test_2[:,t] = (tX_test_2[:,t]-np.mean(tX_test_2[:,t]))/(np.std(tX_test_2[:,t]))\n",
    "    tX_test_3[:,t] = (tX_test_3[:,t]-np.mean(tX_test_3[:,t]))/(np.std(tX_test_3[:,t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568238"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_test_0)+len(ids_test_1)+len(ids_test_2)+len(ids_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\submission_test1.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred_0 = predict_labels(weight_0, tX_test_0)\n",
    "y_pred_1 = predict_labels(weight_1, tX_test_1)\n",
    "y_pred_2 = predict_labels(weight_2, tX_test_2)\n",
    "y_pred_3 = predict_labels(weight_3, tX_test_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine y_pred\n",
    "y_pred  = np.zeros(len(ids_test))\n",
    "y_pred[PRI_jet_num_test==0] = y_pred_0\n",
    "y_pred[PRI_jet_num_test==1] = y_pred_1\n",
    "y_pred[PRI_jet_num_test==2] = y_pred_2\n",
    "y_pred[PRI_jet_num_test==3] = y_pred_3\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
