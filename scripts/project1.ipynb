{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define index of labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30), (250000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape,tX.shape,ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 51.655,  68.768, 162.172, ...,  60.526,  19.362,  72.756])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Transverse Mass \n",
    "def transverse_mass(a_t,a_phi,b_t,b_phi):\n",
    "    \n",
    "    \n",
    "    mass = np.sqrt((a_t+b_t)**2-(a_t*np.cos(a_phi)+b_t*np.cos(b_phi))**2-(a_t*np.sin(a_phi)+b_t*np.sin(b_phi))**2)\n",
    "    \n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 51.66192009  68.78329787 162.174512   ...  60.52079714  19.36191784\n",
      "  72.75137486]\n",
      "[ 51.655  68.768 162.172 ...  60.526  19.362  72.756]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DEVELOPMENT\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#transverse_mass test\n",
    "print(transverse_mass(tX[:,19],tX[:,20],tX[:,16],tX[:,18]))\n",
    "print(tX[:,1])\n",
    "print((transverse_mass(tX[:,19],tX[:,20],tX[:,16],tX[:,18])-tX[:,1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Invariant mass\n",
    "def invariant_mass(a_t,a_eta,a_phi,b_t,b_eta,b_phi):\n",
    "    a_z = a_t*np.sinh(a_eta)\n",
    "    b_z = b_t*np.sinh(b_eta)\n",
    "    \n",
    "    a_xyz = np.sqrt(a_t**2+(a_z)**2)\n",
    "    b_xyz = np.sqrt(b_t**2+(b_z)**2)\n",
    "    ab_x = a_t*np.cos(a_phi)+b_t*np.cos(b_phi)\n",
    "    ab_y =a_t*np.sin(a_phi)+b_t*np.sin(b_phi)\n",
    "    ab_z =a_z+b_z\n",
    "    \n",
    "    \n",
    "    mass = np.sqrt((a_xyz+b_xyz)**2-(ab_x)**2-(ab_y)**2-(ab_z)**2)\n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97.80554839 103.22216251 125.92230193 ...  75.82162756  68.79599297\n",
      "  70.82931157]\n",
      "[ 97.827 103.235 125.953 ...  75.839  68.812  70.831]\n",
      "-0.01725799703735722\n"
     ]
    }
   ],
   "source": [
    "#invariant mass test\n",
    "print(invariant_mass(tX[:,13],tX[:,14],tX[:,15],tX[:,16],tX[:,17],tX[:,18]))\n",
    "print(tX[:,2])\n",
    "print((invariant_mass(tX[:,13],tX[:,14],tX[:,15],tX[:,16],tX[:,17],tX[:,18])-tX[:,2]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Modulus of vecotr sum\n",
    "def modulus_vector(a_t,a_phi,b_t,b_phi,c_met,c_phi):\n",
    "    \n",
    "    x = a_t*np.cos(a_phi)+b_t*np.cos(b_phi)+c_met*np.cos(c_phi)\n",
    "    y = a_t*np.sin(a_phi)+b_t*np.sin(b_phi)+c_met*np.sin(c_phi)\n",
    "    \n",
    "    \n",
    "    p_t = np.sqrt(x**2+y**2)\n",
    "    \n",
    "    modulus = p_t*1.0\n",
    "    return modulus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.94739991 48.12297042 35.62593902 ... 39.78224526 13.5098374\n",
      "  7.47152233]\n",
      "[27.98  48.146 35.635 ... 39.757 13.504  7.479]\n"
     ]
    }
   ],
   "source": [
    "#modulus of the vector sum test\n",
    "print(modulus_vector(tX[:,13],tX[:,15],tX[:,16],tX[:,18],tX[:,19],tX[:,20]))\n",
    "print(tX[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Pseudorapidity Separation\n",
    "def pseudorapidity_separation(jet_num,leading_eta,subleading_eta):\n",
    "    \n",
    "    \n",
    "    N = len(leading_eta)\n",
    "    sep = np.zeros(N)\n",
    "    for t in range(N):\n",
    "        if(jet_num[t]<=1):\n",
    "            sep[t] = -999\n",
    "        else:\n",
    "            sep[t] = np.abs(leading_eta[t] - subleading_eta[t])\n",
    "    return sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.10e-01 -9.99e+02 -9.99e+02 ... -9.99e+02 -9.99e+02 -9.99e+02]\n",
      "[ 9.10e-01 -9.99e+02 -9.99e+02 ... -9.99e+02 -9.99e+02 -9.99e+02]\n",
      "3.0000000000019655e-07\n"
     ]
    }
   ],
   "source": [
    "#P Separation Test\n",
    "print(pseudorapidity_separation(tX[:,22],tX[:,24],tX[:,27]))\n",
    "print(tX[:,4])\n",
    "print((pseudorapidity_separation(tX[:,22],tX[:,24],tX[:,27])-tX[:,4]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to derive feature: DER_mass_transverse_met_lep from other features\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/299): loss=0.44621888821528416\n",
      "SGD(1/299): loss=0.43783224666640025\n",
      "SGD(2/299): loss=0.43708833178048295\n",
      "SGD(3/299): loss=0.4360985526326618\n",
      "SGD(4/299): loss=0.435738410700349\n",
      "SGD(5/299): loss=0.43708527307325534\n",
      "SGD(6/299): loss=0.4353847931871214\n",
      "SGD(7/299): loss=0.4347990636696359\n",
      "SGD(8/299): loss=0.4344113955200877\n",
      "SGD(9/299): loss=0.43329535901501054\n",
      "SGD(10/299): loss=0.43387755449547905\n",
      "SGD(11/299): loss=0.4356372099609092\n",
      "SGD(12/299): loss=0.4317364524955887\n",
      "SGD(13/299): loss=0.4336737680583843\n",
      "SGD(14/299): loss=0.43021140466328617\n",
      "SGD(15/299): loss=0.4301531725329095\n",
      "SGD(16/299): loss=0.4315067867751375\n",
      "SGD(17/299): loss=0.4303715027365334\n",
      "SGD(18/299): loss=0.4287066330906592\n",
      "SGD(19/299): loss=0.428401620093317\n",
      "SGD(20/299): loss=0.43032374547737773\n",
      "SGD(21/299): loss=0.429453811023959\n",
      "SGD(22/299): loss=0.42823430438253995\n",
      "SGD(23/299): loss=0.4281198262742519\n",
      "SGD(24/299): loss=0.42867962580858476\n",
      "SGD(25/299): loss=0.42694994727834584\n",
      "SGD(26/299): loss=0.4271211390104495\n",
      "SGD(27/299): loss=0.4290407513674656\n",
      "SGD(28/299): loss=0.4277467335448705\n",
      "SGD(29/299): loss=0.4272715601358728\n",
      "SGD(30/299): loss=0.42835672983524464\n",
      "SGD(31/299): loss=0.4250348699921298\n",
      "SGD(32/299): loss=0.4257980144800066\n",
      "SGD(33/299): loss=0.4273324571628496\n",
      "SGD(34/299): loss=0.43450183627309363\n",
      "SGD(35/299): loss=0.4303788842606449\n",
      "SGD(36/299): loss=0.42913009956668347\n",
      "SGD(37/299): loss=0.42352448420127287\n",
      "SGD(38/299): loss=0.4243144015884589\n",
      "SGD(39/299): loss=0.4285890282749139\n",
      "SGD(40/299): loss=0.4252297542589814\n",
      "SGD(41/299): loss=0.42368528352601764\n",
      "SGD(42/299): loss=0.42301658757784083\n",
      "SGD(43/299): loss=0.42595977928478684\n",
      "SGD(44/299): loss=0.4230083771976245\n",
      "SGD(45/299): loss=0.42353302362821016\n",
      "SGD(46/299): loss=0.42493802008475046\n",
      "SGD(47/299): loss=0.4219847826581342\n",
      "SGD(48/299): loss=0.4254144177935606\n",
      "SGD(49/299): loss=0.4240322702578285\n",
      "SGD(50/299): loss=0.422519523560869\n",
      "SGD(51/299): loss=0.42504625513475325\n",
      "SGD(52/299): loss=0.4215452139169916\n",
      "SGD(53/299): loss=0.4212302602404231\n",
      "SGD(54/299): loss=0.4216829973169438\n",
      "SGD(55/299): loss=0.4248809245883053\n",
      "SGD(56/299): loss=0.4217189716630347\n",
      "SGD(57/299): loss=0.4202087345617099\n",
      "SGD(58/299): loss=0.4221734707292413\n",
      "SGD(59/299): loss=0.4225400394141411\n",
      "SGD(60/299): loss=0.4204022023906296\n",
      "SGD(61/299): loss=0.42248050846335083\n",
      "SGD(62/299): loss=0.42251298320180924\n",
      "SGD(63/299): loss=0.42208948008091257\n",
      "SGD(64/299): loss=0.4218546848808165\n",
      "SGD(65/299): loss=0.4198816384747932\n",
      "SGD(66/299): loss=0.4199713477046395\n",
      "SGD(67/299): loss=0.42010839848799\n",
      "SGD(68/299): loss=0.42064979379117423\n",
      "SGD(69/299): loss=0.42183544797321587\n",
      "SGD(70/299): loss=0.42169665177141086\n",
      "SGD(71/299): loss=0.4198062003143475\n",
      "SGD(72/299): loss=0.4215326773326818\n",
      "SGD(73/299): loss=0.42216343364925085\n",
      "SGD(74/299): loss=0.4199813248044708\n",
      "SGD(75/299): loss=0.4281151812820072\n",
      "SGD(76/299): loss=0.42880434429461134\n",
      "SGD(77/299): loss=0.4208617281825627\n",
      "SGD(78/299): loss=0.418984018024888\n",
      "SGD(79/299): loss=0.42221989106610114\n",
      "SGD(80/299): loss=0.4187911253212063\n",
      "SGD(81/299): loss=0.4198490256503823\n",
      "SGD(82/299): loss=0.418364969885256\n",
      "SGD(83/299): loss=0.4190391878410787\n",
      "SGD(84/299): loss=0.41770189032342214\n",
      "SGD(85/299): loss=0.4186129828014725\n",
      "SGD(86/299): loss=0.4175213517182176\n",
      "SGD(87/299): loss=0.4171187637465298\n",
      "SGD(88/299): loss=0.41854212942394875\n",
      "SGD(89/299): loss=0.422450595246827\n",
      "SGD(90/299): loss=0.41823395988081263\n",
      "SGD(91/299): loss=0.4172691366328335\n",
      "SGD(92/299): loss=0.41786552244913056\n",
      "SGD(93/299): loss=0.42544983224492094\n",
      "SGD(94/299): loss=0.42022871486711244\n",
      "SGD(95/299): loss=0.4222977758105668\n",
      "SGD(96/299): loss=0.419381855165015\n",
      "SGD(97/299): loss=0.4163642419049024\n",
      "SGD(98/299): loss=0.417563333831719\n",
      "SGD(99/299): loss=0.4159968723077172\n",
      "SGD(100/299): loss=0.41664720977597935\n",
      "SGD(101/299): loss=0.4157405252733255\n",
      "SGD(102/299): loss=0.41649486581199596\n",
      "SGD(103/299): loss=0.4169191136255773\n",
      "SGD(104/299): loss=0.4174829371568409\n",
      "SGD(105/299): loss=0.4153305728691699\n",
      "SGD(106/299): loss=0.4170313438623861\n",
      "SGD(107/299): loss=0.41704015441929304\n",
      "SGD(108/299): loss=0.4152216939657936\n",
      "SGD(109/299): loss=0.4153970626791738\n",
      "SGD(110/299): loss=0.4152009451097221\n",
      "SGD(111/299): loss=0.41517858151732023\n",
      "SGD(112/299): loss=0.4153012268114355\n",
      "SGD(113/299): loss=0.4160129187507923\n",
      "SGD(114/299): loss=0.4215987777357102\n",
      "SGD(115/299): loss=0.4175589314505427\n",
      "SGD(116/299): loss=0.4175527314088999\n",
      "SGD(117/299): loss=0.42144767358410784\n",
      "SGD(118/299): loss=0.414592824489471\n",
      "SGD(119/299): loss=0.41613278410990995\n",
      "SGD(120/299): loss=0.4145962987504273\n",
      "SGD(121/299): loss=0.41491966686457066\n",
      "SGD(122/299): loss=0.41995797942938756\n",
      "SGD(123/299): loss=0.4184193358030184\n",
      "SGD(124/299): loss=0.4148068922663818\n",
      "SGD(125/299): loss=0.414336803920595\n",
      "SGD(126/299): loss=0.4158682682171144\n",
      "SGD(127/299): loss=0.42341895752806474\n",
      "SGD(128/299): loss=0.41890029437179715\n",
      "SGD(129/299): loss=0.4198399147350017\n",
      "SGD(130/299): loss=0.4164129890503997\n",
      "SGD(131/299): loss=0.41535475137399724\n",
      "SGD(132/299): loss=0.4142720587867926\n",
      "SGD(133/299): loss=0.41414813243055487\n",
      "SGD(134/299): loss=0.4146901777227405\n",
      "SGD(135/299): loss=0.4167465735048134\n",
      "SGD(136/299): loss=0.41482732247328735\n",
      "SGD(137/299): loss=0.4140951637952327\n",
      "SGD(138/299): loss=0.41418280333431623\n",
      "SGD(139/299): loss=0.41398808151772576\n",
      "SGD(140/299): loss=0.4141512190396543\n",
      "SGD(141/299): loss=0.4158815327758819\n",
      "SGD(142/299): loss=0.4157092438012619\n",
      "SGD(143/299): loss=0.4137994927122706\n",
      "SGD(144/299): loss=0.41384323895314656\n",
      "SGD(145/299): loss=0.41468276997524\n",
      "SGD(146/299): loss=0.4161402551603905\n",
      "SGD(147/299): loss=0.41399085671482583\n",
      "SGD(148/299): loss=0.4140967965227969\n",
      "SGD(149/299): loss=0.4175434738765805\n",
      "SGD(150/299): loss=0.41432134526412606\n",
      "SGD(151/299): loss=0.41523310315660344\n",
      "SGD(152/299): loss=0.41400258290551073\n",
      "SGD(153/299): loss=0.4159358432140364\n",
      "SGD(154/299): loss=0.417558402685357\n",
      "SGD(155/299): loss=0.41476488910699033\n",
      "SGD(156/299): loss=0.4135886086600054\n",
      "SGD(157/299): loss=0.41595021627389045\n",
      "SGD(158/299): loss=0.4160989837266768\n",
      "SGD(159/299): loss=0.4137468453483117\n",
      "SGD(160/299): loss=0.4153839065907422\n",
      "SGD(161/299): loss=0.4211686137764999\n",
      "SGD(162/299): loss=0.41371033344811686\n",
      "SGD(163/299): loss=0.41271629273308474\n",
      "SGD(164/299): loss=0.4150406558604925\n",
      "SGD(165/299): loss=0.41275565394973646\n",
      "SGD(166/299): loss=0.41381162734358595\n",
      "SGD(167/299): loss=0.4127783607632335\n",
      "SGD(168/299): loss=0.41598005664608273\n",
      "SGD(169/299): loss=0.412883576939984\n",
      "SGD(170/299): loss=0.41286557739469754\n",
      "SGD(171/299): loss=0.4123394164291924\n",
      "SGD(172/299): loss=0.4128811532502452\n",
      "SGD(173/299): loss=0.4125633222797228\n",
      "SGD(174/299): loss=0.41238811764595523\n",
      "SGD(175/299): loss=0.41331794996096954\n",
      "SGD(176/299): loss=0.41291307994243165\n",
      "SGD(177/299): loss=0.41733572947304376\n",
      "SGD(178/299): loss=0.4124625803084332\n",
      "SGD(179/299): loss=0.4121579155271097\n",
      "SGD(180/299): loss=0.41546339420564826\n",
      "SGD(181/299): loss=0.4121115893960868\n",
      "SGD(182/299): loss=0.41258317893837176\n",
      "SGD(183/299): loss=0.41212856267994685\n",
      "SGD(184/299): loss=0.41438570736396974\n",
      "SGD(185/299): loss=0.41582105466089603\n",
      "SGD(186/299): loss=0.41210677544294716\n",
      "SGD(187/299): loss=0.41172231815424004\n",
      "SGD(188/299): loss=0.41233611209748\n",
      "SGD(189/299): loss=0.41173893651422355\n",
      "SGD(190/299): loss=0.4140258846598713\n",
      "SGD(191/299): loss=0.4121046681148108\n",
      "SGD(192/299): loss=0.411662172279937\n",
      "SGD(193/299): loss=0.41660046327942235\n",
      "SGD(194/299): loss=0.41489111504248444\n",
      "SGD(195/299): loss=0.41637727320005874\n",
      "SGD(196/299): loss=0.4153870842225945\n",
      "SGD(197/299): loss=0.4122049757567837\n",
      "SGD(198/299): loss=0.41197512585287765\n",
      "SGD(199/299): loss=0.4127385646002949\n",
      "SGD(200/299): loss=0.4120312380983169\n",
      "SGD(201/299): loss=0.413309307501672\n",
      "SGD(202/299): loss=0.4116791421897203\n",
      "SGD(203/299): loss=0.41415262135007136\n",
      "SGD(204/299): loss=0.4123632710700249\n",
      "SGD(205/299): loss=0.4116723893971292\n",
      "SGD(206/299): loss=0.4118150271669968\n",
      "SGD(207/299): loss=0.4115369138951425\n",
      "SGD(208/299): loss=0.4191567838824262\n",
      "SGD(209/299): loss=0.41421853477529297\n",
      "SGD(210/299): loss=0.4129481792167179\n",
      "SGD(211/299): loss=0.4114792640328963\n",
      "SGD(212/299): loss=0.4113909570496945\n",
      "SGD(213/299): loss=0.4120652135371095\n",
      "SGD(214/299): loss=0.4120039967445837\n",
      "SGD(215/299): loss=0.41291957732628776\n",
      "SGD(216/299): loss=0.4148864422807461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(217/299): loss=0.42533752080866283\n",
      "SGD(218/299): loss=0.4170397297314473\n",
      "SGD(219/299): loss=0.41453656225551877\n",
      "SGD(220/299): loss=0.4135158660007663\n",
      "SGD(221/299): loss=0.41133916007055554\n",
      "SGD(222/299): loss=0.4109925970214171\n",
      "SGD(223/299): loss=0.41148019662322277\n",
      "SGD(224/299): loss=0.41129581023906353\n",
      "SGD(225/299): loss=0.4114942564654783\n",
      "SGD(226/299): loss=0.4119920920419788\n",
      "SGD(227/299): loss=0.4106285980524343\n",
      "SGD(228/299): loss=0.41194354956836304\n",
      "SGD(229/299): loss=0.41053162167351603\n",
      "SGD(230/299): loss=0.41152179117251786\n",
      "SGD(231/299): loss=0.41159968808083397\n",
      "SGD(232/299): loss=0.4111419078782423\n",
      "SGD(233/299): loss=0.4118036082437149\n",
      "SGD(234/299): loss=0.4124045708325791\n",
      "SGD(235/299): loss=0.4109886184174132\n",
      "SGD(236/299): loss=0.41116069293196633\n",
      "SGD(237/299): loss=0.4104425117780421\n",
      "SGD(238/299): loss=0.41018485663708854\n",
      "SGD(239/299): loss=0.4100682349692056\n",
      "SGD(240/299): loss=0.4105936086021653\n",
      "SGD(241/299): loss=0.4109621611355064\n",
      "SGD(242/299): loss=0.41262880607035607\n",
      "SGD(243/299): loss=0.4117265694052887\n",
      "SGD(244/299): loss=0.4162107440129331\n",
      "SGD(245/299): loss=0.4154596829512202\n",
      "SGD(246/299): loss=0.41004742232601693\n",
      "SGD(247/299): loss=0.41090355330665895\n",
      "SGD(248/299): loss=0.41267699153083076\n",
      "SGD(249/299): loss=0.41047371799853966\n",
      "SGD(250/299): loss=0.4157752584860947\n",
      "SGD(251/299): loss=0.4154309449418102\n",
      "SGD(252/299): loss=0.4123144440625936\n",
      "SGD(253/299): loss=0.410261331107469\n",
      "SGD(254/299): loss=0.4100581672890742\n",
      "SGD(255/299): loss=0.40975739846981374\n",
      "SGD(256/299): loss=0.4105167825248544\n",
      "SGD(257/299): loss=0.4124310264769268\n",
      "SGD(258/299): loss=0.41265158741422125\n",
      "SGD(259/299): loss=0.4156934205417342\n",
      "SGD(260/299): loss=0.4107244887275373\n",
      "SGD(261/299): loss=0.40983563881343693\n",
      "SGD(262/299): loss=0.4117185769726034\n",
      "SGD(263/299): loss=0.4102612963508385\n",
      "SGD(264/299): loss=0.4094340898815178\n",
      "SGD(265/299): loss=0.409470402960206\n",
      "SGD(266/299): loss=0.4145566355862431\n",
      "SGD(267/299): loss=0.4180851345434532\n",
      "SGD(268/299): loss=0.4099380755868234\n",
      "SGD(269/299): loss=0.41187668956598\n",
      "SGD(270/299): loss=0.40930248325452745\n",
      "SGD(271/299): loss=0.4099489119923279\n",
      "SGD(272/299): loss=0.41074895061842975\n",
      "SGD(273/299): loss=0.4108032843201223\n",
      "SGD(274/299): loss=0.4102752473853258\n",
      "SGD(275/299): loss=0.4108977803666094\n",
      "SGD(276/299): loss=0.4096221492822392\n",
      "SGD(277/299): loss=0.4115594689605662\n",
      "SGD(278/299): loss=0.41975867265322925\n",
      "SGD(279/299): loss=0.4098620510091158\n",
      "SGD(280/299): loss=0.40919982657612797\n",
      "SGD(281/299): loss=0.4103834439238467\n",
      "SGD(282/299): loss=0.40948429242479567\n",
      "SGD(283/299): loss=0.4106661401364536\n",
      "SGD(284/299): loss=0.4124740878789518\n",
      "SGD(285/299): loss=0.41057580469064464\n",
      "SGD(286/299): loss=0.4096251134315418\n",
      "SGD(287/299): loss=0.4138408165568956\n",
      "SGD(288/299): loss=0.4100288882987929\n",
      "SGD(289/299): loss=0.4100688217985241\n",
      "SGD(290/299): loss=0.4089186840680498\n",
      "SGD(291/299): loss=0.41005745286125156\n",
      "SGD(292/299): loss=0.4089314692049154\n",
      "SGD(293/299): loss=0.40914347304332505\n",
      "SGD(294/299): loss=0.40895573851735745\n",
      "SGD(295/299): loss=0.4089848599843984\n",
      "SGD(296/299): loss=0.4088572992072301\n",
      "SGD(297/299): loss=0.410777535118868\n",
      "SGD(298/299): loss=0.40909322976212215\n",
      "SGD(299/299): loss=0.41025420341775065\n",
      "SGD: execution time=50.366 seconds\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 300\n",
    "gamma = 1e-7\n",
    "batch_size = 100\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=4026597.208806231\n",
      "Gradient Descent(1/999): loss=858320.896280879\n",
      "Gradient Descent(2/999): loss=278103.8668724276\n",
      "Gradient Descent(3/999): loss=168869.76711567058\n",
      "Gradient Descent(4/999): loss=145445.58778938354\n",
      "Gradient Descent(5/999): loss=137750.43407121304\n",
      "Gradient Descent(6/999): loss=133029.59443376292\n",
      "Gradient Descent(7/999): loss=128958.17531372968\n",
      "Gradient Descent(8/999): loss=125109.04764643247\n",
      "Gradient Descent(9/999): loss=121400.56154920557\n",
      "Gradient Descent(10/999): loss=117814.1184050088\n",
      "Gradient Descent(11/999): loss=114342.77422178564\n",
      "Gradient Descent(12/999): loss=110981.87652578918\n",
      "Gradient Descent(13/999): loss=107727.34960038488\n",
      "Gradient Descent(14/999): loss=104575.37214741267\n",
      "Gradient Descent(15/999): loss=101522.30912237997\n",
      "Gradient Descent(16/999): loss=98564.69058087931\n",
      "Gradient Descent(17/999): loss=95699.19977692603\n",
      "Gradient Descent(18/999): loss=92922.66358064636\n",
      "Gradient Descent(19/999): loss=90232.04389708361\n",
      "Gradient Descent(20/999): loss=87624.42979467967\n",
      "Gradient Descent(21/999): loss=85097.03024363433\n",
      "Gradient Descent(22/999): loss=82647.1674035891\n",
      "Gradient Descent(23/999): loss=80272.27041118189\n",
      "Gradient Descent(24/999): loss=77969.86962362191\n",
      "Gradient Descent(25/999): loss=75737.59127869386\n",
      "Gradient Descent(26/999): loss=73573.15253530981\n",
      "Gradient Descent(27/999): loss=71474.35686205597\n",
      "Gradient Descent(28/999): loss=69439.08974418994\n",
      "Gradient Descent(29/999): loss=67465.31468226503\n",
      "Gradient Descent(30/999): loss=65551.06945802284\n",
      "Gradient Descent(31/999): loss=63694.46264542387\n",
      "Gradient Descent(32/999): loss=61893.670346706436\n",
      "Gradient Descent(33/999): loss=60146.93313519134\n",
      "Gradient Descent(34/999): loss=58452.5531882058\n",
      "Gradient Descent(35/999): loss=56808.89159499987\n",
      "Gradient Descent(36/999): loss=55214.36582588652\n",
      "Gradient Descent(37/999): loss=53667.447350067516\n",
      "Gradient Descent(38/999): loss=52166.6593907229\n",
      "Gradient Descent(39/999): loss=50710.57480695124\n",
      "Gradient Descent(40/999): loss=49297.81409306604\n",
      "Gradient Descent(41/999): loss=47927.04348658248\n",
      "Gradient Descent(42/999): loss=46596.973176983214\n",
      "Gradient Descent(43/999): loss=45306.355608035126\n",
      "Gradient Descent(44/999): loss=44053.98386704833\n",
      "Gradient Descent(45/999): loss=42838.69015503228\n",
      "Gradient Descent(46/999): loss=41659.344332213936\n",
      "Gradient Descent(47/999): loss=40514.85253384727\n",
      "Gradient Descent(48/999): loss=39404.15585166425\n",
      "Gradient Descent(49/999): loss=38326.2290767007\n",
      "Gradient Descent(50/999): loss=37280.07949957831\n",
      "Gradient Descent(51/999): loss=36264.745764640174\n",
      "Gradient Descent(52/999): loss=35279.29677462582\n",
      "Gradient Descent(53/999): loss=34322.83064283249\n",
      "Gradient Descent(54/999): loss=33394.4736899495\n",
      "Gradient Descent(55/999): loss=32493.379482968103\n",
      "Gradient Descent(56/999): loss=31618.727913768624\n",
      "Gradient Descent(57/999): loss=30769.724315166284\n",
      "Gradient Descent(58/999): loss=29945.59861236227\n",
      "Gradient Descent(59/999): loss=29145.604507896922\n",
      "Gradient Descent(60/999): loss=28369.01869833902\n",
      "Gradient Descent(61/999): loss=27615.14012107095\n",
      "Gradient Descent(62/999): loss=26883.2892296442\n",
      "Gradient Descent(63/999): loss=26172.807296284587\n",
      "Gradient Descent(64/999): loss=25483.055740223255\n",
      "Gradient Descent(65/999): loss=24813.415480617197\n",
      "Gradient Descent(66/999): loss=24163.286312904438\n",
      "Gradient Descent(67/999): loss=23532.08630751305\n",
      "Gradient Descent(68/999): loss=22919.251229911373\n",
      "Gradient Descent(69/999): loss=22324.233981050238\n",
      "Gradient Descent(70/999): loss=21746.504057304806\n",
      "Gradient Descent(71/999): loss=21185.547029077872\n",
      "Gradient Descent(72/999): loss=20640.864037275052\n",
      "Gradient Descent(73/999): loss=20111.971306907915\n",
      "Gradient Descent(74/999): loss=19598.399677122958\n",
      "Gradient Descent(75/999): loss=19099.694146993195\n",
      "Gradient Descent(76/999): loss=18615.41343644512\n",
      "Gradient Descent(77/999): loss=18145.129561727117\n",
      "Gradient Descent(78/999): loss=17688.427424856265\n",
      "Gradient Descent(79/999): loss=17244.904416509584\n",
      "Gradient Descent(80/999): loss=16814.170031852103\n",
      "Gradient Descent(81/999): loss=16395.845498819588\n",
      "Gradient Descent(82/999): loss=15989.563418396561\n",
      "Gradient Descent(83/999): loss=15594.967416452546\n",
      "Gradient Descent(84/999): loss=15211.711806719215\n",
      "Gradient Descent(85/999): loss=14839.461264510723\n",
      "Gradient Descent(86/999): loss=14477.890510806987\n",
      "Gradient Descent(87/999): loss=14126.684006336623\n",
      "Gradient Descent(88/999): loss=13785.535655311922\n",
      "Gradient Descent(89/999): loss=13454.148518483133\n",
      "Gradient Descent(90/999): loss=13132.234535193287\n",
      "Gradient Descent(91/999): loss=12819.514254127824\n",
      "Gradient Descent(92/999): loss=12515.71657246595\n",
      "Gradient Descent(93/999): loss=12220.578483152272\n",
      "Gradient Descent(94/999): loss=11933.844830018303\n",
      "Gradient Descent(95/999): loss=11655.268070494225\n",
      "Gradient Descent(96/999): loss=11384.60804566103\n",
      "Gradient Descent(97/999): loss=11121.631757402813\n",
      "Gradient Descent(98/999): loss=10866.11315242792\n",
      "Gradient Descent(99/999): loss=10617.832912936368\n",
      "Gradient Descent(100/999): loss=10376.578253718892\n",
      "Gradient Descent(101/999): loss=10142.142725481071\n",
      "Gradient Descent(102/999): loss=9914.32602419313\n",
      "Gradient Descent(103/999): loss=9692.933806273237\n",
      "Gradient Descent(104/999): loss=9477.777509418984\n",
      "Gradient Descent(105/999): loss=9268.67417890799\n",
      "Gradient Descent(106/999): loss=9065.446299194984\n",
      "Gradient Descent(107/999): loss=8867.921630638572\n",
      "Gradient Descent(108/999): loss=8675.933051196555\n",
      "Gradient Descent(109/999): loss=8489.318402934223\n",
      "Gradient Descent(110/999): loss=8307.920343195205\n",
      "Gradient Descent(111/999): loss=8131.586200289465\n",
      "Gradient Descent(112/999): loss=7960.167833558059\n",
      "Gradient Descent(113/999): loss=7793.521497678585\n",
      "Gradient Descent(114/999): loss=7631.507711080049\n",
      "Gradient Descent(115/999): loss=7473.991128340021\n",
      "Gradient Descent(116/999): loss=7320.840416440991\n",
      "Gradient Descent(117/999): loss=7171.928134767126\n",
      "Gradient Descent(118/999): loss=7027.1306187260825\n",
      "Gradient Descent(119/999): loss=6886.327866884584\n",
      "Gradient Descent(120/999): loss=6749.403431509791\n",
      "Gradient Descent(121/999): loss=6616.24431241207\n",
      "Gradient Descent(122/999): loss=6486.740853988037\n",
      "Gradient Descent(123/999): loss=6360.786645365968\n",
      "Gradient Descent(124/999): loss=6238.278423558755\n",
      "Gradient Descent(125/999): loss=6119.115979532618\n",
      "Gradient Descent(126/999): loss=6003.2020671025675\n",
      "Gradient Descent(127/999): loss=5890.442314568555\n",
      "Gradient Descent(128/999): loss=5780.745139008774\n",
      "Gradient Descent(129/999): loss=5674.021663149337\n",
      "Gradient Descent(130/999): loss=5570.185634731947\n",
      "Gradient Descent(131/999): loss=5469.15334830368\n",
      "Gradient Descent(132/999): loss=5370.843569355391\n",
      "Gradient Descent(133/999): loss=5275.177460737386\n",
      "Gradient Descent(134/999): loss=5182.078511283424\n",
      "Gradient Descent(135/999): loss=5091.4724665760095\n",
      "Gradient Descent(136/999): loss=5003.287261788222\n",
      "Gradient Descent(137/999): loss=4917.452956539141\n",
      "Gradient Descent(138/999): loss=4833.901671702007\n",
      "Gradient Descent(139/999): loss=4752.56752810599\n",
      "Gradient Descent(140/999): loss=4673.386587074423\n",
      "Gradient Descent(141/999): loss=4596.296792743906\n",
      "Gradient Descent(142/999): loss=4521.23791611057\n",
      "Gradient Descent(143/999): loss=4448.15150075134\n",
      "Gradient Descent(144/999): loss=4376.980810169625\n",
      "Gradient Descent(145/999): loss=4307.670776716476\n",
      "Gradient Descent(146/999): loss=4240.167952039655\n",
      "Gradient Descent(147/999): loss=4174.420459014599\n",
      "Gradient Descent(148/999): loss=4110.377945112605\n",
      "Gradient Descent(149/999): loss=4047.9915371629254\n",
      "Gradient Descent(150/999): loss=3987.213797466841\n",
      "Gradient Descent(151/999): loss=3927.9986812229545\n",
      "Gradient Descent(152/999): loss=3870.30149522429\n",
      "Gradient Descent(153/999): loss=3814.078857788901\n",
      "Gradient Descent(154/999): loss=3759.288659886908\n",
      "Gradient Descent(155/999): loss=3705.8900274279663\n",
      "Gradient Descent(156/999): loss=3653.8432846742985\n",
      "Gradient Descent(157/999): loss=3603.109918745449\n",
      "Gradient Descent(158/999): loss=3553.652545181976\n",
      "Gradient Descent(159/999): loss=3505.4348745362563\n",
      "Gradient Descent(160/999): loss=3458.4216799595483\n",
      "Gradient Descent(161/999): loss=3412.578765755442\n",
      "Gradient Descent(162/999): loss=3367.872936870646\n",
      "Gradient Descent(163/999): loss=3324.271969294999\n",
      "Gradient Descent(164/999): loss=3281.744581343435\n",
      "Gradient Descent(165/999): loss=3240.260405793439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(166/999): loss=3199.789962852331\n",
      "Gradient Descent(167/999): loss=3160.30463392951\n",
      "Gradient Descent(168/999): loss=3121.7766361895406\n",
      "Gradient Descent(169/999): loss=3084.178997862635\n",
      "Gradient Descent(170/999): loss=3047.485534289902\n",
      "Gradient Descent(171/999): loss=3011.6708246813027\n",
      "Gradient Descent(172/999): loss=2976.7101895649803\n",
      "Gradient Descent(173/999): loss=2942.5796689072804\n",
      "Gradient Descent(174/999): loss=2909.2560008833525\n",
      "Gradient Descent(175/999): loss=2876.716601278893\n",
      "Gradient Descent(176/999): loss=2844.939543504118\n",
      "Gradient Descent(177/999): loss=2813.9035392016626\n",
      "Gradient Descent(178/999): loss=2783.5879194306276\n",
      "Gradient Descent(179/999): loss=2753.972616409573\n",
      "Gradient Descent(180/999): loss=2725.0381458017\n",
      "Gradient Descent(181/999): loss=2696.765589526051\n",
      "Gradient Descent(182/999): loss=2669.1365790790023\n",
      "Gradient Descent(183/999): loss=2642.133279350772\n",
      "Gradient Descent(184/999): loss=2615.7383729221897\n",
      "Gradient Descent(185/999): loss=2589.935044827358\n",
      "Gradient Descent(186/999): loss=2564.706967768302\n",
      "Gradient Descent(187/999): loss=2540.0382877681154\n",
      "Gradient Descent(188/999): loss=2515.913610249498\n",
      "Gradient Descent(189/999): loss=2492.317986526\n",
      "Gradient Descent(190/999): loss=2469.2369006936597\n",
      "Gradient Descent(191/999): loss=2446.656256911077\n",
      "Gradient Descent(192/999): loss=2424.5623670563464\n",
      "Gradient Descent(193/999): loss=2402.941938749601\n",
      "Gradient Descent(194/999): loss=2381.7820637302775\n",
      "Gradient Descent(195/999): loss=2361.070206578517\n",
      "Gradient Descent(196/999): loss=2340.794193770444\n",
      "Gradient Descent(197/999): loss=2320.942203057386\n",
      "Gradient Descent(198/999): loss=2301.5027531593655\n",
      "Gradient Descent(199/999): loss=2282.4646937635143\n",
      "Gradient Descent(200/999): loss=2263.8171958183234\n",
      "Gradient Descent(201/999): loss=2245.5497421149216\n",
      "Gradient Descent(202/999): loss=2227.6521181468324\n",
      "Gradient Descent(203/999): loss=2210.1144032399325\n",
      "Gradient Descent(204/999): loss=2192.926961944549\n",
      "Gradient Descent(205/999): loss=2176.0804356819235\n",
      "Gradient Descent(206/999): loss=2159.565734637444\n",
      "Gradient Descent(207/999): loss=2143.3740298933403\n",
      "Gradient Descent(208/999): loss=2127.496745793687\n",
      "Gradient Descent(209/999): loss=2111.9255525348326\n",
      "Gradient Descent(210/999): loss=2096.6523589745457\n",
      "Gradient Descent(211/999): loss=2081.66930565337\n",
      "Gradient Descent(212/999): loss=2066.9687580218974\n",
      "Gradient Descent(213/999): loss=2052.543299867827\n",
      "Gradient Descent(214/999): loss=2038.3857269368907\n",
      "Gradient Descent(215/999): loss=2024.4890407418702\n",
      "Gradient Descent(216/999): loss=2010.8464425541397\n",
      "Gradient Descent(217/999): loss=1997.4513275723007\n",
      "Gradient Descent(218/999): loss=1984.297279262662\n",
      "Gradient Descent(219/999): loss=1971.3780638664634\n",
      "Gradient Descent(220/999): loss=1958.6876250688915\n",
      "Gradient Descent(221/999): loss=1946.2200788250966\n",
      "Gradient Descent(222/999): loss=1933.969708338548\n",
      "Gradient Descent(223/999): loss=1921.9309591872113\n",
      "Gradient Descent(224/999): loss=1910.0984345931715\n",
      "Gradient Descent(225/999): loss=1898.4668908314443\n",
      "Gradient Descent(226/999): loss=1887.0312327738507\n",
      "Gradient Descent(227/999): loss=1875.7865095639593\n",
      "Gradient Descent(228/999): loss=1864.7279104192028\n",
      "Gradient Descent(229/999): loss=1853.8507605564205\n",
      "Gradient Descent(230/999): loss=1843.150517237151\n",
      "Gradient Descent(231/999): loss=1832.622765929141\n",
      "Gradient Descent(232/999): loss=1822.263216580642\n",
      "Gradient Descent(233/999): loss=1812.0677000041212\n",
      "Gradient Descent(234/999): loss=1802.0321643662007\n",
      "Gradient Descent(235/999): loss=1792.152671780627\n",
      "Gradient Descent(236/999): loss=1782.4253950012758\n",
      "Gradient Descent(237/999): loss=1772.8466142121933\n",
      "Gradient Descent(238/999): loss=1763.412713911829\n",
      "Gradient Descent(239/999): loss=1754.1201798886689\n",
      "Gradient Descent(240/999): loss=1744.9655962855634\n",
      "Gradient Descent(241/999): loss=1735.94564275014\n",
      "Gradient Descent(242/999): loss=1727.057091668746\n",
      "Gradient Descent(243/999): loss=1718.2968054814633\n",
      "Gradient Descent(244/999): loss=1709.6617340757998\n",
      "Gradient Descent(245/999): loss=1701.1489122567268\n",
      "Gradient Descent(246/999): loss=1692.7554572908218\n",
      "Gradient Descent(247/999): loss=1684.478566522319\n",
      "Gradient Descent(248/999): loss=1676.3155150589512\n",
      "Gradient Descent(249/999): loss=1668.2636535255253\n",
      "Gradient Descent(250/999): loss=1660.3204058832277\n",
      "Gradient Descent(251/999): loss=1652.4832673127298\n",
      "Gradient Descent(252/999): loss=1644.7498021592064\n",
      "Gradient Descent(253/999): loss=1637.1176419374403\n",
      "Gradient Descent(254/999): loss=1629.5844833952542\n",
      "Gradient Descent(255/999): loss=1622.1480866335387\n",
      "Gradient Descent(256/999): loss=1614.8062732812145\n",
      "Gradient Descent(257/999): loss=1607.5569247235105\n",
      "Gradient Descent(258/999): loss=1600.3979803819877\n",
      "Gradient Descent(259/999): loss=1593.3274360447883\n",
      "Gradient Descent(260/999): loss=1586.3433422456249\n",
      "Gradient Descent(261/999): loss=1579.4438026900868\n",
      "Gradient Descent(262/999): loss=1572.6269727278534\n",
      "Gradient Descent(263/999): loss=1565.8910578694865\n",
      "Gradient Descent(264/999): loss=1559.2343123464748\n",
      "Gradient Descent(265/999): loss=1552.655037713262\n",
      "Gradient Descent(266/999): loss=1546.1515814900229\n",
      "Gradient Descent(267/999): loss=1539.722335845007\n",
      "Gradient Descent(268/999): loss=1533.365736315252\n",
      "Gradient Descent(269/999): loss=1527.0802605645752\n",
      "Gradient Descent(270/999): loss=1520.864427177731\n",
      "Gradient Descent(271/999): loss=1514.7167944896696\n",
      "Gradient Descent(272/999): loss=1508.6359594488738\n",
      "Gradient Descent(273/999): loss=1502.6205565137748\n",
      "Gradient Descent(274/999): loss=1496.6692565812673\n",
      "Gradient Descent(275/999): loss=1490.7807659463888\n",
      "Gradient Descent(276/999): loss=1484.9538252922505\n",
      "Gradient Descent(277/999): loss=1479.1872087093236\n",
      "Gradient Descent(278/999): loss=1473.4797227432318\n",
      "Gradient Descent(279/999): loss=1467.8302054702037\n",
      "Gradient Descent(280/999): loss=1462.2375255993813\n",
      "Gradient Descent(281/999): loss=1456.7005816011974\n",
      "Gradient Descent(282/999): loss=1451.2183008610505\n",
      "Gradient Descent(283/999): loss=1445.7896388575457\n",
      "Gradient Descent(284/999): loss=1440.4135783645797\n",
      "Gradient Descent(285/999): loss=1435.0891286765657\n",
      "Gradient Descent(286/999): loss=1429.8153248561264\n",
      "Gradient Descent(287/999): loss=1424.5912270035983\n",
      "Gradient Descent(288/999): loss=1419.4159195477077\n",
      "Gradient Descent(289/999): loss=1414.2885105567964\n",
      "Gradient Descent(290/999): loss=1409.208131069995\n",
      "Gradient Descent(291/999): loss=1404.1739344477703\n",
      "Gradient Descent(292/999): loss=1399.1850957412714\n",
      "Gradient Descent(293/999): loss=1394.2408110799217\n",
      "Gradient Descent(294/999): loss=1389.3402970767368\n",
      "Gradient Descent(295/999): loss=1384.4827902508378\n",
      "Gradient Descent(296/999): loss=1379.6675464666648\n",
      "Gradient Descent(297/999): loss=1374.8938403893976\n",
      "Gradient Descent(298/999): loss=1370.1609649561185\n",
      "Gradient Descent(299/999): loss=1365.4682308622469\n",
      "Gradient Descent(300/999): loss=1360.8149660628083\n",
      "Gradient Descent(301/999): loss=1356.2005152881027\n",
      "Gradient Descent(302/999): loss=1351.6242395733507\n",
      "Gradient Descent(303/999): loss=1347.0855158019087\n",
      "Gradient Descent(304/999): loss=1342.5837362616649\n",
      "Gradient Descent(305/999): loss=1338.1183082142177\n",
      "Gradient Descent(306/999): loss=1333.6886534764828\n",
      "Gradient Descent(307/999): loss=1329.294208014347\n",
      "Gradient Descent(308/999): loss=1324.9344215480342\n",
      "Gradient Descent(309/999): loss=1320.6087571688317\n",
      "Gradient Descent(310/999): loss=1316.3166909668503\n",
      "Gradient Descent(311/999): loss=1312.0577116694988\n",
      "Gradient Descent(312/999): loss=1307.8313202903532\n",
      "Gradient Descent(313/999): loss=1303.6370297881315\n",
      "Gradient Descent(314/999): loss=1299.4743647354642\n",
      "Gradient Descent(315/999): loss=1295.3428609971866\n",
      "Gradient Descent(316/999): loss=1291.2420654178734\n",
      "Gradient Descent(317/999): loss=1287.1715355183474\n",
      "Gradient Descent(318/999): loss=1283.1308392008948\n",
      "Gradient Descent(319/999): loss=1279.1195544629477\n",
      "Gradient Descent(320/999): loss=1275.1372691189695\n",
      "Gradient Descent(321/999): loss=1271.183580530325\n",
      "Gradient Descent(322/999): loss=1267.258095342887\n",
      "Gradient Descent(323/999): loss=1263.3604292321622\n",
      "Gradient Descent(324/999): loss=1259.490206655719\n",
      "Gradient Descent(325/999): loss=1255.6470606126986\n",
      "Gradient Descent(326/999): loss=1251.830632410216\n",
      "Gradient Descent(327/999): loss=1248.0405714364347\n",
      "Gradient Descent(328/999): loss=1244.2765349401384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(329/999): loss=1240.5381878165947\n",
      "Gradient Descent(330/999): loss=1236.8252023995492\n",
      "Gradient Descent(331/999): loss=1233.1372582591455\n",
      "Gradient Descent(332/999): loss=1229.4740420056237\n",
      "Gradient Descent(333/999): loss=1225.8352470986201\n",
      "Gradient Descent(334/999): loss=1222.2205736619003\n",
      "Gradient Descent(335/999): loss=1218.629728303379\n",
      "Gradient Descent(336/999): loss=1215.0624239402675\n",
      "Gradient Descent(337/999): loss=1211.5183796292017\n",
      "Gradient Descent(338/999): loss=1207.9973204012035\n",
      "Gradient Descent(339/999): loss=1204.4989771013445\n",
      "Gradient Descent(340/999): loss=1201.0230862329645\n",
      "Gradient Descent(341/999): loss=1197.569389806323\n",
      "Gradient Descent(342/999): loss=1194.137635191548\n",
      "Gradient Descent(343/999): loss=1190.7275749757634\n",
      "Gradient Descent(344/999): loss=1187.3389668242733\n",
      "Gradient Descent(345/999): loss=1183.9715733456812\n",
      "Gradient Descent(346/999): loss=1180.625161960837\n",
      "Gradient Descent(347/999): loss=1177.2995047754982\n",
      "Gradient Descent(348/999): loss=1173.994378456598\n",
      "Gradient Descent(349/999): loss=1170.7095641120166\n",
      "Gradient Descent(350/999): loss=1167.4448471737535\n",
      "Gradient Descent(351/999): loss=1164.2000172844082\n",
      "Gradient Descent(352/999): loss=1160.9748681868625\n",
      "Gradient Descent(353/999): loss=1157.7691976170854\n",
      "Gradient Descent(354/999): loss=1154.5828071999642\n",
      "Gradient Descent(355/999): loss=1151.4155023480685\n",
      "Gradient Descent(356/999): loss=1148.2670921632762\n",
      "Gradient Descent(357/999): loss=1145.1373893411699\n",
      "Gradient Descent(358/999): loss=1142.0262100781226\n",
      "Gradient Descent(359/999): loss=1138.9333739810052\n",
      "Gradient Descent(360/999): loss=1135.8587039794247\n",
      "Gradient Descent(361/999): loss=1132.8020262404393\n",
      "Gradient Descent(362/999): loss=1129.7631700856628\n",
      "Gradient Descent(363/999): loss=1126.7419679106965\n",
      "Gradient Descent(364/999): loss=1123.7382551068274\n",
      "Gradient Descent(365/999): loss=1120.7518699849127\n",
      "Gradient Descent(366/999): loss=1117.7826537014037\n",
      "Gradient Descent(367/999): loss=1114.8304501864354\n",
      "Gradient Descent(368/999): loss=1111.8951060739355\n",
      "Gradient Descent(369/999): loss=1108.9764706336757\n",
      "Gradient Descent(370/999): loss=1106.0743957052332\n",
      "Gradient Descent(371/999): loss=1103.1887356337838\n",
      "Gradient Descent(372/999): loss=1100.3193472076985\n",
      "Gradient Descent(373/999): loss=1097.466089597864\n",
      "Gradient Descent(374/999): loss=1094.628824298709\n",
      "Gradient Descent(375/999): loss=1091.8074150708567\n",
      "Gradient Descent(376/999): loss=1089.001727885381\n",
      "Gradient Descent(377/999): loss=1086.211630869609\n",
      "Gradient Descent(378/999): loss=1083.4369942544251\n",
      "Gradient Descent(379/999): loss=1080.6776903230361\n",
      "Gradient Descent(380/999): loss=1077.9335933611612\n",
      "Gradient Descent(381/999): loss=1075.2045796085924\n",
      "Gradient Descent(382/999): loss=1072.4905272121025\n",
      "Gradient Descent(383/999): loss=1069.7913161796453\n",
      "Gradient Descent(384/999): loss=1067.1068283358302\n",
      "Gradient Descent(385/999): loss=1064.4369472786134\n",
      "Gradient Descent(386/999): loss=1061.78155833719\n",
      "Gradient Descent(387/999): loss=1059.1405485310427\n",
      "Gradient Descent(388/999): loss=1056.513806530113\n",
      "Gradient Descent(389/999): loss=1053.901222616073\n",
      "Gradient Descent(390/999): loss=1051.3026886446496\n",
      "Gradient Descent(391/999): loss=1048.718098008991\n",
      "Gradient Descent(392/999): loss=1046.1473456040276\n",
      "Gradient Descent(393/999): loss=1043.5903277918158\n",
      "Gradient Descent(394/999): loss=1041.0469423678176\n",
      "Gradient Descent(395/999): loss=1038.5170885281104\n",
      "Gradient Descent(396/999): loss=1036.0006668374822\n",
      "Gradient Descent(397/999): loss=1033.4975791983998\n",
      "Gradient Descent(398/999): loss=1031.0077288208172\n",
      "Gradient Descent(399/999): loss=1028.531020192805\n",
      "Gradient Descent(400/999): loss=1026.0673590519764\n",
      "Gradient Descent(401/999): loss=1023.616652357685\n",
      "Gradient Descent(402/999): loss=1021.1788082639752\n",
      "Gradient Descent(403/999): loss=1018.7537360932635\n",
      "Gradient Descent(404/999): loss=1016.3413463107279\n",
      "Gradient Descent(405/999): loss=1013.9415504993875\n",
      "Gradient Descent(406/999): loss=1011.5542613358526\n",
      "Gradient Descent(407/999): loss=1009.1793925667249\n",
      "Gradient Descent(408/999): loss=1006.8168589856315\n",
      "Gradient Descent(409/999): loss=1004.4665764108697\n",
      "Gradient Descent(410/999): loss=1002.1284616636561\n",
      "Gradient Descent(411/999): loss=999.80243254695\n",
      "Gradient Descent(412/999): loss=997.4884078248409\n",
      "Gradient Descent(413/999): loss=995.1863072024911\n",
      "Gradient Descent(414/999): loss=992.8960513065998\n",
      "Gradient Descent(415/999): loss=990.6175616663963\n",
      "Gradient Descent(416/999): loss=988.3507606951268\n",
      "Gradient Descent(417/999): loss=986.0955716720359\n",
      "Gradient Descent(418/999): loss=983.8519187248219\n",
      "Gradient Descent(419/999): loss=981.6197268125505\n",
      "Gradient Descent(420/999): loss=979.3989217090233\n",
      "Gradient Descent(421/999): loss=977.1894299865745\n",
      "Gradient Descent(422/999): loss=974.9911790003024\n",
      "Gradient Descent(423/999): loss=972.8040968726984\n",
      "Gradient Descent(424/999): loss=970.6281124786921\n",
      "Gradient Descent(425/999): loss=968.46315543107\n",
      "Gradient Descent(426/999): loss=966.3091560662878\n",
      "Gradient Descent(427/999): loss=964.1660454306361\n",
      "Gradient Descent(428/999): loss=962.0337552667736\n",
      "Gradient Descent(429/999): loss=959.9122180006027\n",
      "Gradient Descent(430/999): loss=957.8013667284854\n",
      "Gradient Descent(431/999): loss=955.7011352047824\n",
      "Gradient Descent(432/999): loss=953.6114578297204\n",
      "Gradient Descent(433/999): loss=951.5322696375566\n",
      "Gradient Descent(434/999): loss=949.4635062850591\n",
      "Gradient Descent(435/999): loss=947.4051040402674\n",
      "Gradient Descent(436/999): loss=945.3569997715457\n",
      "Gradient Descent(437/999): loss=943.3191309369115\n",
      "Gradient Descent(438/999): loss=941.2914355736326\n",
      "Gradient Descent(439/999): loss=939.273852288083\n",
      "Gradient Descent(440/999): loss=937.2663202458597\n",
      "Gradient Descent(441/999): loss=935.2687791621424\n",
      "Gradient Descent(442/999): loss=933.2811692922947\n",
      "Gradient Descent(443/999): loss=931.3034314226979\n",
      "Gradient Descent(444/999): loss=929.3355068618155\n",
      "Gradient Descent(445/999): loss=927.3773374314744\n",
      "Gradient Descent(446/999): loss=925.4288654583664\n",
      "Gradient Descent(447/999): loss=923.4900337657526\n",
      "Gradient Descent(448/999): loss=921.5607856653758\n",
      "Gradient Descent(449/999): loss=919.6410649495685\n",
      "Gradient Descent(450/999): loss=917.7308158835536\n",
      "Gradient Descent(451/999): loss=915.8299831979323\n",
      "Gradient Descent(452/999): loss=913.9385120813512\n",
      "Gradient Descent(453/999): loss=912.0563481733512\n",
      "Gradient Descent(454/999): loss=910.1834375573827\n",
      "Gradient Descent(455/999): loss=908.3197267539923\n",
      "Gradient Descent(456/999): loss=906.4651627141694\n",
      "Gradient Descent(457/999): loss=904.619692812852\n",
      "Gradient Descent(458/999): loss=902.7832648425866\n",
      "Gradient Descent(459/999): loss=900.9558270073372\n",
      "Gradient Descent(460/999): loss=899.1373279164385\n",
      "Gradient Descent(461/999): loss=897.3277165786949\n",
      "Gradient Descent(462/999): loss=895.5269423966109\n",
      "Gradient Descent(463/999): loss=893.7349551607616\n",
      "Gradient Descent(464/999): loss=891.9517050442886\n",
      "Gradient Descent(465/999): loss=890.1771425975253\n",
      "Gradient Descent(466/999): loss=888.4112187427463\n",
      "Gradient Descent(467/999): loss=886.6538847690332\n",
      "Gradient Descent(468/999): loss=884.905092327261\n",
      "Gradient Descent(469/999): loss=883.1647934251943\n",
      "Gradient Descent(470/999): loss=881.4329404226983\n",
      "Gradient Descent(471/999): loss=879.7094860270538\n",
      "Gradient Descent(472/999): loss=877.9943832883774\n",
      "Gradient Descent(473/999): loss=876.2875855951455\n",
      "Gradient Descent(474/999): loss=874.5890466698146\n",
      "Gradient Descent(475/999): loss=872.8987205645401\n",
      "Gradient Descent(476/999): loss=871.2165616569874\n",
      "Gradient Descent(477/999): loss=869.5425246462358\n",
      "Gradient Descent(478/999): loss=867.87656454877\n",
      "Gradient Descent(479/999): loss=866.2186366945583\n",
      "Gradient Descent(480/999): loss=864.5686967232152\n",
      "Gradient Descent(481/999): loss=862.9267005802465\n",
      "Gradient Descent(482/999): loss=861.2926045133736\n",
      "Gradient Descent(483/999): loss=859.6663650689345\n",
      "Gradient Descent(484/999): loss=858.0479390883609\n",
      "Gradient Descent(485/999): loss=856.4372837047318\n",
      "Gradient Descent(486/999): loss=854.8343563393931\n",
      "Gradient Descent(487/999): loss=853.2391146986525\n",
      "Gradient Descent(488/999): loss=851.6515167705381\n",
      "Gradient Descent(489/999): loss=850.0715208216257\n",
      "Gradient Descent(490/999): loss=848.4990853939295\n",
      "Gradient Descent(491/999): loss=846.9341693018562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(492/999): loss=845.376731629218\n",
      "Gradient Descent(493/999): loss=843.8267317263084\n",
      "Gradient Descent(494/999): loss=842.284129207034\n",
      "Gradient Descent(495/999): loss=840.7488839461016\n",
      "Gradient Descent(496/999): loss=839.2209560762632\n",
      "Gradient Descent(497/999): loss=837.7003059856122\n",
      "Gradient Descent(498/999): loss=836.1868943149315\n",
      "Gradient Descent(499/999): loss=834.6806819550955\n",
      "Gradient Descent(500/999): loss=833.1816300445174\n",
      "Gradient Descent(501/999): loss=831.689699966648\n",
      "Gradient Descent(502/999): loss=830.2048533475196\n",
      "Gradient Descent(503/999): loss=828.7270520533367\n",
      "Gradient Descent(504/999): loss=827.2562581881105\n",
      "Gradient Descent(505/999): loss=825.7924340913378\n",
      "Gradient Descent(506/999): loss=824.3355423357228\n",
      "Gradient Descent(507/999): loss=822.8855457249356\n",
      "Gradient Descent(508/999): loss=821.4424072914196\n",
      "Gradient Descent(509/999): loss=820.0060902942286\n",
      "Gradient Descent(510/999): loss=818.5765582169096\n",
      "Gradient Descent(511/999): loss=817.1537747654186\n",
      "Gradient Descent(512/999): loss=815.7377038660751\n",
      "Gradient Descent(513/999): loss=814.32830966355\n",
      "Gradient Descent(514/999): loss=812.9255565188901\n",
      "Gradient Descent(515/999): loss=811.5294090075739\n",
      "Gradient Descent(516/999): loss=810.1398319176051\n",
      "Gradient Descent(517/999): loss=808.7567902476302\n",
      "Gradient Descent(518/999): loss=807.3802492050957\n",
      "Gradient Descent(519/999): loss=806.0101742044309\n",
      "Gradient Descent(520/999): loss=804.6465308652619\n",
      "Gradient Descent(521/999): loss=803.2892850106532\n",
      "Gradient Descent(522/999): loss=801.9384026653821\n",
      "Gradient Descent(523/999): loss=800.5938500542335\n",
      "Gradient Descent(524/999): loss=799.2555936003286\n",
      "Gradient Descent(525/999): loss=797.9235999234761\n",
      "Gradient Descent(526/999): loss=796.5978358385506\n",
      "Gradient Descent(527/999): loss=795.2782683538942\n",
      "Gradient Descent(528/999): loss=793.9648646697467\n",
      "Gradient Descent(529/999): loss=792.6575921766937\n",
      "Gradient Descent(530/999): loss=791.3564184541434\n",
      "Gradient Descent(531/999): loss=790.0613112688238\n",
      "Gradient Descent(532/999): loss=788.7722385733023\n",
      "Gradient Descent(533/999): loss=787.4891685045275\n",
      "Gradient Descent(534/999): loss=786.2120693823906\n",
      "Gradient Descent(535/999): loss=784.940909708312\n",
      "Gradient Descent(536/999): loss=783.6756581638407\n",
      "Gradient Descent(537/999): loss=782.4162836092823\n",
      "Gradient Descent(538/999): loss=781.1627550823388\n",
      "Gradient Descent(539/999): loss=779.9150417967714\n",
      "Gradient Descent(540/999): loss=778.6731131410816\n",
      "Gradient Descent(541/999): loss=777.4369386772075\n",
      "Gradient Descent(542/999): loss=776.2064881392424\n",
      "Gradient Descent(543/999): loss=774.9817314321654\n",
      "Gradient Descent(544/999): loss=773.7626386305935\n",
      "Gradient Descent(545/999): loss=772.5491799775472\n",
      "Gradient Descent(546/999): loss=771.3413258832326\n",
      "Gradient Descent(547/999): loss=770.1390469238419\n",
      "Gradient Descent(548/999): loss=768.9423138403647\n",
      "Gradient Descent(549/999): loss=767.7510975374184\n",
      "Gradient Descent(550/999): loss=766.5653690820936\n",
      "Gradient Descent(551/999): loss=765.3850997028077\n",
      "Gradient Descent(552/999): loss=764.2102607881819\n",
      "Gradient Descent(553/999): loss=763.0408238859234\n",
      "Gradient Descent(554/999): loss=761.8767607017295\n",
      "Gradient Descent(555/999): loss=760.7180430981956\n",
      "Gradient Descent(556/999): loss=759.564643093745\n",
      "Gradient Descent(557/999): loss=758.4165328615645\n",
      "Gradient Descent(558/999): loss=757.2736847285586\n",
      "Gradient Descent(559/999): loss=756.1360711743085\n",
      "Gradient Descent(560/999): loss=755.0036648300511\n",
      "Gradient Descent(561/999): loss=753.8764384776639\n",
      "Gradient Descent(562/999): loss=752.7543650486627\n",
      "Gradient Descent(563/999): loss=751.6374176232127\n",
      "Gradient Descent(564/999): loss=750.5255694291473\n",
      "Gradient Descent(565/999): loss=749.4187938410003\n",
      "Gradient Descent(566/999): loss=748.3170643790463\n",
      "Gradient Descent(567/999): loss=747.2203547083552\n",
      "Gradient Descent(568/999): loss=746.1286386378517\n",
      "Gradient Descent(569/999): loss=745.0418901193888\n",
      "Gradient Descent(570/999): loss=743.9600832468296\n",
      "Gradient Descent(571/999): loss=742.8831922551389\n",
      "Gradient Descent(572/999): loss=741.8111915194827\n",
      "Gradient Descent(573/999): loss=740.7440555543395\n",
      "Gradient Descent(574/999): loss=739.6817590126207\n",
      "Gradient Descent(575/999): loss=738.6242766847945\n",
      "Gradient Descent(576/999): loss=737.5715834980265\n",
      "Gradient Descent(577/999): loss=736.523654515323\n",
      "Gradient Descent(578/999): loss=735.4804649346827\n",
      "Gradient Descent(579/999): loss=734.4419900882637\n",
      "Gradient Descent(580/999): loss=733.408205441548\n",
      "Gradient Descent(581/999): loss=732.3790865925206\n",
      "Gradient Descent(582/999): loss=731.3546092708576\n",
      "Gradient Descent(583/999): loss=730.3347493371156\n",
      "Gradient Descent(584/999): loss=729.3194827819354\n",
      "Gradient Descent(585/999): loss=728.3087857252476\n",
      "Gradient Descent(586/999): loss=727.3026344154902\n",
      "Gradient Descent(587/999): loss=726.3010052288297\n",
      "Gradient Descent(588/999): loss=725.3038746683899\n",
      "Gradient Descent(589/999): loss=724.31121936349\n",
      "Gradient Descent(590/999): loss=723.3230160688868\n",
      "Gradient Descent(591/999): loss=722.3392416640244\n",
      "Gradient Descent(592/999): loss=721.3598731522897\n",
      "Gradient Descent(593/999): loss=720.3848876602773\n",
      "Gradient Descent(594/999): loss=719.4142624370567\n",
      "Gradient Descent(595/999): loss=718.4479748534492\n",
      "Gradient Descent(596/999): loss=717.4860024013072\n",
      "Gradient Descent(597/999): loss=716.5283226928038\n",
      "Gradient Descent(598/999): loss=715.5749134597252\n",
      "Gradient Descent(599/999): loss=714.6257525527703\n",
      "Gradient Descent(600/999): loss=713.6808179408561\n",
      "Gradient Descent(601/999): loss=712.7400877104278\n",
      "Gradient Descent(602/999): loss=711.8035400647748\n",
      "Gradient Descent(603/999): loss=710.8711533233542\n",
      "Gradient Descent(604/999): loss=709.9429059211162\n",
      "Gradient Descent(605/999): loss=709.018776407838\n",
      "Gradient Descent(606/999): loss=708.0987434474607\n",
      "Gradient Descent(607/999): loss=707.1827858174333\n",
      "Gradient Descent(608/999): loss=706.2708824080596\n",
      "Gradient Descent(609/999): loss=705.3630122218525\n",
      "Gradient Descent(610/999): loss=704.4591543728915\n",
      "Gradient Descent(611/999): loss=703.559288086186\n",
      "Gradient Descent(612/999): loss=702.6633926970432\n",
      "Gradient Descent(613/999): loss=701.7714476504401\n",
      "Gradient Descent(614/999): loss=700.8834325004016\n",
      "Gradient Descent(615/999): loss=699.9993269093831\n",
      "Gradient Descent(616/999): loss=699.1191106476542\n",
      "Gradient Descent(617/999): loss=698.2427635926927\n",
      "Gradient Descent(618/999): loss=697.3702657285786\n",
      "Gradient Descent(619/999): loss=696.5015971453946\n",
      "Gradient Descent(620/999): loss=695.6367380386288\n",
      "Gradient Descent(621/999): loss=694.7756687085862\n",
      "Gradient Descent(622/999): loss=693.9183695597974\n",
      "Gradient Descent(623/999): loss=693.0648211004384\n",
      "Gradient Descent(624/999): loss=692.2150039417508\n",
      "Gradient Descent(625/999): loss=691.3688987974663\n",
      "Gradient Descent(626/999): loss=690.5264864832362\n",
      "Gradient Descent(627/999): loss=689.6877479160637\n",
      "Gradient Descent(628/999): loss=688.8526641137427\n",
      "Gradient Descent(629/999): loss=688.0212161942957\n",
      "Gradient Descent(630/999): loss=687.1933853754207\n",
      "Gradient Descent(631/999): loss=686.3691529739389\n",
      "Gradient Descent(632/999): loss=685.5485004052464\n",
      "Gradient Descent(633/999): loss=684.7314091827711\n",
      "Gradient Descent(634/999): loss=683.91786091743\n",
      "Gradient Descent(635/999): loss=683.1078373170949\n",
      "Gradient Descent(636/999): loss=682.301320186058\n",
      "Gradient Descent(637/999): loss=681.4982914245005\n",
      "Gradient Descent(638/999): loss=680.6987330279693\n",
      "Gradient Descent(639/999): loss=679.9026270868509\n",
      "Gradient Descent(640/999): loss=679.1099557858545\n",
      "Gradient Descent(641/999): loss=678.3207014034948\n",
      "Gradient Descent(642/999): loss=677.5348463115783\n",
      "Gradient Descent(643/999): loss=676.7523729746964\n",
      "Gradient Descent(644/999): loss=675.9732639497159\n",
      "Gradient Descent(645/999): loss=675.1975018852797\n",
      "Gradient Descent(646/999): loss=674.4250695213032\n",
      "Gradient Descent(647/999): loss=673.655949688481\n",
      "Gradient Descent(648/999): loss=672.8901253077917\n",
      "Gradient Descent(649/999): loss=672.127579390007\n",
      "Gradient Descent(650/999): loss=671.3682950352078\n",
      "Gradient Descent(651/999): loss=670.6122554322951\n",
      "Gradient Descent(652/999): loss=669.8594438585126\n",
      "Gradient Descent(653/999): loss=669.1098436789671\n",
      "Gradient Descent(654/999): loss=668.3634383461535\n",
      "Gradient Descent(655/999): loss=667.620211399482\n",
      "Gradient Descent(656/999): loss=666.8801464648084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(657/999): loss=666.14322725397\n",
      "Gradient Descent(658/999): loss=665.4094375643184\n",
      "Gradient Descent(659/999): loss=664.6787612782615\n",
      "Gradient Descent(660/999): loss=663.9511823628033\n",
      "Gradient Descent(661/999): loss=663.2266848690901\n",
      "Gradient Descent(662/999): loss=662.5052529319576\n",
      "Gradient Descent(663/999): loss=661.7868707694801\n",
      "Gradient Descent(664/999): loss=661.0715226825251\n",
      "Gradient Descent(665/999): loss=660.3591930543073\n",
      "Gradient Descent(666/999): loss=659.6498663499475\n",
      "Gradient Descent(667/999): loss=658.943527116033\n",
      "Gradient Descent(668/999): loss=658.2401599801823\n",
      "Gradient Descent(669/999): loss=657.5397496506089\n",
      "Gradient Descent(670/999): loss=656.8422809156923\n",
      "Gradient Descent(671/999): loss=656.1477386435467\n",
      "Gradient Descent(672/999): loss=655.4561077815966\n",
      "Gradient Descent(673/999): loss=654.7673733561519\n",
      "Gradient Descent(674/999): loss=654.081520471987\n",
      "Gradient Descent(675/999): loss=653.3985343119218\n",
      "Gradient Descent(676/999): loss=652.7184001364041\n",
      "Gradient Descent(677/999): loss=652.0411032830975\n",
      "Gradient Descent(678/999): loss=651.3666291664679\n",
      "Gradient Descent(679/999): loss=650.6949632773755\n",
      "Gradient Descent(680/999): loss=650.0260911826666\n",
      "Gradient Descent(681/999): loss=649.3599985247708\n",
      "Gradient Descent(682/999): loss=648.696671021297\n",
      "Gradient Descent(683/999): loss=648.0360944646334\n",
      "Gradient Descent(684/999): loss=647.3782547215509\n",
      "Gradient Descent(685/999): loss=646.7231377328087\n",
      "Gradient Descent(686/999): loss=646.0707295127573\n",
      "Gradient Descent(687/999): loss=645.4210161489511\n",
      "Gradient Descent(688/999): loss=644.773983801759\n",
      "Gradient Descent(689/999): loss=644.1296187039766\n",
      "Gradient Descent(690/999): loss=643.4879071604435\n",
      "Gradient Descent(691/999): loss=642.8488355476596\n",
      "Gradient Descent(692/999): loss=642.2123903134062\n",
      "Gradient Descent(693/999): loss=641.5785579763685\n",
      "Gradient Descent(694/999): loss=640.9473251257584\n",
      "Gradient Descent(695/999): loss=640.3186784209423\n",
      "Gradient Descent(696/999): loss=639.6926045910682\n",
      "Gradient Descent(697/999): loss=639.0690904346983\n",
      "Gradient Descent(698/999): loss=638.4481228194395\n",
      "Gradient Descent(699/999): loss=637.8296886815791\n",
      "Gradient Descent(700/999): loss=637.2137750257218\n",
      "Gradient Descent(701/999): loss=636.6003689244272\n",
      "Gradient Descent(702/999): loss=635.9894575178525\n",
      "Gradient Descent(703/999): loss=635.3810280133928\n",
      "Gradient Descent(704/999): loss=634.7750676853283\n",
      "Gradient Descent(705/999): loss=634.1715638744689\n",
      "Gradient Descent(706/999): loss=633.570503987804\n",
      "Gradient Descent(707/999): loss=632.9718754981532\n",
      "Gradient Descent(708/999): loss=632.3756659438181\n",
      "Gradient Descent(709/999): loss=631.7818629282366\n",
      "Gradient Descent(710/999): loss=631.1904541196398\n",
      "Gradient Descent(711/999): loss=630.6014272507106\n",
      "Gradient Descent(712/999): loss=630.014770118243\n",
      "Gradient Descent(713/999): loss=629.4304705828035\n",
      "Gradient Descent(714/999): loss=628.8485165683967\n",
      "Gradient Descent(715/999): loss=628.2688960621292\n",
      "Gradient Descent(716/999): loss=627.6915971138783\n",
      "Gradient Descent(717/999): loss=627.1166078359603\n",
      "Gradient Descent(718/999): loss=626.5439164028026\n",
      "Gradient Descent(719/999): loss=625.9735110506149\n",
      "Gradient Descent(720/999): loss=625.4053800770664\n",
      "Gradient Descent(721/999): loss=624.8395118409602\n",
      "Gradient Descent(722/999): loss=624.275894761912\n",
      "Gradient Descent(723/999): loss=623.7145173200298\n",
      "Gradient Descent(724/999): loss=623.1553680555976\n",
      "Gradient Descent(725/999): loss=622.5984355687553\n",
      "Gradient Descent(726/999): loss=622.043708519186\n",
      "Gradient Descent(727/999): loss=621.4911756258035\n",
      "Gradient Descent(728/999): loss=620.9408256664389\n",
      "Gradient Descent(729/999): loss=620.3926474775323\n",
      "Gradient Descent(730/999): loss=619.8466299538233\n",
      "Gradient Descent(731/999): loss=619.3027620480465\n",
      "Gradient Descent(732/999): loss=618.7610327706248\n",
      "Gradient Descent(733/999): loss=618.2214311893667\n",
      "Gradient Descent(734/999): loss=617.6839464291659\n",
      "Gradient Descent(735/999): loss=617.1485676716998\n",
      "Gradient Descent(736/999): loss=616.6152841551328\n",
      "Gradient Descent(737/999): loss=616.0840851738176\n",
      "Gradient Descent(738/999): loss=615.5549600780029\n",
      "Gradient Descent(739/999): loss=615.0278982735362\n",
      "Gradient Descent(740/999): loss=614.502889221576\n",
      "Gradient Descent(741/999): loss=613.9799224382975\n",
      "Gradient Descent(742/999): loss=613.4589874946072\n",
      "Gradient Descent(743/999): loss=612.9400740158534\n",
      "Gradient Descent(744/999): loss=612.4231716815412\n",
      "Gradient Descent(745/999): loss=611.9082702250481\n",
      "Gradient Descent(746/999): loss=611.3953594333417\n",
      "Gradient Descent(747/999): loss=610.8844291466986\n",
      "Gradient Descent(748/999): loss=610.3754692584239\n",
      "Gradient Descent(749/999): loss=609.8684697145741\n",
      "Gradient Descent(750/999): loss=609.3634205136797\n",
      "Gradient Descent(751/999): loss=608.8603117064697\n",
      "Gradient Descent(752/999): loss=608.3591333955992\n",
      "Gradient Descent(753/999): loss=607.8598757353756\n",
      "Gradient Descent(754/999): loss=607.362528931489\n",
      "Gradient Descent(755/999): loss=606.8670832407419\n",
      "Gradient Descent(756/999): loss=606.3735289707818\n",
      "Gradient Descent(757/999): loss=605.8818564798343\n",
      "Gradient Descent(758/999): loss=605.3920561764381\n",
      "Gradient Descent(759/999): loss=604.9041185191818\n",
      "Gradient Descent(760/999): loss=604.4180340164402\n",
      "Gradient Descent(761/999): loss=603.933793226116\n",
      "Gradient Descent(762/999): loss=603.4513867553766\n",
      "Gradient Descent(763/999): loss=602.9708052603995\n",
      "Gradient Descent(764/999): loss=602.4920394461138\n",
      "Gradient Descent(765/999): loss=602.0150800659442\n",
      "Gradient Descent(766/999): loss=601.5399179215588\n",
      "Gradient Descent(767/999): loss=601.0665438626143\n",
      "Gradient Descent(768/999): loss=600.5949487865067\n",
      "Gradient Descent(769/999): loss=600.1251236381195\n",
      "Gradient Descent(770/999): loss=599.657059409576\n",
      "Gradient Descent(771/999): loss=599.1907471399919\n",
      "Gradient Descent(772/999): loss=598.7261779152283\n",
      "Gradient Descent(773/999): loss=598.263342867648\n",
      "Gradient Descent(774/999): loss=597.8022331758715\n",
      "Gradient Descent(775/999): loss=597.3428400645349\n",
      "Gradient Descent(776/999): loss=596.885154804049\n",
      "Gradient Descent(777/999): loss=596.4291687103597\n",
      "Gradient Descent(778/999): loss=595.974873144709\n",
      "Gradient Descent(779/999): loss=595.5222595133988\n",
      "Gradient Descent(780/999): loss=595.0713192675547\n",
      "Gradient Descent(781/999): loss=594.6220439028915\n",
      "Gradient Descent(782/999): loss=594.174424959479\n",
      "Gradient Descent(783/999): loss=593.7284540215106\n",
      "Gradient Descent(784/999): loss=593.2841227170728\n",
      "Gradient Descent(785/999): loss=592.8414227179153\n",
      "Gradient Descent(786/999): loss=592.4003457392204\n",
      "Gradient Descent(787/999): loss=591.960883539379\n",
      "Gradient Descent(788/999): loss=591.5230279197629\n",
      "Gradient Descent(789/999): loss=591.0867707244988\n",
      "Gradient Descent(790/999): loss=590.6521038402474\n",
      "Gradient Descent(791/999): loss=590.2190191959779\n",
      "Gradient Descent(792/999): loss=589.7875087627485\n",
      "Gradient Descent(793/999): loss=589.3575645534859\n",
      "Gradient Descent(794/999): loss=588.9291786227652\n",
      "Gradient Descent(795/999): loss=588.5023430665933\n",
      "Gradient Descent(796/999): loss=588.0770500221923\n",
      "Gradient Descent(797/999): loss=587.6532916677825\n",
      "Gradient Descent(798/999): loss=587.2310602223687\n",
      "Gradient Descent(799/999): loss=586.8103479455278\n",
      "Gradient Descent(800/999): loss=586.3911471371946\n",
      "Gradient Descent(801/999): loss=585.9734501374523\n",
      "Gradient Descent(802/999): loss=585.5572493263222\n",
      "Gradient Descent(803/999): loss=585.1425371235545\n",
      "Gradient Descent(804/999): loss=584.7293059884204\n",
      "Gradient Descent(805/999): loss=584.3175484195056\n",
      "Gradient Descent(806/999): loss=583.9072569545058\n",
      "Gradient Descent(807/999): loss=583.4984241700199\n",
      "Gradient Descent(808/999): loss=583.0910426813485\n",
      "Gradient Descent(809/999): loss=582.6851051422901\n",
      "Gradient Descent(810/999): loss=582.2806042449417\n",
      "Gradient Descent(811/999): loss=581.8775327194963\n",
      "Gradient Descent(812/999): loss=581.4758833340447\n",
      "Gradient Descent(813/999): loss=581.0756488943786\n",
      "Gradient Descent(814/999): loss=580.6768222437909\n",
      "Gradient Descent(815/999): loss=580.2793962628817\n",
      "Gradient Descent(816/999): loss=579.8833638693624\n",
      "Gradient Descent(817/999): loss=579.4887180178616\n",
      "Gradient Descent(818/999): loss=579.0954516997328\n",
      "Gradient Descent(819/999): loss=578.7035579428609\n",
      "Gradient Descent(820/999): loss=578.3130298114734\n",
      "Gradient Descent(821/999): loss=577.9238604059485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(822/999): loss=577.536042862627\n",
      "Gradient Descent(823/999): loss=577.1495703536242\n",
      "Gradient Descent(824/999): loss=576.7644360866425\n",
      "Gradient Descent(825/999): loss=576.3806333047858\n",
      "Gradient Descent(826/999): loss=575.9981552863745\n",
      "Gradient Descent(827/999): loss=575.6169953447612\n",
      "Gradient Descent(828/999): loss=575.237146828148\n",
      "Gradient Descent(829/999): loss=574.8586031194021\n",
      "Gradient Descent(830/999): loss=574.4813576358792\n",
      "Gradient Descent(831/999): loss=574.1054038292388\n",
      "Gradient Descent(832/999): loss=573.7307351852664\n",
      "Gradient Descent(833/999): loss=573.3573452236965\n",
      "Gradient Descent(834/999): loss=572.9852274980328\n",
      "Gradient Descent(835/999): loss=572.6143755953734\n",
      "Gradient Descent(836/999): loss=572.2447831362349\n",
      "Gradient Descent(837/999): loss=571.8764437743769\n",
      "Gradient Descent(838/999): loss=571.5093511966296\n",
      "Gradient Descent(839/999): loss=571.1434991227196\n",
      "Gradient Descent(840/999): loss=570.7788813050994\n",
      "Gradient Descent(841/999): loss=570.4154915287745\n",
      "Gradient Descent(842/999): loss=570.0533236111353\n",
      "Gradient Descent(843/999): loss=569.6923714017863\n",
      "Gradient Descent(844/999): loss=569.3326287823791\n",
      "Gradient Descent(845/999): loss=568.9740896664432\n",
      "Gradient Descent(846/999): loss=568.6167479992213\n",
      "Gradient Descent(847/999): loss=568.2605977575018\n",
      "Gradient Descent(848/999): loss=567.9056329494564\n",
      "Gradient Descent(849/999): loss=567.5518476144724\n",
      "Gradient Descent(850/999): loss=567.1992358229929\n",
      "Gradient Descent(851/999): loss=566.8477916763534\n",
      "Gradient Descent(852/999): loss=566.4975093066198\n",
      "Gradient Descent(853/999): loss=566.1483828764285\n",
      "Gradient Descent(854/999): loss=565.8004065788268\n",
      "Gradient Descent(855/999): loss=565.453574637113\n",
      "Gradient Descent(856/999): loss=565.1078813046796\n",
      "Gradient Descent(857/999): loss=564.7633208648544\n",
      "Gradient Descent(858/999): loss=564.4198876307466\n",
      "Gradient Descent(859/999): loss=564.0775759450886\n",
      "Gradient Descent(860/999): loss=563.7363801800828\n",
      "Gradient Descent(861/999): loss=563.3962947372471\n",
      "Gradient Descent(862/999): loss=563.057314047261\n",
      "Gradient Descent(863/999): loss=562.719432569815\n",
      "Gradient Descent(864/999): loss=562.3826447934575\n",
      "Gradient Descent(865/999): loss=562.0469452354446\n",
      "Gradient Descent(866/999): loss=561.7123284415899\n",
      "Gradient Descent(867/999): loss=561.3787889861156\n",
      "Gradient Descent(868/999): loss=561.0463214715041\n",
      "Gradient Descent(869/999): loss=560.7149205283494\n",
      "Gradient Descent(870/999): loss=560.3845808152117\n",
      "Gradient Descent(871/999): loss=560.05529701847\n",
      "Gradient Descent(872/999): loss=559.7270638521779\n",
      "Gradient Descent(873/999): loss=559.3998760579178\n",
      "Gradient Descent(874/999): loss=559.073728404658\n",
      "Gradient Descent(875/999): loss=558.7486156886091\n",
      "Gradient Descent(876/999): loss=558.4245327330825\n",
      "Gradient Descent(877/999): loss=558.1014743883471\n",
      "Gradient Descent(878/999): loss=557.7794355314907\n",
      "Gradient Descent(879/999): loss=557.4584110662779\n",
      "Gradient Descent(880/999): loss=557.1383959230116\n",
      "Gradient Descent(881/999): loss=556.8193850583954\n",
      "Gradient Descent(882/999): loss=556.5013734553931\n",
      "Gradient Descent(883/999): loss=556.1843561230942\n",
      "Gradient Descent(884/999): loss=555.8683280965755\n",
      "Gradient Descent(885/999): loss=555.5532844367666\n",
      "Gradient Descent(886/999): loss=555.2392202303135\n",
      "Gradient Descent(887/999): loss=554.926130589446\n",
      "Gradient Descent(888/999): loss=554.6140106518425\n",
      "Gradient Descent(889/999): loss=554.3028555804977\n",
      "Gradient Descent(890/999): loss=553.9926605635907\n",
      "Gradient Descent(891/999): loss=553.6834208143524\n",
      "Gradient Descent(892/999): loss=553.3751315709361\n",
      "Gradient Descent(893/999): loss=553.0677880962855\n",
      "Gradient Descent(894/999): loss=552.7613856780068\n",
      "Gradient Descent(895/999): loss=552.4559196282398\n",
      "Gradient Descent(896/999): loss=552.1513852835275\n",
      "Gradient Descent(897/999): loss=551.8477780046918\n",
      "Gradient Descent(898/999): loss=551.5450931767042\n",
      "Gradient Descent(899/999): loss=551.2433262085618\n",
      "Gradient Descent(900/999): loss=550.9424725331588\n",
      "Gradient Descent(901/999): loss=550.6425276071654\n",
      "Gradient Descent(902/999): loss=550.3434869109003\n",
      "Gradient Descent(903/999): loss=550.0453459482093\n",
      "Gradient Descent(904/999): loss=549.7481002463411\n",
      "Gradient Descent(905/999): loss=549.4517453558259\n",
      "Gradient Descent(906/999): loss=549.1562768503535\n",
      "Gradient Descent(907/999): loss=548.8616903266523\n",
      "Gradient Descent(908/999): loss=548.5679814043687\n",
      "Gradient Descent(909/999): loss=548.2751457259475\n",
      "Gradient Descent(910/999): loss=547.9831789565138\n",
      "Gradient Descent(911/999): loss=547.6920767837531\n",
      "Gradient Descent(912/999): loss=547.4018349177935\n",
      "Gradient Descent(913/999): loss=547.11244909109\n",
      "Gradient Descent(914/999): loss=546.8239150583062\n",
      "Gradient Descent(915/999): loss=546.5362285961991\n",
      "Gradient Descent(916/999): loss=546.2493855035038\n",
      "Gradient Descent(917/999): loss=545.9633816008184\n",
      "Gradient Descent(918/999): loss=545.6782127304895\n",
      "Gradient Descent(919/999): loss=545.3938747564994\n",
      "Gradient Descent(920/999): loss=545.1103635643524\n",
      "Gradient Descent(921/999): loss=544.8276750609625\n",
      "Gradient Descent(922/999): loss=544.5458051745424\n",
      "Gradient Descent(923/999): loss=544.2647498544907\n",
      "Gradient Descent(924/999): loss=543.9845050712823\n",
      "Gradient Descent(925/999): loss=543.7050668163587\n",
      "Gradient Descent(926/999): loss=543.4264311020169\n",
      "Gradient Descent(927/999): loss=543.1485939613023\n",
      "Gradient Descent(928/999): loss=542.8715514478992\n",
      "Gradient Descent(929/999): loss=542.5952996360229\n",
      "Gradient Descent(930/999): loss=542.3198346203135\n",
      "Gradient Descent(931/999): loss=542.045152515728\n",
      "Gradient Descent(932/999): loss=541.7712494574349\n",
      "Gradient Descent(933/999): loss=541.498121600709\n",
      "Gradient Descent(934/999): loss=541.2257651208248\n",
      "Gradient Descent(935/999): loss=540.9541762129543\n",
      "Gradient Descent(936/999): loss=540.6833510920603\n",
      "Gradient Descent(937/999): loss=540.4132859927964\n",
      "Gradient Descent(938/999): loss=540.1439771694012\n",
      "Gradient Descent(939/999): loss=539.8754208955971\n",
      "Gradient Descent(940/999): loss=539.6076134644893\n",
      "Gradient Descent(941/999): loss=539.3405511884632\n",
      "Gradient Descent(942/999): loss=539.0742303990847\n",
      "Gradient Descent(943/999): loss=538.8086474469997\n",
      "Gradient Descent(944/999): loss=538.5437987018339\n",
      "Gradient Descent(945/999): loss=538.2796805520949\n",
      "Gradient Descent(946/999): loss=538.0162894050724\n",
      "Gradient Descent(947/999): loss=537.7536216867408\n",
      "Gradient Descent(948/999): loss=537.4916738416607\n",
      "Gradient Descent(949/999): loss=537.230442332884\n",
      "Gradient Descent(950/999): loss=536.9699236418543\n",
      "Gradient Descent(951/999): loss=536.710114268313\n",
      "Gradient Descent(952/999): loss=536.4510107302033\n",
      "Gradient Descent(953/999): loss=536.1926095635743\n",
      "Gradient Descent(954/999): loss=535.9349073224876\n",
      "Gradient Descent(955/999): loss=535.6779005789226\n",
      "Gradient Descent(956/999): loss=535.421585922683\n",
      "Gradient Descent(957/999): loss=535.1659599613039\n",
      "Gradient Descent(958/999): loss=534.9110193199579\n",
      "Gradient Descent(959/999): loss=534.6567606413656\n",
      "Gradient Descent(960/999): loss=534.403180585702\n",
      "Gradient Descent(961/999): loss=534.1502758305053\n",
      "Gradient Descent(962/999): loss=533.8980430705869\n",
      "Gradient Descent(963/999): loss=533.6464790179411\n",
      "Gradient Descent(964/999): loss=533.3955804016555\n",
      "Gradient Descent(965/999): loss=533.1453439678207\n",
      "Gradient Descent(966/999): loss=532.8957664794433\n",
      "Gradient Descent(967/999): loss=532.6468447163556\n",
      "Gradient Descent(968/999): loss=532.3985754751294\n",
      "Gradient Descent(969/999): loss=532.1509555689872\n",
      "Gradient Descent(970/999): loss=531.903981827716\n",
      "Gradient Descent(971/999): loss=531.6576510975812\n",
      "Gradient Descent(972/999): loss=531.4119602412391\n",
      "Gradient Descent(973/999): loss=531.1669061376527\n",
      "Gradient Descent(974/999): loss=530.9224856820067\n",
      "Gradient Descent(975/999): loss=530.6786957856202\n",
      "Gradient Descent(976/999): loss=530.4355333758666\n",
      "Gradient Descent(977/999): loss=530.1929953960861\n",
      "Gradient Descent(978/999): loss=529.9510788055038\n",
      "Gradient Descent(979/999): loss=529.7097805791472\n",
      "Gradient Descent(980/999): loss=529.4690977077627\n",
      "Gradient Descent(981/999): loss=529.2290271977341\n",
      "Gradient Descent(982/999): loss=528.9895660710009\n",
      "Gradient Descent(983/999): loss=528.7507113649767\n",
      "Gradient Descent(984/999): loss=528.5124601324687\n",
      "Gradient Descent(985/999): loss=528.2748094415972\n",
      "Gradient Descent(986/999): loss=528.0377563757154\n",
      "Gradient Descent(987/999): loss=527.801298033331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(988/999): loss=527.5654315280249\n",
      "Gradient Descent(989/999): loss=527.3301539883747\n",
      "Gradient Descent(990/999): loss=527.0954625578752\n",
      "Gradient Descent(991/999): loss=526.8613543948605\n",
      "Gradient Descent(992/999): loss=526.6278266724269\n",
      "Gradient Descent(993/999): loss=526.3948765783555\n",
      "Gradient Descent(994/999): loss=526.1625013150358\n",
      "Gradient Descent(995/999): loss=525.9306980993891\n",
      "Gradient Descent(996/999): loss=525.6994641627928\n",
      "Gradient Descent(997/999): loss=525.4687967510051\n",
      "Gradient Descent(998/999): loss=525.2386931240895\n",
      "Gradient Descent(999/999): loss=525.0091505563403\n",
      "Gradient Descent: execution time=198.645 seconds\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 1e-7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.uniform(size=30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sgd_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=sgd_ws[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'D:\\\\Jupyter Notebook\\Machine Learning\\project1\\data\\\\submission_test1.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
